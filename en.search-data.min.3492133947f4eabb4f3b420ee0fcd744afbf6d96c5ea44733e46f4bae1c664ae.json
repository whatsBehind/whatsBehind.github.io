[{"id":0,"href":"/docs/programming/project/online-chat/","title":"Online Chat","section":"Project","content":" Demo # GitHub\nHigh Level Architecture # This is an online chat system built with BIO (Blocking IO) Using Java. Each client has two socket connections with the server, one connection supports message push mode and another one supports message pull mode.\nThe system now supports below features:\nLogin Pull online users Online chat Logoff Because of supports for push and pull modes, features like\nUpload/Download files Group chat can be easily added in the system. However, this project is just for learning purpose, so I didn\u0026rsquo;t spend too much time on it. Key Components # Client # Server Listener It\u0026rsquo;s basically a class extending Thread containing a socket connected with the server. In its run() method, it listens to InputStream of the socket which takes messages from the server. The socket in this component connects to Publisher from the server to support message push mode\npublic class ServerListener extends Thread { private Socket socket; private boolean listening = true; public ServerListener(Socket socket, User user) { this.socket = socket; this.user = user; } public void run() { try { while(listening) { ObjectInputStream ois = new ObjectInputStream(socket.getInputStream()); Message response = (Message) ois.readObject(); MessageType type = response.getType(); switch(type) { ... // Operations } } } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } } Online Chat Client It\u0026rsquo;s a client class containing another socket connecting with the server. It supports message pull mode, which is that client sends a request to server and receives a response, and it\u0026rsquo;s a synchronous request. It has two private methods sendRequest and receiveResponse which are used to support communication with the server.\npublic class OnlineChatClient { private Socket client; private ObjectOutputStream oos; private ObjectInputStream ois; private void sendRequest(Message request) throws IOException { oos = new ObjectOutputStream(client.getOutputStream()); oos.writeObject(request); } private Message receiveResponse() throws IOException, ClassNotFoundException { ois = new ObjectInputStream(client.getInputStream()); return (Message) ois.readObject(); } } Server # Message Publisher The name of this component is descriptive. Its duty is to publish a message to a corresponding client and receive the response from the server. It works with Server Listener in clients to support message push mode. Each message publisher has a 1 to 1 relationship with the client. All message publishers are managed by Message Publisher Manager\npublic class MessagePublisher { private Socket socket; private ObjectOutputStream oos; public void publish(Object o) throws IOException { oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(o); } } Message Publisher Manager It maintains a Map to store all message publishers. The map\u0026rsquo;s key is user id and the value is the message publisher. It provides methods like add() get(String userId) and delete(String userId) to manage all message publishers.\npublic class MessagePublisherManager { private final static Map\u0026lt;String, MessagePublisher\u0026gt; publishers = new ConcurrentHashMap\u0026lt;\u0026gt;(); public static boolean contains(User user) { final String id = user.getId(); return publishers.containsKey(id); } public static MessagePublisher add(User user, Socket socket) { if (contains(user)) { return publishers.get(user); } final String id = user.getId(); MessagePublisher publisher = new MessagePublisher(user, socket); return MessagePublisherManager.publishers.put(id, publisher); } public static MessagePublisher get(String userId) { return publishers.get(userId); } public static MessagePublisher get(User user) { return get(user.getId()); } public static void remove(User user) { publishers.remove(user.getId()); } } Client Listener It is similar to Server Listener in clients. It\u0026rsquo;s a Thread subclass waiting for message from the server. The thread in the most time is blocked at line\nMessage response = (Message) ois.readObject(); Once messages sent to the socket\u0026rsquo;s receive queue (InputStream), it reads the message and performs corresponding operation and returns a response to the client.\npublic class ClientListener extends Thread { private Socket socket; private ObjectOutputStream oos; private boolean listening = true; public void run() { try { while(listening) { ObjectInputStream ois = new ObjectInputStream(socket.getInputStream()); Message response = (Message) ois.readObject(); MessageType type = response.getType(); switch(type) { ... // Operations } } } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } private void sendResponse(Message message) throws IOException { oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(message); } } Client Listener Manager It manages all client listeners in a Map, supporting methods like add, get and remove.\npublic class ClientListenerManager { private final static Map\u0026lt;String, ClientListener\u0026gt; clientListeners = new ConcurrentHashMap\u0026lt;\u0026gt;(); private static ObjectOutputStream oos; public static void add(Socket socket, User user) throws IOException { Message response = Message.builder() .type(MessageType.CONNECT) .content(gson.toJson(BaseResponse.builder().successful(true).build())) .timeStamp(new Date().toString()).build(); oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(response); ClientListener listener = new ClientListener(socket, user); clientListeners.put(user.getId(), listener); listener.start(); } public static ClientListener get(User user) { final String id = user.getId(); return clientListeners.get(id); } public static ClientListener get(String id) { return clientListeners.get(id); } public static void remove(User user) { final String id = user.getId(); clientListeners.remove(id); } } Supported Features # Login # Client creates a new Socket connecting to port 9999 in local host (Server and clients are in local host) by sending below message. Message type is USER_LOGIN Before sending the message, user needs to enter user id and password Message request = Message.builder() .sender(user.getId()) .timeStamp(new Date().toString()) .type(MessageType.USER_LOGIN) .content(gson.toJson(user)) .build(); Server receives the login message from the client First server checks database if the user enters valid user id and password (Not implemented) After id and password validations, server creates a new MessagePublisher and add it into MessagePublisherManager In the end, server sends back a response to notify the client if login succeeds Client receives response from server. If login succeeded, client start a new ServerListener which is a subclass of Thread to listen to message from server. The thread is blocked when there is no messages from server. Connect: # This is not an API, but a process implicitly done in advance for features like GetOnlineUsers which utilizes OnlineChatClient\nClient creates a new Socket connecting to port 9999 in local host After server receives the request, it starts a new ClientListener, the subclass of Thread, which is listening message from the client. Then server add the ClientListener to ClientListenerManager Server send back response to the client Client receives response from server After above steps, a new socket connection between server and client is built, which supports message pull mode.\nGetOnlineUsers # Client sends a request to server Server checks MessagePublisherManager, get all online users\u0026rsquo; id Server sends a response back to client Client renders online users\u0026rsquo; id in terminal Chat # ClientA sends a request to server containing receiver\u0026rsquo;s id (ClientB) and the message Client listenerA in the server receives the request, it passes the message to message publisherB Message publisherB sends a request containing the message to clientB ClientB responds after receiving the message After message was successfully sent to clientB, client listenerA sends a response to clientA ClientA receives the response Logoff # Client sends a request to the server to logoff Server receives the request, responds to the client. Server close the socket in the client listener, and remove the listener from listener manager Server sends a request to client through message publisher Client receives the request, close socket in the server listen Client responds. Server receives the response from client. Then server closes the socket in message publisher and remove the message listener from listener manager After all aboves steps, all sockets are closed and each listener thread (ServerListener in the client and ClientListener in the server) is terminated.\nTo Be Improved # This system is built on BIO. In server and client sides, there is one thread blocked to listen to messages sent to the input stream of the socket. Thread is expensive because of the memory it occupies and performance influence when CPU switching among different threads Listener is a resource shared by two threads, main thread and the listener thread. Current code doesn\u0026rsquo;t ensure thread safety. The quick fix is to expose InputStream and OutputStream of the socket from the listener, and main thread can only access the two streams by calling the exposed methods. Also, the two methods should be synchronized. "},{"id":1,"href":"/docs/programming/backend/java/thread/start-thread/","title":"Start Thread","section":"Thread","content":" Main Lesson # Java threads are crucial for executing multiple tasks concurrently in a program. Let\u0026rsquo;s dive into this topic:\nWhat is a Thread in Java? 🧵\nIn Java, a thread is the smallest unit of execution within a process. Think of it like a worker who performs a part of a larger task. Creating a Thread 💻\nThere are two ways to create a thread: By extending the Thread class. By implementing the Runnable interface. Example: Let\u0026rsquo;s create a simple thread that prints \u0026ldquo;Hello, Java Threads!\u0026rdquo;. Starting a Thread ✨\nUse the .start() method to begin the execution of a thread. It\u0026rsquo;s like telling our worker, \u0026ldquo;Go ahead and start your task!\u0026rdquo; Thread Lifecycle 📚\nUnderstand the states: New, Runnable, Running, Waiting/Blocked, and Terminated. It\u0026rsquo;s like a day in the life of our worker, from arrival to completion of the task. Synchronization 🤝\nWhen threads share resources, we need to ensure they don\u0026rsquo;t interfere with each other. It\u0026rsquo;s like coordinating workers to avoid conflicts and ensure smooth operation. Thread Safety 🔒\nEnsuring that our threads don\u0026rsquo;t cause data corruption or inconsistent results. Think of it as safety measures in a workplace. Several Ways to Start A Thread # Extends Thread class\npublic class ExtendsThread { public static void main(String[] args) { Thread_1 thread_ = new Thread_1(); thread_.start(); } } class Thread_1 extends Thread { public void run() { System.out.println(\u0026#34;Hello World!\u0026#34;); } } Thread_1 extends Thread class and overrides method run(). We create a Thread_1 object and invoked start() method inherited from parent class Thread to start a new thread. Besides extending Thread class, we can also create a thread by implementing Runnable interface\nImplements Runnable interface\npublic class ImplementsRunnable { public static void main(String[] args) { Thread thread = new Thread(new Thread_2()); thread.start(); } } class Thread_2 implements Runnable { public void run() { System.out.println(\u0026#34;Hello World!\u0026#34;); } } Create FutureTask object\nFutureTask has a Callable field. Callable and Runnable both stand for a task, the difference is that Callable has return value, but Runnable doesn\u0026rsquo;t In below code snippet, we pass the FutureTask to a thread, and start the thread task.get() method is a blocking thread. It stop current thread, which is main here until the task finishes (run() finish in FutureTask) // Create a new future task FutureTask\u0026lt;Integer\u0026gt; task = new FutureTask\u0026lt;\u0026gt;(() -\u0026gt; { log.debug(\u0026#34;running\u0026#34;); Thread.sleep(1000); return 100; }); // Create and start a new thread Thread t = new Thread(task, \u0026#34;Thread_Get_Int\u0026#34;); t.start(); log.debug(\u0026#34;In main thread\u0026#34;); // Blocking method Integer result = task.get(); log.debug(\u0026#34;{}\u0026#34;, result); Open Question: Why do we invoke start() to start a thread, what if I invoke run() directly? # That\u0026rsquo;s a great question about thread management in Java! Let\u0026rsquo;s delve into the difference between invoking start() and run() when dealing with threads.\nUsing start() Method:\nThe start() method is used to begin the execution of a new thread in the Java runtime environment. When start() is called, it performs necessary preparations and creates a new thread in which the run() method will execute. This means the run() method executes in a separate call stack. Key Point: start() enables concurrent execution, aligning with the whole purpose of using threads. Directly Calling run() Method:\nIf you directly call the run() method, it does not start a new thread. Instead, the run() method executes in the current thread\u0026rsquo;s call stack, just like a normal method call. This means no concurrent execution happens; it\u0026rsquo;s just a regular method call. Key Point: Calling run() directly defeats the purpose of threading, as it doesn\u0026rsquo;t utilize the multithreading capabilities of Java. Consequences of Invoking run() Instead of start():\nWhen you call run() directly, you\u0026rsquo;re not utilizing the threading capabilities of Java; your code is executed on the same thread that called run(). This can lead to performance issues if the purpose is to execute tasks concurrently. It essentially turns your thread into a regular object, not a thread of execution. Summary:\nUse start() to actually start a new thread, leading to concurrent execution. Directly calling run() is like any other method call and does not start a new thread. This distinction is crucial for effective multithreading in Java. It\u0026rsquo;s part of ensuring that your program benefits from concurrency and parallelism. 🦌💻🧵 public class InvokesRun { public static void main(String[] args) { System.out.println(\u0026#34;Thread for main method \u0026#34; + Thread.currentThread().getName()); RunThread runThread = new RunThread(); StartThread startThread = new StartThread(); runThread.run(); startThread.start(); } } class RunThread extends Thread { public void run() { System.out.println(\u0026#34;Thread for RunThread: \u0026#34; + Thread.currentThread().getName()); } } class StartThread extends Thread { public void run() { System.out.println(\u0026#34;Thread for StartThread: \u0026#34; + Thread.currentThread().getName()); } } "},{"id":2,"href":"/docs/programming/web/security/asymmetric-encryption/","title":"Asymmetric Encryption","section":"Security","content":" What Is Asymmetric Encryption? # Asymmetric encryption, also known as public-key encryption, is a method of encrypting data that involves two separate keys: a public key and a private key. These keys are mathematically linked but not identical, hence the term \u0026ldquo;asymmetric.\u0026rdquo; This method provides a secure way of encrypting and decrypting information, and it\u0026rsquo;s widely used in various forms of digital communication and security protocols\nHow Does It Work? # Key Generation: # A pair of cryptographic keys is generated. The process involves complex algorithms like RSA or ECC, ensuring that these keys are mathematically linked. The public key is designed to be shared, while the private key is kept confidential by the owner.\nThe message encrypted with the public key can only be decrypted by the linked private key and vise verse.\nPublic Key Sharing: # The public key is distributed to anyone who might need to encrypt data intended for the owner. This can be done through public directories, digital certificates, or direct sharing.\nAfter Key Generation and Public Key Sharing,\nA has public key of B, and it own private key (A) B has public key of A, and it own private key (B) Encryption Process: # To send an encrypted message, the sender uses the recipient\u0026rsquo;s public key. This key encrypts the plaintext (original message), transforming it into ciphertext (encrypted message). The encryption is such that only the corresponding private key can efficiently decrypt the ciphertext.\nTransmission: # The ciphertext, now encrypted and secure, is transmitted over the network. Even if intercepted, the message remains secure because it can only be decrypted by the private key holder.\nDecryption: # The recipient uses their private key to decrypt the message. The private key reverses the encryption process, converting the ciphertext back into readable plaintext. Since only the intended recipient possesses the private key, the confidentiality and integrity of the message are maintained.\nReference # YouTube Video\n"},{"id":3,"href":"/docs/programming/backend/java/nio/byte-buffer/","title":"Byte Buffer","section":"Nio","content":" Introduction # Java ByteBuffer is a class in Java\u0026rsquo;s java.nio package. It\u0026rsquo;s used for reading and writing data to and from buffers efficiently. Buffers are blocks of memory that can store data temporarily. ByteBuffer is particularly useful when dealing with I/O operations and for high-performance applications. 📘\nByteBuffer can be used in two modes:\nRead Mode: You can read data from the buffer. 💡 Write Mode: You can write data to the buffer. 💡 Practical Example # // Create a new ByteBuffer ByteBuffer buffer = ByteBuffer.allocate(10); // Write data into the buffer buffer.put((byte) 10); buffer.put((byte) 20); // Flip the buffer to read mode buffer.flip(); // Read data from the buffer byte first = buffer.get(); byte second = buffer.get(); In this example, we first write two bytes into the buffer and then read them back.\nImportant Concepts and Operations # Capacity, Limit, and Position: ByteBuffer has three important properties:\nCapacity: The maximum number of bytes it can hold. It\u0026rsquo;s set when the buffer is created and cannot be changed. Limit: The limit is the index of the first element that should not be read or written. It can change as you read/write data. Position: The next element to be read or written. Position will increase as you read or write data. Operations # Create a new ByteBuffer\nByteBuffer buffer = ByteBuffer.allocate(10); Write ByteBuffer\nPut single byte ByteBuffer buffer = ByteBuffer.allocate(10); buffer.put((byte) \u0026#39;a\u0026#39;); Put byte array ByteBuffer buffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;}); Read ByteBuffer\nflip(): switch to read mode\nbuffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); Here we create a new ByteBuffer, and write four bytes to the ByteBuffer. The ByteBuffer is still in write mode, and to switch to read mode, we can invoke flip() method.\nbuffer.flip(); flip() updates values of limit and position\nthis.limit = position; this.position = 0; get(): read one byte and increment position\nbuffer.flip(); char data = (char) buffer.get(); get(int i): read byte of index w/o position change\nbuffer.flip(); char data = (char) buffer.get(0); // \u0026#39;a\u0026#39; pointer position doesn\u0026rsquo;t increment clear() \u0026amp; compact(): switch from read mode to write mode\nclear(): clear() resets position to 0, and limit to the capacity of the ByteBuffer. It doesn\u0026rsquo;t erase the data, but prepares a new write operation for the buffer.\nbuffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); buffer.flip(); buffer.get(); buffer.get(); buffer.clear(); compact(): compact() moves unread data to the beginning of the buffer, and sets position after the last unread byte and limit to the capacity of the buffer.\nUse case: When you have partially processed the data in the buffer and want to keep remaining unprocessed data while sill making room for the new data to be added buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); buffer.flip(); buffer.get(); buffer.get(); buffer.compact(); Manipulate position pointer\nrewind(): reset position back to 0\nByteBuffer buffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); buffer.rewind(); rewind() only does one thing that is to reset position to 0. After rewind, you can write/read from the beginning of the ByteBuffer\nmark() and reset(): bookmark position and re-visit\nByteBuffer buffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); buffer.flip(); buffer.get(); buffer.get(); buffer.mark(); buffer.get(); buffer.get(); buffer.reset(); mark() and reset() introduce a new pointer mark in ByteBuffer, and the two methods are normally used together. Value of mark in most case is -1. When mark() is invoked, current position will be remembered by assigning position to mark\nIn above code snippet, after remembering position (mark()), we continue read two more bytes from the ByteBuffer where position is 4. reset() resets position to previous mark which is 2.\nComparing to rewind() which can only reset position to 0, mark() and reset() have enhanced feature which bookmarks position and resets position to previous mark\nWork with Channel\nChannel\nChannel is an important component in NIO, and it is like Stream in BIO. Different from Stream, Channel can be used bi-directional and can be used in conjunction with Buffer\nWhen Using Channel and Buffer, data can be read from a Channel into a Buffer or written from a Buffer into a Channel Read Channel\ntry (FileChannel channel = new FileInputStream(\u0026#34;~/example.txt\u0026#34;).getChannel()) { channel.read(buffer); } Here we get the Channel from FileInputStream, and read data from the channel into a ByteBuffer\nWrite Channel\ntry (FileChannel channel = new FileOutputStream(\u0026#34;~/example.txt\u0026#34;).getChannel()) { ByteBuffer buffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); // Need to switch byteBuffer to read mode before writing to channel buffer.flip(); int len = channel.write(buffer); log.debug(String.format(\u0026#34;Write %s bytes to file\u0026#34;, len)); } Work with String\nfrom string to buffer String.getBytes():\nbuffer = ByteBuffer.allocate(16); buffer.put(\u0026#34;Hello World\u0026#34;.getBytes()); charset:\nByteBuffer buffer = StandardCharsets.UTF_8.encode(\u0026#34;Hello World\u0026#34;); ByteBuffer.wrap():\nByteBuffer buffer = ByteBuffer.wrap(\u0026#34;Hello World\u0026#34;.getBytes()); from buffer to string charset: ByteBuffer buffer = StandardCharsets.UTF_8.encode(\u0026#34;Hello World\u0026#34;); String str = StandardCharsets.UTF_8.decode(buffer).toString(); "},{"id":4,"href":"/docs/programming/aws/cloudformation/","title":"CloudFormation","section":"Aws","content":" CloudFormation # Resources # Resource Type Reference\nResources Introduction\nSyntax # Resources: Logical ID: Type: Resource type Properties: Set of properties Logical ID: A unique logical ID for that resource, which can be referenced by other parts in the template\nResource Type: An identifier of the resource that you are declaring\nResource Type Reference Resource Properties: Additional options that you can specify for a resource\nParameters (Optional) # Guide Parameters allow you to input custom values for your template. You can think the template as a module or a function with arguments. Each time creating or updating a stack using the template is like invoking the function with parameters you defined in the template.\nSyntax # Parameters: ParameterLogicalID: Type: DataType ParameterProperty: value Example Parameters: InstanceTypeParameter: Type: String Default: t2.micro AllowedValues: - t2.micro - m1.small - m1.large Referencing a Parameter # User Ref intrinsic function to reference a parameter\nExample Ec2Instance: Type: AWS::EC2::Instance Properties: InstanceType: Ref: InstanceTypeParameter ImageId: ami-0ff8a91507f77f867 Pseudo Parameter # Guide Parameters are predefined by AWS CloudFormation. You can think them as environment variables\nMappings (Optional) # Guide Mappings are fixed values in your CloudFormation template. Different from Parameters which are unknown before creating the stack, Mappings are hard coded in the template and are known in advance\nYou can think Mappings as static configurations in your project\nSyntax # YAML # Mappings: MappingName: PrimaryKey01: SecondaryKey01: Value01 PrimaryKey02: SecondaryKey02: Value02 PrimaryKey03: SecondaryKey03: Value03 Example # Mappings: DependencyAwsAccountMapping: us-ease-1: dependency01: 123456789012 dependency02: 123456789013 us-west-2: dependency01: 987654321098 dependency02: 987654321987 Find a Value in a Mapping # You can use intrinsic function !FindInMap\nSyntax !FindInMap [MappingName, PrimaryKey, Secondary] Above function returns value01\nExample AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Mappings: RegionMap: us-east-1: HVM64: ami-0ff8a91507f77f867 HVMG2: ami-0a584ac55a7631c0c us-west-2: HVM64: ami-0bdb828fd58c52235 HVMG2: ami-066ee5fd4a9ef77f1 Resources: myEC2Instance: Type: \u0026#34;AWS::EC2::Instance\u0026#34; Properties: ImageId: !FindInMap [RegionMap, !Ref \u0026#34;AWS::Region\u0026#34;, HVM64] InstanceType: m1.small Outputs (Optional) # Guide\nCross-stack Reference: Outputs section declares output values that can be imported into other templates\nYou can\u0026rsquo;t delete the stack if its outputs are referenced by other stacks\nSyntax # Outputs: LogicalId: Description: Information about the value Value: Value to return Export: Name: Name of resource to export You can think above export statements as below to better understand\nexport LogicalId as Name; Import Outputs into Other Stacks # Output from StackA\nOutputs: WebServerSecurityGroup: Description: The security group ID to use for public web servers Value: \u0026#39;Fn::GetAtt\u0026#39;: - WebServerSecurityGroup - GroupId Export: Name: \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${AWS::StackName}-SecurityGroupID\u0026#39; Import into StackB\nResources: WebServerInstance: Type: \u0026#39;AWS::EC2::Instance\u0026#39; Properties: InstanceType: t2.micro ImageId: ami-a1b23456 NetworkInterfaces: - GroupSet: - Fn::ImportValue: \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${NetworkStackNameParameter}-SecurityGroupID\u0026#39; Conditions (Optional) # Guide The Conditions section contains statements that define the circumstances under which entities are created or configured\nDefine a Condition # AWSTemplateFormatVersion: 2010-09-09 Parameters: EnvType: Description: Environment type. Default: test Type: String AllowedValues: - prod - test ConstraintDescription: must specify prod or test. Conditions: IsProd: !Equals - !Ref EnvType - prod Define a Nested Condition # Conditions: IsProd: !Equals - !Ref EnvType - prod CreateBucket: !Not - !Equals - !Ref BucketName - \u0026#39;\u0026#39; CreateBucketPolicy: !And - !Condition IsProd - !Condition CreateBucket Use Conditions # Resources: EC2Instance: Type: \u0026#39;AWS::EC2::Instance\u0026#39; Condition: IsProd Rules (Optional) # Guide Rules validates parameters passed to a template during creation or updates of a stack\nWorking with Rules # Each template rule contains two properties:\nRule Condition (Optional) - determine when a rule takes effect Each rule can only have one condition If the condition is not defined, then the rule\u0026rsquo;s assertions always take effect Rule Assertion (Required) - describe what values users can use for a specific parameter One rule can have multiple assertions Example # Rules: testInstanceType: RuleCondition: !Equals - !Ref Environment - test Assertions: - Assert: \u0026#39;Fn::Contains\u0026#39;: - - a1.medium - !Ref InstanceType AssertDescription: \u0026#39;For a test environment, the instance type must be a1.medium\u0026#39; prodInstanceType: RuleCondition: !Equals - !Ref Environment - prod Assertions: - Assert: \u0026#39;Fn::Contains\u0026#39;: - - a1.large - !Ref InstanceType AssertDescription: \u0026#39;For a production environment, the instance type must be a1.large\u0026#39; Change Set # Guide Change Set allows you to preview how existing resources could be affected, like adding or deleting a resource, once you update the template or parameters for a stack\nNested Stack # Guide Nested stacks are stacks created as part of other stacks. You create a nested stack within another stack by using the AWS::CloudFormation::Stack resource.\nYou can think nested stack as a function invoked by other modules (stacks). Similar with functions which have arguments and return values, nested stacks have parameters and outputs\nExample # Nested Stack Template\nAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Parameters: BucketName: Type: String Description: The name of the S3 bucket Resources: MySampleBucket: Type: AWS::S3::Bucket Properties: BucketName: !Ref BucketName Outputs: CreatedBucketName: Description: Name of the created S3 bucket Value: !Ref MySampleBucket Export: Name: !Sub \u0026#34;${AWS::StackName}-BucketName\u0026#34; Parent Stack Template\nAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Resources: MyNestedStack: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026#39;https://s3.amazonaws.com/mybucket/NestedStack.yaml\u0026#39; Parameters: BucketName: MyCustomBucketName Outputs: NestedStackBucketName: Description: The name of the bucket created in the nested stack Value: !GetAtt MyNestedStack.Outputs.CreatedBucketName Function Analogy # As I mentioned above, we can think nested stacks as functions invocations. We can transform above examples to a pseudo-code to illustrate the idea\nNested Function\ndef nested_function(bucket_name): created_bucket_name = create_bucket(bucket_name) return created_bucket_name Parent Function\ndef parent_function(): # Specify the bucket name to pass to the nested function (like Parameters in CloudFormation). bucket_name = \u0026#34;MyCustomBucketName\u0026#34; # Call the nested function, mimicking the creation of the nested stack. created_bucket_name = nested_function(bucket_name) # Output the result (mimicking the Outputs in CloudFormation). return created_bucket_name # Run the parent function to simulate the stack deployment. parent_function() CloudFormation Drift # A CloudFormation Drift is that the actual configuration of a stack is drifted or different from the template of the stack, in other words, the stack was modified manually\nStack Policy # A Stack Policy is a JSON file that defines the update actions that are allowed on specific resources during stack updates\n"},{"id":5,"href":"/docs/programming/network/dns/","title":"DNS","section":"Network","content":" What Is DNS? # DNS, which stands for Domain Name System, is a fundamental component of the internet\u0026rsquo;s infrastructure. It functions like a phone book for the internet by translating human-friendly domain names (like www.example.com) into IP addresses (like 192.0.2.1) that computers use to identify each other on the network.\nHierarchy of Domain Names # Root Level Domain: The root level is the highest level in the DNS hierarchy and is represented by a dot (.) but is typically not visible in domain names. Root servers are the backbone of DNS, directing traffic to the correct Top-Level Domain (TLD) servers.\nTop-Level Domain (TLD): TLDs are the part of the domain name to the right of the dot. They are broadly categorized into two types: generic TLDs (gTLDs) like .com, .org, .net, and country code TLDs (ccTLDs) like .uk, .de, .ca. The TLD serves as a key part of the domain, indicating either the nature of the domain (like .com for commercial) or the geographical location (like .uk for the United Kingdom).\nSecond-Level Domain: The second-level domain is the segment of a domain name that directly precedes the TLD. For businesses or entities, this part of the domain name is often their brand or identity. For example, in google.com, \u0026ldquo;google\u0026rdquo; is the domain name chosen by the entity.\nSubdomain: Subdomains are additional parts of the domain that are added to the left of the second-level domain and separated by dots. They are used to organizing different sections or functions of a website. For instance, a company might use a subdomain for its blog (like blog.example.com) or a specific product (like product.example.com).\nDNS Resolution Process for www.google.com # User Requests the Domain:\nThe user enters www.google.com into their web browser. Browser Checks Cache:\nThe browser checks its own cache to see if it already has the IP address for www.google.com. Query to DNS Resolver:\nIf the IP address is not in the browser cache, the browser sends a query to a DNS resolver, typically provided by the ISP or a third-party service. DNS Resolver Checks Cache:\nThe DNS resolver checks its cache to see if it has a recent record of the IP address for www.google.com. DNS Resolver Queries Root Server:\nIf the IP address is not in the resolver\u0026rsquo;s cache, it queries one of the root name servers, which provides the IP address to the TLD name servers for .com. There are 13 logical root name servers around world. Each logical root name server has hundreds of physical hosts with same IP address. The query will be sent to the nearest physical root name server by router Query to .com TLD Name Server:\nThe resolver queries the .com TLD name server, which directs it to the name servers for the google.com domain. Query to google.com Name Server:\nThe resolver queries the google.com name servers to get the IP address for www.google.com. Resolver Receives IP Address:\nThe google.com name server responds with the IP address for www.google.com. DNS Resolver Caches the Response:\nThe DNS resolver caches the IP address for a specified time based on the TTL value. The TTL value is returned together with the IP address Resolver Returns IP Address to Browser:\nThe resolver sends the IP address back to the browser. Browser Caches the Response:\nThe browser caches the IP address for future use. Browser Connects to the IP Address:\nThe browser uses the IP address to establish a connection to the server hosting www.google.com, allowing the user to access the website. Reference # Amazon Route 53 Bilibili Video "},{"id":6,"href":"/docs/programming/system-design/technology/docker/docker-basics/","title":"Docker Basics","section":"Docker","content":" Docker Basics # 🖥️ \u0026ldquo;It Works on My Machine\u0026rdquo; and Other Challenges # Imagine you\u0026rsquo;re an engineer working on a software project, and your code runs flawlessly on your machine. But as soon as your teammates try it, something breaks. Maybe it\u0026rsquo;s a missing library 📚, an incompatible runtime version 🛠️, or an environment configuration mismatch ⚙️. These issues make sharing and deploying software across different systems a tedious and error-prone process. The infamous saying, \u0026ldquo;It works on my machine,\u0026rdquo; encapsulates this frustration perfectly. 🙄\nThe root of the problem lies in dependencies 🔗. A service relies on specific runtimes, libraries, and configurations to function correctly. If even one element differs between environments, the application might behave unpredictably or fail entirely. Scaling the service across multiple machines compounds the issue, as each instance must be configured identically. Developers often spent countless hours 🕒 manually setting up environments, resolving mismatches, and troubleshooting issues, turning even small deployments into significant challenges. 😩\n💾 Virtual Machines: A Heavyweight Solution # In the early 2010s, engineers sought a better solution to ensure consistent environments for running applications. Virtual machines (VMs) 🖥️ helped by bundling an operating system and dependencies together, creating isolated environments. However, they had key drawbacks: VMs were large and resource-intensive because each included an entire OS 🐘, making them overkill for many applications that didn’t need such heavy isolation. Running multiple VMs required substantial CPU and memory 💾, limiting scalability. Their long startup times ⏳ caused delays, particularly for tasks requiring rapid initialization, such as restarting services. Portability was another challenge, as VM images were bulky and difficult to transfer between environments 📦. Managing VMs at scale was complex, with updates and patches requiring significant manual effort. These limitations made VMs inefficient for many use cases, creating a demand for a lightweight, portable alternative.\n🐳 Docker: A Lightweight Alternative # Docker was designed as a solution to the limitations of Virtual Machines (VMs), offering a lightweight and efficient way to package and deploy applications 🚀. Unlike VMs, Docker does not bundle a full operating system. Instead, it uses the host OS kernel and includes only the essential components needed to run applications. This makes Docker faster ⚡, more resource-efficient, and less demanding on hardware.\nAnother key distinction is that Docker is platform-independent. While VMs rely on the host operating system and include their own full OS stack, Docker containers are portable across different platforms. This is achieved by leveraging Docker’s ability to isolate applications from the underlying system, enabling them to run consistently on any environment that supports Docker.\nDocker’s foundation rests on two key components: images 🖼️ and containers 📦. A Docker image is a blueprint 📐 that defines everything an application needs to run, such as code, libraries, and configurations. A container is an instance 🏠 of this image, providing an isolated environment where the application runs consistently, regardless of the underlying system. With Docker, developers can create these images once and use them anywhere, ensuring that the application behaves the same way across development, testing, and production environments. This eliminates the \u0026ldquo;it works on my machine\u0026rdquo; problem while making deployments faster and more reliable. Scaling becomes straightforward, as launching additional containers from the same image is quick and efficient. ⚙️\n🛠️ Simple Example: Running Nginx with Docker # Before diving deeper, let’s look at a simple example to demonstrate how Docker works in practice. We’ll use Docker to run an Nginx web server, a lightweight and widely used server for hosting websites. 🌐\nStep 1️⃣: Pull the Nginx Image # Docker provides prebuilt images for popular software through Docker Hub. To get the Nginx image, run:\ndocker pull nginx This command downloads the latest Nginx image to your local machine. 📥\nStep 2️⃣: Run the Nginx Container # Start a container using the Nginx image:\ndocker run -d -p 8080:80 --name my-nginx nginx -d: Runs the container in detached mode (in the background). -p 8080:80: Maps port 80 inside the container to port 8080 on the host. --name my-nginx: Names the container \u0026ldquo;my-nginx\u0026rdquo; for easy reference. Step 3️⃣: Access the Web Server # Open your browser and navigate to http://localhost:8080. You’ll see the default Nginx welcome page, confirming the container is running successfully. 🎉\nStep 4️⃣: Stop and Remove the Container # When you’re done, stop and remove the container:\ndocker stop my-nginx docker rm my-nginx This simple example demonstrates how Docker simplifies running software in isolated environments while managing dependencies and configurations seamlessly. 🚀\n🔑 Understanding Key Docker Components # To fully appreciate Docker, it’s essential to understand its core architecture. Here are the key components introduced in a logical dependency order:\n1️⃣ Docker Registry # The Docker Registry is a repository 🏢 where Docker images are stored. Popular public registries like Docker Hub host a wide variety of pre-built images. You can also create private registries 🔒 for internal use. The registry serves as the source for pulling and pushing images, making it the starting point for creating containers.\n2️⃣ Docker Image # A Docker image is a pre-built package containing everything an application needs to run, such as the application code, runtime, libraries, and configurations. Think of it as a \u0026ldquo;blueprint\u0026rdquo; 📝 for your application. Images are pulled from the Docker Registry or built locally.\n3️⃣ Docker Client # The Docker Client is the primary interface 🎛️ that allows users to interact with Docker. While the most common client is the command-line interface (CLI) 💻, other interfaces, such as APIs or graphical user interfaces (GUIs), can also act as Docker Clients. For example, Docker Desktop includes a GUI for managing containers. The client sends commands like docker run or docker build to the Docker Daemon 🛠️, which processes and executes them. The Docker Client and Docker Daemon do not need to run on the same host. The client can connect to a remote daemon over a network 🌐, enabling you to manage containers on other machines from your local environment.\n4️⃣ Docker Server (Docker Daemon) # The Docker Daemon is the background process ⚙️ running on the host server. It manages Docker objects like images, containers, networks, and volumes. The daemon listens for requests from the Docker Client and executes them, such as building images or starting containers.\n5️⃣ Host Server # The host server 🖥️ is the machine where the Docker Daemon runs. It can be your local machine, a virtual machine, or a cloud server ☁️. The host provides the underlying resources like CPU, memory, and storage needed to run containers.\n6️⃣ Docker Container # A Docker container is a running instance of a Docker image 📦. Every container is like a lightweight Linux OS 🐧, providing an isolated environment for running applications. Just like a server, a container has its own ports 🔌 that can be mapped to the host system\u0026rsquo;s ports to enable communication with the outside world.\n🖼️ Visualizing the Architecture # Here’s a visual representation of the Docker components and how they interact:\nImage Source: How Docker Works\u0026hellip;.!\nThis diagram provides a clear view of how Docker images, containers, the Docker Daemon, and other components work together to simplify development and deployment. 🛠️\n🛠️ Step-by-Step Breakdown of docker run # To better understand how Docker works in practice, let’s break down what happens internally when you execute the docker run command. Each step highlights a specific component’s role in the process:\nStep 1️⃣: Parsing the Command # The Docker Client parses the docker run command and sends an API request to the Docker Daemon with the necessary instructions.\nStep 2️⃣: Checking the Image # The Docker Daemon checks if the specified image is available locally:\nIf the image exists locally, the daemon proceeds. If not, the daemon pulls the image from the Docker Registry (e.g., Docker Hub). Step 3️⃣: Pulling the Image (if necessary) # If required, the daemon downloads the image in layers 📥 from the registry. This ensures only missing parts are downloaded, optimizing bandwidth usage.\nStep 4️⃣: Creating the Container # The daemon creates a container from the image by:\nSetting up the container’s filesystem based on the image. Allocating resources like CPU and memory. Assigning a unique ID to the container. Step 5️⃣: Setting Up Networking # Docker sets up networking for the container by:\nCreating a virtual network interface. Establishing port mappings (e.g., host port 8080 to container port 80). Connecting the container to a Docker network, if required. Step 6️⃣: Starting the Container # The container starts, executing the default or specified command defined in the image.\nStep 7️⃣: Running the Application # The application inside the container runs, and logs or outputs are streamed back to the Docker Client. 🐳\n📝 Introducing Dockerfile # A Dockerfile is a script-like template that defines the steps required to build a Docker image. It provides a systematic way to package an application by specifying its dependencies, configurations, and commands. By using a Dockerfile, developers can ensure consistent builds across different environments, automate image creation, and streamline deployments effectively.\nExample of a Basic Dockerfile # Here’s a simpler example of a Dockerfile for a basic Node.js application:\n# Use an official Node.js runtime as the base image FROM node:14 # Set the working directory inside the container WORKDIR /usr/src/app # Copy package.json and package-lock.json for dependency installation COPY package.json package-lock.json ./ # Install application dependencies RUN npm install # Copy application source code to the container COPY . . # Expose the application port EXPOSE 3000 # Define the default command to run the application CMD [\u0026#34;node\u0026#34;, \u0026#34;server.js\u0026#34;] Key Instructions # FROM: Specifies the base image (e.g., python:3.9-slim). WORKDIR: Sets the working directory inside the container. COPY: Copies files from the host to the container. RUN: Executes commands to install dependencies or configure the image. CMD: Specifies the default command to execute when the container starts. Building an Image with Dockerfile # To build an image from this Dockerfile, run:\ndocker build -t my-python-app . -t: Tags the image with a name (my-python-app). .: Indicates the build context (the current directory). Running the Built Image # Once built, you can run a container from the image:\ndocker run -d -p 5000:5000 my-python-app This starts the container, maps port 5000 on the host to port 5000 in the container, and runs your Python application. 🚀\n🛠️ Understanding Docker Layers # Docker layers are the foundational building blocks of Docker images, enabling its lightweight and efficient design. Each layer represents a set of instructions from a Dockerfile, such as RUN, COPY, or ADD. These layers work together to create a single, unified image while maintaining modularity and efficiency. 🌟\nA Simple Dockerfile Example # Before diving deeper, here’s a simple Dockerfile to demonstrate how layers are created:\n# Use a base image FROM ubuntu:20.04 # Install dependencies RUN apt-get update \u0026amp;\u0026amp; apt-get install -y python3 # Copy application code COPY app.py /app/app.py # Define the default command CMD [\u0026#34;python3\u0026#34;, \u0026#34;/app/app.py\u0026#34;] How Docker Builds Layers During Image Creation # When you run docker build with this Dockerfile, Docker processes each instruction and creates a new layer for every step:\nBase Layer: The FROM ubuntu:20.04 instruction pulls the base image and sets it as the foundational layer. 🚀 Instruction Layers: RUN apt-get update \u0026amp;\u0026amp; apt-get install -y python3: Adds a new layer containing the installed dependencies. COPY app.py /app/app.py: Creates another layer with the copied application code. Layer Caching: If you rebuild the image without changing COPY app.py, Docker reuses the cached layers for the RUN and COPY steps, speeding up the process. ⚡ Final Image: Docker combines all layers into a single, cohesive image that can be reused and shared. 🖼️ Characteristics of Docker Layers # Read-Only Layers: All layers in an image are immutable. Once created, they cannot be modified. 📂 Writable Layers: When a container is started, Docker adds a writable layer on top of the image’s read-only layers. This writable layer stores any runtime changes made by the container. ✏️ Layer Reuse: Docker optimizes storage and build times by reusing existing layers. For example, if two images share a base layer, Docker stores it only once. 🔄 How Docker Ensures Isolation and Efficiency # Isolation: Each container has its own writable layer, ensuring changes made in one container do not affect others. 🔒 Efficient Reuse: Shared read-only layers allow multiple containers to use the same base image without duplication. 📦 How Docker Modifies and Accesses Files in a Container # File Access: When a container requests a file, Docker checks the writable layer first. If the file doesn’t exist there, Docker looks through the underlying read-only layers from top to bottom. 🔍 File Modification: If a container modifies a file, Docker uses a Copy-on-Write mechanism: A copy of the file is moved to the writable layer. The container modifies this private copy, leaving the original in the read-only layer untouched. ✂️ By using this layered approach, Docker achieves both efficiency and isolation, making it ideal for modern application development. 🚀\n🖼️ Union File System: The Layer Management Interface 🌐 # While Docker layers provide the building blocks for images and containers, the question arises: how are these layers managed and unified? This is where the Union File System (UFS) comes into play. UFS acts as the interface and abstraction layer responsible for merging and managing the various layers, enabling Docker to seamlessly handle file systems during image creation, builds, container creation, and runtime. 🌟\nRole of the Union File System # Image Creation and Storage:\nWhen building an image, UFS overlays the layers created by each Dockerfile instruction into a unified file system. Each new instruction (e.g., RUN, COPY) adds a layer, and UFS merges them to form the final image. 🖼️ UFS ensures that read-only layers in images are stored efficiently, avoiding duplication by sharing common layers across images. For example, if two images use the same base image, UFS ensures that the base layer is stored only once. 📦 Container Creation:\nDuring container startup, UFS overlays the read-only image layers with a writable layer that is unique to each container. This writable layer is placed on top of the stack and allows the container to make changes while preserving the original image layers. The uniqueness of the writable layer ensures complete isolation between containers, so changes in one container do not affect others or the base image. 🔒 Runtime Operations:\nAt runtime, UFS provides a unified view of the container’s file system by facilitating efficient access to files across layers. Each file in the container exists as a potential version within the writable or read-only layers. 🔄 File Access: When a file is requested, UFS starts searching from the writable layer. If the file exists, its writable-layer version is used. If not, UFS searches sequentially through the underlying read-only layers until the file is found. 🔍 File Modification: If a container modifies a file, UFS employs a Copy-on-Write mechanism: A copy of the file from the read-only layer is created in the writable layer. The writable-layer version is updated, while the original remains unchanged in the lower layers. ✏️ Summary of UFS Benefits # Layer Caching and Reuse: Enhances build efficiency by reusing unchanged layers, reducing storage requirements and build times. ⚡ Writable Layer Isolation: Ensures every container has a private writable layer for runtime changes, preserving isolation and security. 🔒 Unified Management: Abstracts layer and file system operations, enabling seamless container creation, builds, and runtime efficiency. 🌐 "},{"id":7,"href":"/docs/programming/backend/java/netty/event-loop-group/","title":"EventLoop \u0026 EventLoopGroup","section":"Netty","content":" EventLoop \u0026amp; EventLoopGroup # EventLoop # What is an EventLoop?\nAn EventLoop in Netty is a fundamental component that handles all the events related to a single Channel.\nHow does EventLoop work?\nSingle Threaded: Each EventLoop is bound to a single thread, and each Channel is registered with one EventLoop. This means all I/O operations of a Channel are always executed by the same thread, ensuring thread safety and consistency.\nEvent Processing Loop: The EventLoop continuously checks for new events in its loop and processes them. Events might include connection acceptance, data read/write, or disconnection.\nTask Queue: Besides I/O operations, EventLoops have a task queue for tasks that are not directly related to I/O operations. This ensures that these tasks are also executed in the same thread, maintaining thread safety.\nEventLoopGroup # What is EventLoopGroup? An EventLoopGroup is a collection of EventLoop instances in Netty. It\u0026rsquo;s responsible for managing these EventLoops, which are the core components handling I/O operations and events for Channels (a connection or a socket).\nRoles of EventLoopGroup?\nBoss and Worker Groups: In server-side applications, there are typically two EventLoopGroups. The \u0026lsquo;boss\u0026rsquo; group accepts incoming connections and hands them over to the \u0026lsquo;worker\u0026rsquo; group, which then handles the actual I/O operations. See boss-and-work How does EventLoopGroup work?\nWhen a client attempts to connect to the server, this connection request first arrives at the boss EventLoopGroup. The boss EventLoopGroup accepts the connection and registers it with an EventLoop in the worker EventLoopGroup. Once a connection is assigned to an EventLoop in the worker group, that EventLoop is responsible for all the events and operations associated with that connection. The EventLoop in the worker EventLoopGroup processes incoming data, sends responses, and can execute tasks like message decoding, business logic processing, and message encoding. Examples # Execute a single task\nCode\nEventLoopGroup group = new NioEventLoopGroup(2); group.next().execute(() -\u0026gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { throw new RuntimeException(e); } log.debug(\u0026#34;Hello World!\u0026#34;); }); log.debug(\u0026#34;Hello main!\u0026#34;); Execution Result\n22:10:25.111 [main] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello main! 22:10:26.111 [nioEventLoopGroup-2-1] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello World! group.next() returns a EventLoop\nThe EventLoop can execute a Runnable task in its thread\nMain thread is not blocked\nExecute scheduled tasks\nCode EventLoopGroup group = new NioEventLoopGroup(2); AtomicInteger count = new AtomicInteger(0); group.next().scheduleAtFixedRate(() -\u0026gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { throw new RuntimeException(e); } log.debug(\u0026#34;Hello EventLoop-{}!\u0026#34;, count.getAndIncrement()); }, 0, 1, TimeUnit.SECONDS); Execution Result 22:10:26.112 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-0! 22:10:27.112 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-1! 22:10:28.113 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-2! 22:10:29.113 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-3! 22:10:30.114 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-4! 22:10:31.114 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-5! 22:10:32.114 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-6! ... "},{"id":8,"href":"/docs/programming/system-design/load-balancing/haproxy/get-start/","title":"HAProxy: Get Started","section":"Haproxy","content":" Module 2: Installing HAProxy and Running Your First TCP Load Balancer (Using Docker + Netcat) # ✨ Goal of this Module # You will:\nSet up HAProxy using Docker Create a working HAProxy config for Layer 4 (TCP) load balancing Use nc (netcat) to simulate backend TCP servers Verify that HAProxy distributes connections across backends 🧱 Step 1: Project Setup # Create a folder for your HAProxy lab:\nmkdir haproxy-lab cd haproxy-lab Create two files inside it:\nhaproxy.cfg → the HAProxy configuration docker-compose.yml → to run HAProxy in Docker 🐳 Step 2: Docker Compose Setup # docker-compose.yml: # services: haproxy: image: haproxy:latest ports: - \u0026#34;9999:9999\u0026#34; # Expose HAProxy frontend on host port 9999 volumes: - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro This starts HAProxy in a container and loads your local haproxy.cfg.\n⚙️ Step 3: HAProxy Configuration # haproxy.cfg: # global log stdout format raw daemon defaults mode tcp timeout connect 5s timeout client 10s timeout server 10s frontend tcp_front bind *:9999 # Listen on port 9999 on all interfaces default_backend tcp_backends # Forward all traffic to the backend group backend tcp_backends balance roundrobin server s1 host.docker.internal:9001 check server s2 host.docker.internal:9002 check host.docker.internal allows the HAProxy container to connect to services running on your host machine (your Mac or PC).\n💠 Step 4: Start HAProxy # Run this inside your haproxy-lab folder:\ndocker-compose up You should see logs like:\nServer s1 is UP Server s2 is UP That means HAProxy is running and can reach both backend ports.\n🗅 Step 5: Start Persistent Netcat Servers # In two separate terminals, run these commands:\nServer A: # while true; do echo \u0026#34;Hello from Server A\u0026#34; | nc -l 9001; done Server B: # while true; do echo \u0026#34;Hello from Server B\u0026#34; | nc -l 9002; done This setup makes sure one message is sent per connection, and the server stays alive forever.\n🧲 Step 6: Test the Load Balancer # In a third terminal:\nnc localhost 9999 You should see:\nHello from Server A Try it again:\nHello from Server B Keep running it and watch HAProxy alternate between servers — round-robin in action!\n✅ What You Learned # How to set up HAProxy with Docker What bind, frontend, backend, and server mean in HAProxy How to simulate TCP servers using nc How Layer 4 load balancing works (HAProxy doesn\u0026rsquo;t care about message content) How to debug with netstat and nc 🗃 Next: Module 3 – Deeper TCP Features # Coming up:\nAdd richer health checks (inter, fall, rise) Try other balancing strategies like leastconn and source Enable TCP logging (option tcplog) Use more realistic backend services (e.g., Redis or Python TCP apps) Stay tuned!\n"},{"id":9,"href":"/docs/programming/backend/python/executing-a-python-file/","title":"How Is Python File Executed","section":"Python","content":" Concepts # __name__ # __name__ is a built-in variable (attribute) of a module. It is being used to indicate if a module is being run directly or being imported into other modules\nValue when module is running directly: When you run a Python module directly, __name__ of this module is set to __main__\nValue when module is imported: When the module is imported into others modules, __name__ is set to the module name (file name w/o .py extension). For example, a Python module my_module.py is imported, then its __name__ is set to my_module when it is imported\nExample # Assume you have two Python files main.py and my_module.py, and main.py is the running script and imports my_module.py\nDirectory structure\nsrc ├── main.py └── my_module.py my_module.py\n# my_module.py file print(f\u0026#34;__name__ in my_module file: {__name__}\u0026#34;) main.py\n# main.py file import my_module print(f\u0026#34;__name__ in main file: {__name__}\u0026#34;) Run python3 main.py in src directory, and you see logs in the terminal\n__name__ in my_module file: my_module __name__ in main file: __main__ Python Path (sys.path) # The sys.path in Python is a list of strings that specifies the search path for modules. When you import a module, the Python interpreter searches for the module in each directory listed in sys.path in order, from the first to the last. The search process stops as soon as the interpreter finds a module that matches the import statement, and then it attempts to load the module.\nHere\u0026rsquo;s what typically comprises sys.path:\nThe directory of the running script: The first entry in sys.path is the path to the directory containing the running script.\nPYTHONPATH Directories: If the PYTHONPATH environment variable is set, its contents are added to sys.path. PYTHONPATH is a list of directories separated by os.pathsep (which is ; on Windows and : on Unix/Linux) where Python looks for modules to import.\nStandard Library Directories: These are the directories that contain the Python standard library modules. The location of these directories depends on the Python installation.\nSite-packages Directory: This directory contains third-party modules installed using package managers like pip. There can be multiple site-packages directories if Python is configured with virtual environments or if there are user-specific installations (using pip install --user).\nBuilt-in Modules # Built-in modules are an integral part of Python and are automatically loaded with the Python interpreter, providing foundational functionality that Python code can utilize. They don\u0026rsquo;t have specific files like standard Python modules (.py files) on the disk, so they are not under directories in sys.path\nTo see all built-in modules of your Python interpreters:\nimport sys print(sys.builtin_module_names) You will see a list of built-in modules printed in the terminal. Some common built-in modules:\n(\u0026#39;sys\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;math\u0026#39;) To see origin of a module:\nimport importlib.util module_name = \u0026#34;sys\u0026#34; module_spec = importlib.util.find_spec(module_name) print(f\u0026#34;Origin of module [{module_name}]: [{module_spec.origin}]\u0026#34;) stdout Origin of module [sys]: [built-in] Origin of non-built-in modules, like site-package modules, is the directory containing the module. For example\nOrigin of module [langchain]: [/home/\u0026lt;user_name\u0026gt;/.local/lib/python3.10/site-packages/langchain/__init__.py] sys.modules # sys.modules is a dictionary in Python. It acts as a cache of all modules that have already been imported during the runtime of a program. This dictionary maps module names (as strings) to module objects, ensuring that each module is imported only once, even if it is referenced in multiple import statements across the program.\nCaching Mechanism # When you import a module, Python first checks sys.modules to see if the module is already loaded. If it is, Python uses the existing module object from sys.modules instead of reloading the module from file.\nAssume you have two Python files. When running python3 module_cache.py, you should only see \u0026ldquo;I am imported\u0026rdquo; get printed once.\nsrc ├── module_cache.py └── my_module.py my_module.py\n# my_module.py print(\u0026#34;I am imported\u0026#34;) module_cache.py\n# module_cache.py import my_module import my_module Module Compilation # Source Code Compilation\nWhen a Python module is imported or a Python script is executed, the Python interpreter first compiles the source code (.py files) into bytecode (.pyc files). Bytecode is a low-level, platform-independent representation of the source code that can be executed by the Python Virtual Machine (PVM).\nBytecode Execution\nAfter compilation, the bytecode is directly executed by the Python interpreter.\nIn-Memory Compilation\nThe bytecode for the running script is compiled in memory and is not automatically saved to disk.\nOn-Disk Cache\nThe bytecode of imported modules are cached on disk (__pycache__ directory) for further use. The __pycache__ directory is typically located in the same directory as the .py source file it corresponds to\nNamespace # A namespace in Python is essentially a mapping from names to objects (dict[str, Any]). It\u0026rsquo;s a container of all variables and their values (primitive type) or references (objects) in current scope\nType of Namespaces # Built-in namespace: Contains names provided by Python itself, such as built-in functions (print, len) and exception names.\nGlobal Namespace: Specific to a module, this contains names from various imported modules and variables defined at the level of the current module. Each module has its own global namespace.\nLocal Namespace: Specific to a function or method invocation, this contains names defined in the function. These names only exist while the function is executing.\nExamples # Below example explains the main purpose of namespace: Prevent naming conflicts by isolating names in separate scopes\nstring = \u0026#34;global namespace\u0026#34; def my_func(): string = \u0026#34;local namespace of my_func\u0026#34; print(string) def nested_func(): string = \u0026#34;local namespace of nested_func\u0026#34; print(string) nested_func() my_func() print(string) Initially, string is defined in global namespace of current module\nWhen function my_func() is invoked, a local namespace is created for it during its execution. A new string variable is defined in the local namespace\nWithin my_func(), nested_func() is defined and invoked. Another local namespace is created and nested within the local namespace of my_func(). Within the nested local namespace, a new string is defined\nOutput\nlocal namespace of my_func local namespace of nested_func global namespace Execution Process # File and File Structure # File structure\nsrc ├── main.py └── my_module.py Files\n# my_module.py print(\u0026#34;I am my_module.\u0026#34;) def greeting(): print(\u0026#34;Hello World!\u0026#34;) # main.py import my_module my_module.greeting() Executing python3 main.py # Python interpreter (python3) initializes\nPython constructs sys.path, including:\nThe directory of the script being run (src in this case)\nStandard library directories\nDirectories in PYTHONPATH\nSite-packages directories\nPython compiles and executes main.py\nPython loads my_module.py\nFirst Python searches the module in sys.modules cache\nBecause it is first time importing my_module, so the module is not in sys.modules cache. Then Python starts to search directories in sys.path list\nPython found my_module.py in the same directory containing the running script main.py and loads the file\nIf there is no cache file in __pycache__ under src or the source code of my_module.py has been modified since last compilation, Python compiles the source code into bytecode and stores it in __pycache__\nAfter compiling my_module, Python starts to execute it and print I am my_module in the terminal\nPython adds my_module to global namespace of main.py\nPython continues executing main.py\nInvokes greeting() function from my_module, and prints Hello World! in the terminal "},{"id":10,"href":"/docs/programming/aws/security/kms/","title":"KMS","section":"Security","content":" What is KMS? # AWS Key Management Service is a full management service to generate and manage encryption keys\nKey Concepts # Customer Master Key (CMK) # CMKs are primary resources created and managed by KMS. It\u0026rsquo;s a logical representation of a master key. It includes metadata like key ID, description, alias and key state. More importantly, it contains key materials which are used to encrypt and decrypt your data.\nCMKs can be:\nCustomer-managed: These are created and managed by AWS users AWS-managed: There are created and managed by AWS services, like aws/s3, aws/ddb etc\u0026hellip; Key policies # Key policies are resource policies that are attached to every CMK to determine who has what permissions on the CMK under which conditions. Every CMK must have one and only one key policy\nData Keys # Data keys are SYMMETRIC keys generated by AWS KMS. They are normally used to encrypt and decrypt large size data on client side. You can use CMK to generate, encrypt and decrypt data keys\nWhy Need Data Keys? # AWS KMS APIs kms:Encrypt and kms:Decrypt only support data size less than 4 KB. The workaround to encrypt large size data is that KMS generates data keys and clients use the generated data keys to encrypt and decrypt large size data on their sides and no need to upload the data to KMS.\nHow to Use Data Keys? # Creating a CMK # First calling create-key API to create a CMK\naws kms create-key You will get response like:\n{ \u0026#34;KeyMetadata\u0026#34;: { ... \u0026#34;KeyId\u0026#34;: \u0026#34;bca1a73c-b9d2-43e2-97e7-XXXXXXXXXXXX\u0026#34;, ... } } Creating an alias for the CMK\naws kms create-alias --target-key-id \u0026#34;bca1a73c-b9d2-43e2-97e7-XXXXXXXXXXXX\u0026#34; --alias-name alias/demo-data-key We won\u0026rsquo;t use the generated CMK to encrypt data, but use the data key. That is called Envelop Encryption\nGenerating a Data Key # Generating a symmetric data key\naws kms generate-data-key --key-id alias/demo-data-key --key-spec AES_256 You will get response:\n{ \u0026#34;CiphertextBlob\u0026#34;: \u0026#34;AQIDAHiNuXXnu8SiDat5B2+53PrUzQvrztxy2goKhceTVwNoDAFboLcM9DYLJRprvSF16VWhAAAAfjB8BgkqhkiG9w0BBwagbzBtAgEAMGgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMaAKf1DtSn99SoSYsAgEQgDsPACB1itRaJX6yTjEQuWidGXpMYwL9x47oxbepRkjQjIta4jdisEZU3VbqJhGRKLJzwFepzU+ofZWKBA==\u0026#34;, \u0026#34;Plaintext\u0026#34;: \u0026#34;tJ6HM0yWt2SwkEGDhG7IgVilYlU8j2IaX+wrr24Af7w=\u0026#34;, \u0026#34;KeyId\u0026#34;: \u0026#34;arn\\:aws\\:kms\\:eu-central-1\\:XXXXXXXXXXXX\\:key/0265a04d-b823-4a16-9fbe-772a5cb6baea\u0026#34; } Components\nPlaintext: The data key, unencrypted. It is used to encrypt or decrypt data by clients. Clients should remove it from memory every time after use it CiphertextBlob: The data key, encrypted by the CMK. Clients should keep it persistently. Relationship\nThe plaintext and encrypted data key are generated by running an encryption algorithm on the CMK. One CMK can generate multiple data keys\nStoring encrypted data key\nexport DATA_KEY_CIPHERTEXT=\u0026#34;AQIDAHiNuXXnu8SiDat5B2+53PrUzQvrztxy2goKhceTVwNoDAFboLcM9DYLJRprvSF16VWhAAAAfjB8BgkqhkiG9w0BBwagbzBtAgEAMGgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMaAKf1DtSn99SoSYsAgEQgDsPACB1itRaJX6yTjEQuWidGXpMYwL9x47oxbepRkjQjIta4jdisEZU3VbqJhGRKLJzwFepzU+ofZWKBA==\u0026#34; echo $DATA_KEY_CIPHERTEXT | base64 --decode \u0026gt; data_key_ciphertext Decrypting Encrypted Data Key # After generating a data key, client keeps the encrypted data key and should never keep the plaintext data key in their disk. To encrypt data using data key, client needs to decrypt the encrypted data key to get the plaintext data key\nexport DATA_KEY_PLAINTEXT=`aws kms decrypt --ciphertext-blob fileb://./data_key_ciphertext --query Plaintext --output text` Encrypting with Plaintext Data Key # Now you have the plaintext data key. You can use your preferred encryption algorithm to encrypt your data with the plaintext data key. Remember to remove the plaintext data key from memory after using\nEnvelop Encryption # Envelop Encryption is to encrypt your plaintext data with a plaintext data key and then encrypt the plaintext data key with a master key (CMK)\nHow AWS Services Utilize KMS # Data Protection # Data protection in AWS refers to protecting data in transit (data transmits over wire) and protecting data at rest (storing data at disk). You can protect your data in transit using TLS/SSL protocol (HTTPS), or encrypt your data at client side. To protect data at rest, you have two options:\nServer-Side encryption Client-Side encryption Use S3 as example to introduce how KMS is involved in server-side encryption\nS3 Server-Side Encryption # SSE-S3 # SSE-S3 stands for Server-Side Encryption with Amazon S3-managed Keys.\nDefault option No KMS is involved, so no extra costs Only S3 related permissions are required to access objects in S3 buckets It\u0026rsquo;s the most basic encryption method provided by S3. After you upload an object to S3 buckets, before it is saved to disk, the object will be encrypted by a S3-managed key. When you download objects from S3 buckets, the encrypted objects will be decrypted first and then downloaded to client side.\nSSE-KMS # SSE-KMS stands for Server-Side Encryption with CMK stored in KMS\nLoss: KMS is involved, so you need to pay extra costs Gain: Extra layer of security. You need to have permissions of S3 and KMS to access objects in S3 buckets To upload an object encrypted with an AWS KMS key to Amazon S3, you need kms:GenerateDataKey permissions on the key. To download an object encrypted with an AWS KMS key, you need kms:Decrypt permissions. Encryption on each object provides high security, but that results in too many calls from S3 to KMS. To balance security and HTTP traffic (also costs), instead of generating data key for each object, you can use bucket key to encrypt objects.\nS3 Bucket Keys for SSE-KMS After you configure your bucket to use S3 Bucket Key for SSE-KMS, AWS generates a short-live key from AWS KMS and then keep it in S3 for a period of time. The bucket key will create data keys for new objects during its lifecycle. That eliminates a lot of requests to KMS to generate data key for each object. SSE-C # AWS S3 allows you to use custom encryption key for server-side encryption. Clients manage the encryption key and provide the key as part of their requests and S3 manages the encryption process.\nReference # AWS KMS Workshop\n"},{"id":11,"href":"/docs/programming/system-design/load-balancing/load-balancer/","title":"Load Balancer","section":"Load Balancing","content":" Why Load Balancer? # Assume you are holding a service on a server and the service supports multiple connections. The traffic that it serves is not high, so the server can handle those traffic concurrently. Everything looks good, but unfortunately the server some day goes down, and it can\u0026rsquo;t serve any traffic. To improve the availability of your service, you bought another server and replicate your service code in the new server which can serves traffic also. Now you have two servers to receive clients\u0026rsquo; requests, and the question is how to determine the target server for each request? That\u0026rsquo;s where Load Balancer comes in\nWhat is Load Balancer? # Load Balancer is a network component sitting in front of a set of servers (hosts) to distribute clients\u0026rsquo; requests evenly to those servers behind it\nWhat are Benefits of Load Balancers? # Improve availability of your service Load balancer monitors servers behind it and forwards requests to healthy servers. If one server goes down, the load balancer removes it from the target list and forwards requests to other servers. Client can still send request and get response from the server\nEvenly distribute requests If you choose load balancer algorithm properly, requests should be sent to each server evenly and that protects servers from being overwhelmed\nType of Load Balancer # Hardware Load Balancer # Software Load Balancer # Application Load Balancer # This type of load balancer operates on application layer of OSI model, and it typically supports HTTPS/HTTP protocol. It routes requests based on request contents, so the SSL/TLS session needs termination, in other words, decrypting the request contents. The request will be forwarded to a server w/ or w/o TLS encryption.\nNetwork Load Balancer # This type of load balancer operates on transport layer, and it supports TCP protocol. It examines IP address and other network information to redirect traffic. Typically, no SSL/TLS termination is needed, because it has no idea what HTTP and TLS are.\nHow Does Load Balancer work? # The client builds a TCP connection with the load balancer and has a TLS handshake\nClient sends an HTTPS request to the load balancer\nLoad balancer decrypts the request\nLoad balancer reads and parses request contents to determine the target of the request\nLoad balancer builds a TCP connection with the selected target or use the active connections that was built\nLoad balancer forwards the request to the server and receives the response\nLoad balancer forwards the response to the client\nLoad Balancing Algorithm # A load balancer has multiple servers behind it, and load balancing algorithm is to choose the target for client requests.\nRound Robin method The requests are sent to server in a predefined order\nleast connection method The load balancer checks the connections with each server and select the server with the least connections\nWeighted least connection method Some servers can handle more active connections than others. You can assign different weights to servers and the load balancer will choose the server with least \u0026ldquo;active connections / weights\u0026rdquo;\nHealth Check # Load balancer monitors servers by pinging the service (IP address + port). That helps load balancer to manage health status of servers and forward requests to healthy servers. Once a server is unhealthy, the load balancer removes it from the target list until the server is healthy again.\nThrottling # When there is a peak of requests, and load balancer or the servers can\u0026rsquo;t handle so many requests, there are two main solutions\nSpillover/Throttling: The load balancer fails the request and return a response with error status code like 503 Request Queue: Instead of failing the request immediately, the load balancer queues the requests and forwards the requests once servers have capacity to handle them "},{"id":12,"href":"/docs/programming/system-design/monitoring/metric-log/","title":"Metric \u0026 Log","section":"Monitoring","content":" Introduction # Metrics and logs are important for a service. They help to monitor the health of a service and can also be used to debug when service goes down or crashes. This post won\u0026rsquo;t discuss importance of monitoring system but focuses on how to design a monitoring system in your host\nRequirements # Upload Data to A Web Monitoring System # Logs are records of code execution that are stored somewhere in your host. Log files rotate with given interval, like 1 hr. You log files may look like below:\nservice.log-2024-01-01-1 There are some limitations to store logs in you host:\nSize limitation Depends on how long you want to retain your logs. However, regardless the log retention, 5 years or 10 years, it is a waste of disk storage to store logs in you host Limited query function When debugging service issues, you only care about specific log patterns within a range of time. What you can do is to use regular expression and grep to query the logs which is not handy. Apart from that, it is hard to virtualize the metrics for your system Considering above limitation, a good solution is to upload your logs and metrics to a monitoring system (like AWS CloudWatch) which\nhas unlimited storage provides out-of-box query features is able to virtualize your metrics monitors metrics and triggers actions if needed and more\u0026hellip; Decouple Data Collection and Publication # To upload data to a monitoring system, we can invoke provided APIs. However, it is not a good practice to call the APIs for each single log or metric in your service code considering a lot of network traffic and low efficiency\nThe better way is to decouple the process of emitting logs and metrics with publishing them. One solution is to buffer your logs and metrics in you hosts and have an out-of-process tool that publishes the records to remote machines\nservice.log service.metrics SDKs to Emit Logs and Metrics # Comparing with metrics, logs are easier to format. The most basic log should contain two parts, timestamp and message. Timestamp is the time when the log is emitted and message contains the execution details, including class and code line that emit the log, log level (INFO, DEBUG, WARNING, ERROR), and custom message.\nMetrics are more complex which contains more information. The SKD should have a model class for the metric. Also, it should provide APIs to emit the metrics which serializes the metric instance to a string with predefined format to the buffer files\nSample codes\nMetric metric = new Metric(); metric.setTime(...); metric.setName(...); metric.setValue(...); ... metric.emit(); Sample metric string\nThe contents within delimiter ----- is a metric serialization containing all parameters of a metric\n----- time: ... name: ... value: ... ... ----- Agent to publish metrics and logs # We should have a tool to publish metrics and logs to remote monitoring system. It could be out-of-process, so it doesn\u0026rsquo;t impact your service. It periodically reads log and metric files and uploads them. To balance the traffic and size of one request, selecting a proper time interval is important\nArchitecture # Components # Service # Your service that is running in the host and generating logs and metrics. It utilizes APIs from log/metric SKD to write logs and metrics to local files. The SDK should provide below features:\nMetric model APIs to write log and emit metric Serialize metric to a specific schema that is understandable by the log/metric agent Rotate log/metric files Metric/Log Files # Files that buffer logs and metrics generated by the service\nMetric/Log Agent # An out-of-process software that\npolls data from metric/log files transform metric schema to specific format that is predefined by the cloud monitoring system periodically push metrics and logs to the remote system Cloud Monitoring System # A third party tool that stores your metrics and logs. It provides some feature like data virtualization, log query, metric action triggering etc\u0026hellip;\nConcepts # Metric # A metric is a behavior of your service or system. It is easier to explain it using examples. Common metrics for a service including count, latency, errors of API calls.\nA metric is a collection of multiple data point which is one measurement. For example, when you receive an API request, the count metric should have one data point with value 1, which means your API gets called once. You record that time when receiving the request and when you finish the process before respond to the client, you record current time. The subtraction between endTime and startTime is the value of one latency data point. If the API process fails, then data point of error with value 1 is recorded.\nSchema # Metric: { nameSpace: string, unit: string, dimensions: { key: string, value: string } dataPoints: [ { timestamp: date, value: number/string } ] } Above is a simple schema of AWS CloudWatch metric.\nnameSpace: Group name of your metrics. You manage your metrics under different groups. You can create groups based on service name, resource type etc\u0026hellip; unit: The unit for the value of metric\u0026rsquo;s data point, i.e, COUNT, MILLI_SECONDS dimensions: Map of key value pairs, which are attributes of your metric dataPoints: Collection of metric measurements timestamp: The time when the data point is recorded value: The value for one data point, like ONE API call, 50 ms latency "},{"id":13,"href":"/docs/programming/backend/api/restful-get/","title":"RESTful GET","section":"API","content":" Understanding GET in RESTful APIs: Best Practices \u0026amp; Use Cases # Introduction # The GET method is one of the most fundamental HTTP methods used in RESTful APIs. It is designed to retrieve data from a server without making modifications. In this guide, we\u0026rsquo;ll explore how GET works, when to use it, best practices, and common patterns for designing RESTful GET APIs.\n1. What is the GET Method? # The GET method is used for reading and retrieving data. Unlike POST, PUT, or DELETE, it does not modify any server-side resources.\nKey Characteristics of GET: # ✔️ Read-only: It fetches data but does not change anything. ✔️ Idempotent: Multiple identical GET requests return the same result. ✔️ Safe: No risk of unintended changes to the server. ✔️ Cacheable: Can be cached to improve performance. ✔️ Does not require a request body: Data is sent through the URL using path parameters or query parameters.\n2. Common GET API Patterns in REST # a) Fetching a Specific Resource (Path Parameter :) # When retrieving a single resource, a path parameter (:) is commonly used.\n🔹 Example:\nGET /users/:userId ➡ GET /users/12345 retrieves user 12345.\n🔹 Response:\n{ \u0026#34;id\u0026#34;: 12345, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34; } ✅ When to use: When requesting a single entity.\nb) Fetching Multiple Resources (Collection Pattern) # When retrieving a list of resources, a collection endpoint is used.\n🔹 Example:\nGET /products ➡ Retrieves all products.\n🔹 Response:\n[ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Laptop\u0026#34;, \u0026#34;price\u0026#34;: 1200 }, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Phone\u0026#34;, \u0026#34;price\u0026#34;: 800 } ] ✅ When to use: When requesting a collection of items.\nc) Searching and Filtering (Query Parameters ?) # Use query parameters to filter, search, or refine results dynamically.\n🔹 Example:\nGET /products?category=electronics\u0026amp;price_lt=1000 ➡ Retrieves electronics under $1000.\n🔹 Response:\n[ { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Phone\u0026#34;, \u0026#34;price\u0026#34;: 800 } ] ✅ When to use: When applying search filters to a dataset.\nd) Pagination (Handling Large Datasets Efficiently) # Use page and pageSize to limit the number of results.\n🔹 Example:\nGET /users?page=2\u0026amp;pageSize=20 ➡ Retrieves page 2 with 20 users per page.\n🔹 Response:\n{ \u0026#34;page\u0026#34;: 2, \u0026#34;pageSize\u0026#34;: 20, \u0026#34;totalPages\u0026#34;: 5, \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: 21, \u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34; }, { \u0026#34;id\u0026#34;: 22, \u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34; } ] } ✅ When to use: When dealing with large datasets.\ne) Sorting Results # Use sort and order query parameters to arrange results.\n🔹 Example:\nGET /products?sort=price\u0026amp;order=desc ➡ Retrieves products sorted by price in descending order.\n🔹 Response:\n[ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Laptop\u0026#34;, \u0026#34;price\u0026#34;: 1200 }, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Phone\u0026#34;, \u0026#34;price\u0026#34;: 800 } ] ✅ When to use: When users need sorted lists.\n3. GET API Best Practices # ✅ 1. Use Nouns in URLs (Avoid Verbs) # ❌ Bad: GET /getUsers\n✅ Good: GET /users\n✅ 2. Use Query Parameters for Filters \u0026amp; Pagination # ❌ Bad: GET /users/filter?name=John\n✅ Good: GET /users?name=John\n✅ 3. Use Proper Status Codes # 200 OK → Request was successful. 204 No Content → No data available. 404 Not Found → Resource does not exist. 400 Bad Request → Invalid query parameters. ✅ 4. Support Caching # Use ETag and Cache-Control headers for faster responses.\n✅ 5. Use Pagination for Large Datasets # Do not return thousands of records in one response—use limit \u0026amp; offset.\n4. When NOT to Use GET # ❌ 1. When modifying data\nUse POST, PUT, or DELETE instead. Example: GET /deleteUser?userId=123 ❌ (wrong, use DELETE /users/123) ❌ 2. When sending sensitive data\nAvoid sending passwords or tokens in the URL (GET /login?password=abc123). ❌ 3. When handling complex search logic\nUse POST with a request body if the query is too large. 5. Summary # Feature GET Method Purpose Retrieve data (read-only) Modifies Data? ❌ No Idempotent? ✅ Yes Safe? ✅ Yes Caching Support? ✅ Yes Request Body? ❌ No (parameters in URL) Best Use Cases Fetching single or multiple resources "},{"id":14,"href":"/docs/programming/os/mac-os/terminal-command/text-processing-and-searching/","title":"Text Processing and Searching","section":"Terminal Command","content":" grep # Introduction to grep # grep stands for \u0026ldquo;Global Regular Expression Print.\u0026rdquo; It is a powerful command-line utility used for searching plain-text data for lines that match a regular expression. grep is widely used for searching specific patterns within files and outputting the matching lines.\nBasic Syntax # grep [options] pattern [file...] pattern: The string or regular expression to search for. [file...]: The file(s) to search. If no files are specified, grep searches the input provided from the standard input. Common Options # -i: Ignore case distinctions in the pattern. -v: Invert the match, showing lines that do not match the pattern. -r or -R: Recursively search directories. -n: Prefix each line of output with the line number within its input file. -l: Print only the names of files containing matches. -c: Print only a count of matching lines per file. -E: Use extended regular expressions. -o: Only show the matched words Practices # Let\u0026rsquo;s have some hands-on practices for grep.\nSet up Practicing Env # Navigate to a directory that you want to practice grep\ncd ~/practics/mac-os/terminal-commands/grep Write contents to file practice.txt\ncat \u0026lt;\u0026lt;EOL \u0026gt; practice.txt practice practices Practices Learn with practices I am practicing grep command EOL Create a new directory and copy practice.txt to the nested directory\nmkdir nested_dir cp practice.txt nested_dir Verify that you have the practice env set up\ntree . The file structure should look like below\n. ├── nested_dir │ └── practice.txt └── practice.txt 2 directories, 2 files Make Your Hands Dirty # Basic Search\nSearches for \u0026ldquo;practice\u0026rdquo; in file practice.txt, and prints the matching lines\ngrep practice practice.txt The matching words are case-sensitive\npractice practices Learn with practices Case-Insensitive Search\nSearches for \u0026ldquo;practice\u0026rdquo; in practice.txt, ignoring case.\ngrep -i practice practice.txt Word \u0026ldquo;Practice\u0026rdquo; with capital \u0026ldquo;P\u0026rdquo; is matched\npractice practices Practices Learn with practices Recursive Search\nSearch for practice in all files under current directory, including nested directory\ngrep -r practice . Output\n./nested_dir/practice.txt:practice ./nested_dir/practice.txt:practices ./nested_dir/practice.txt:Learn with practices ./practice.txt:practice ./practice.txt:practices ./practice.txt:Learn with practices Count Matches\nPrints the number of lines containing \u0026lsquo;pattern\u0026rsquo; in practice.txt\ngrep -c practice practice.txt Output\n3 Display Line Numbers\nSearches for \u0026ldquo;practice\u0026rdquo; in practice.txt and displays the line numbers of matching lines\ngrep -n practice practice.txt Output\n1:practice 2:practices 4:Learn with practices Search for Exact Word\nSearches for the exact word \u0026ldquo;practice\u0026rdquo; in practice.txt\ngrep -w practice practice.txt Only word practice is exactly matched\npractice Invert Match\nPrints lines that do not match \u0026ldquo;practice\u0026rdquo; in practice.txt\ngrep -v practice practice.txt Output\nPractices I am practicing grep command Display Only Filenames\nPrints the names of files containing \u0026lsquo;pattern\u0026rsquo; within the current directory\ngrep -lr practice . Output\n./nested_dir/practice.txt ./practice.txt Using Extended RegEx\nSearches for \u0026ldquo;practice\u0026rdquo; and \u0026ldquo;practicing\u0026rdquo; in file practice.txt using extended regular expressions\ngrep -E \u0026#34;practice|practicing\u0026#34; practice.txt Output\npractice practices Learn with practices I am practicing grep command Only Show Matched Parts\nOnly shows the matched word \u0026ldquo;practice\u0026rdquo; and prints line number\ngrep -on practice practice.txt Output\n1:practice 2:practice 4:practice sed # "},{"id":15,"href":"/docs/programming/system-design/technology/redis/why-is-redis-so-fast/","title":"Why Is Redis So Fast","section":"Redis","content":" Why is Redis So Fast? # Redis is a blazing-fast, in-memory data structure store that serves as a database, cache, and message broker. Its name stands for \u0026ldquo;Remote Dictionary Server,\u0026rdquo; and it’s an open-source project known for its speed, simplicity, and versatility.\nRedis achieves its incredible speed through several key design principles and optimizations:\n1. In-Memory Operations # Unlike traditional databases that rely on disk storage, Redis stores all data in memory. Modern DRAM (Dynamic Random-Access Memory) is a type of volatile memory widely used in computers for its high speed and low latency.\nUnderstanding MT/s and Bandwidth # For example, DDR5 DRAM—the latest generation of DRAM—can achieve memory access speeds of around 4800 MT/s (megatransfers per second). MT/s reflects the number of data transfers per second over the memory bus. DRAM employs double data rate (DDR) technology, which allows it to send data on both the rising and falling edges of the clock cycle, effectively doubling the transfer rate. Each transfer moves a fixed amount of data, and this is used to calculate the bandwidth (GB/s):\nBandwidth (GB/s) = MT/s × Bus Width (Bytes) ÷ 1000\nFor instance, with DDR5-4800, having 4800 MT/s and a bus width of 64 bits (8 bytes): Bandwidth = 4800 × 8 ÷ 1000 = 38.4 GB/s per channel.\nCalculating Redis Operations # Assume each Redis operation requires 100 bytes of data (both request and response). Using the theoretical bandwidth of 38.4 GB/s for a single channel, the rough queries per second (QPS) Redis could achieve is:\nQPS = Bandwidth (Bytes/s) ÷ Operation Size (Bytes) QPS = 38.4 × 10^9 ÷ 100 = 384 million operations per second.\nThis theoretical calculation showcases the massive potential throughput of Redis when operating entirely in memory with modern DRAM, far exceeding the capabilities of traditional disk-based systems and enabling sub-millisecond response times. However, this is just a rough calculation. In real-world scenarios, the QPS of Redis is bottlenecked by other factors such as CPU, network bandwidth, and I/O limitations, with memory being just one of the constraints.\n2. Efficient Data Structures # Redis uses highly optimized data structures specifically designed to maximize performance. For example, dictionaries in Redis allow O(1) average-time complexity for lookups and inserts, making key-value storage extremely efficient. Skip lists are used for sorted sets, enabling fast range queries and ordered operations with O(log n) complexity. Additionally, Redis employs lightweight compression techniques to reduce memory usage while ensuring minimal impact on data access speed. These tailored data structures ensure that Redis can handle millions of operations per second while maintaining consistent low latency.\n3. Single-Threaded Event Loop # The benefit of using a single-threaded event loop in Redis is that it eliminates the need for context switching and complex locking mechanisms, enabling more predictable and efficient operation. This simplicity also avoids issues like race conditions and deadlocks, making Redis highly reliable for lightweight commands and in-memory operations.\nHowever, this raises a critical question: can a single thread handle high volumes of operations in a modern computing environment? To address this, let’s calculate the theoretical performance of a single-threaded Redis instance, assuming one operation takes 100 CPU cycles on average.\nCalculating Redis Operations Per Second (CPU-bound) # CPU Clock Speed: 4.5 GHz (4.5 × 10^9 cycles/second) Cycles Per Operation: 100 CPU cycles per operation (assumption for lightweight Redis commands). Thus, the number of operations Redis can handle is:\nQPS = Clock Speed / Cycles Per Operation QPS = 4.5 × 10^9 / 100 = 45 million operations per second.\n4. Minimal Protocol Overhead # RESP (Redis Serialization Protocol) is a lightweight, custom protocol dedicated to Redis that offers several key advantages:\nCompact Format: RESP eliminates the overhead of verbose headers and negotiation mechanisms, which eases CPU usage by requiring significantly fewer cycles to parse and execute compared to general-purpose protocols like HTTP. Reduced Network Bandwidth: By containing fewer bytes in both requests and responses, RESP minimizes the network bandwidth required for communication, improving overall throughput. Integrated Pipelining: RESP seamlessly supports pipelining, allowing multiple commands to be sent in a single request, which further reduces latency and boosts performance. 5. Pipelining # Redis allows clients to send multiple commands in a single request through its pipelining feature, which reduces round-trip latency and significantly improves throughput. For example, instead of waiting for each command\u0026rsquo;s response before sending the next, a client can send a batch of commands like SET key1 value1, SET key2 value2, and GET key1 all at once. The server processes them sequentially and sends back a batch of responses, minimizing network delays. 6. Optimized Network IO # Redis handles network requests and responses efficiently using non-blocking I/O and event-driven mechanisms. Here’s a step-by-step explanation of how Redis handles client requests and responses:\nStep-by-Step Handling of a Request and Response in Non-Blocking I/O # Client Sends a TCP Request:\nThe client sends a command (e.g., GET key1) over an established TCP connection to Redis. The data is broken into packets and sent to the server. OS Receives TCP Packets:\nThe operating system (OS) kernel on the server receives the incoming packets, reassembles them, and stores the complete request in the socket buffer, which is managed by the OS. In non-blocking mode, Redis’s main thread does not wait for the complete message to be ready in the buffer. Instead, Redis continues processing other ready events in its event loop until notified by the OS that the request is ready for reading. Redis Reads the Request:\nWhen the complete request is ready in the socket buffer, the OS notifies Redis that it is ready to be read and processed. Redis’s main thread processes the \u0026ldquo;ready-to-read\u0026rdquo; event in its event loop. It executes a non-blocking read() system call to fetch the request data from the socket buffer into Redis’s user-space memory. If the full request is not yet available, Redis skips this socket and continues processing other events, avoiding blocking. Redis Processes the Request:\nRedis parses and executes the command (e.g., GET key1) by retrieving the key’s value from its in-memory data structure. The response (e.g., value1) is prepared in memory, waiting to be written to the socket buffer. Redis Writes the Response to the Socket Buffer:\nRedis executes a non-blocking write() system call to send the response to the socket buffer. If the socket buffer has enough space, the data is copied immediately, and the write() call returns. If the buffer is full (e.g., due to a slow client), the write() call defers, and Redis marks the socket as \u0026ldquo;waiting for readiness.\u0026rdquo; After marking, Redis does not block; instead, it immediately moves on to process other ready requests or tasks in its event loop. Redis retries the write when the OS notifies that the socket is writable again, ensuring efficient use of the main thread while maintaining responsiveness. OS Transmits the Data to the Client:\nOnce the response is in the socket buffer, the OS and the network interface card (NIC) handle the actual transmission of the data over the network. Redis itself is not involved in this process; the OS takes full responsibility for reliably sending the data to the client, including handling retransmissions and acknowledgments. Key Points of Non-Blocking I/O # Redis never blocks its main thread while waiting for I/O operations (e.g., receiving a request, writing a response). The OS handles low-level tasks like managing socket buffers and transmitting data over the network. Redis’s event loop ensures it efficiently processes ready events (e.g., requests from other clients) while waiting for I/O readiness. This non-blocking I/O model, combined with Redis’s single-threaded design, allows it to scale efficiently and handle thousands of concurrent connections without bottlenecks.\nNetwork Bandwidth Calculation # Let’s calculate the theoretical maximum number of requests Redis can handle based solely on network bandwidth. Assume both the request and response are 100 bytes each.\nAssumptions: # Request size = 200 bytes, considering network overhead like TCP/IP headers Response size = 200 bytes Total data per operation = 200 (request) + 200 (response) = 400 bytes Network bandwidth = 10 Gbps (10 gigabits per second) Calculation: # Convert bandwidth to bytes per second:\n10 Gbps = 10 × 10^9 bits/second\n1 byte = 8 bits, so:\nBandwidth (bytes/second) = 10 × 10^9 ÷ 8 = 1250 × 10^6 bytes/second\nCalculate the maximum requests per second:\nRequests per second (RPS) = Bandwidth (bytes/second) ÷ Data per operation (bytes)\nRPS = 1250 × 10^6 ÷ 400 = 3.125 million requests per second\nConclusion: # With a 10 Gbps network connection, Redis can theoretically handle up to 3.125 million requests per second network bandwidth-wise, assuming no other bottlenecks (e.g., CPU or memory).\n7. Partitioning # With Redis Cluster, data can be partitioned across multiple nodes, enabling horizontal scaling while maintaining high performance. If higher QPS is required and a single thread cannot handle the load, the system can be scaled by partitioning incoming requests and deploying more Redis instances to distribute the workload efficiently.\n"},{"id":16,"href":"/docs/programming/backend/java/nio/blocking-mode/","title":"Blocking Mode","section":"Nio","content":" Blocking Mode # This post will introduce the blocking mode of network connection and communication in Java code. We will first wirte both server and client codes. Then demo the blocking mode and its problems.\nCode Example # Server\n@Slf4j public class Server { public static void main(String[] args) throws IOException { ByteBuffer buffer = ByteBuffer.allocate(32); // Create server ServerSocketChannel ssc = ServerSocketChannel.open(); // Server listens to port 9999 at local host ssc.bind(new InetSocketAddress(9999)); log.debug(\u0026#34;Create server listening to port 9999\u0026#34;); List\u0026lt;SocketChannel\u0026gt; socketChannels = new ArrayList\u0026lt;\u0026gt;(); while (true) { // Accept: build connection with client log.debug(\u0026#34;Waiting for client connection...\u0026#34;); SocketChannel socketChannel = ssc.accept(); log.debug(\u0026#34;Build connection with client \u0026#34; + socketChannel.getRemoteAddress()); // Add socketChannel to a list socketChannels.add(socketChannel); // Iterate all SocketChannels and read data from channel and write it to the ByteBuffer for (SocketChannel channel : socketChannels) { log.debug(\u0026#34;Start to read channel from client \u0026#34; + channel.getRemoteAddress()); channel.read(buffer); buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); log.debug(\u0026#34;Complete reading data\u0026#34;); } } } } This is code for the server. First we create a ServerSocketChannel listens to port 9999 of local host We have an infinite while loop where the server accepts connection from clients. Once connection is built, the server creates a new SocketChannel and add it into a list After connection is built, we iterate collection of all SocketChannel, and read data from the channel and write it to a ByteBuffer In the server code above, we add multiple logs before and after client connection and reading data from the channel to better demonstrate blocking mode.\nClient\n@Slf4j public class Client { public static void main(String[] args) throws IOException { SocketChannel sc = SocketChannel.open(); log.debug(\u0026#34;Connecting to server\u0026#34;); sc.connect(new InetSocketAddress(InetAddress.getLocalHost(), 9999)); log.debug(\u0026#34;Connected with server\u0026#34;); while (true) { String input = Scanner_.scanLine(\u0026#34;Input: \u0026#34;); log.debug(\u0026#34;Sending data [{}] to server\u0026#34;, input); sc.write(StandardCharsets.UTF_8.encode(input)); log.debug(\u0026#34;Sent data [{}] to server\u0026#34;, input); } } } In client\u0026rsquo;s code, we connect to the server Then in a while loop, we scan input from terminal and send the data to the server Same as server\u0026rsquo;s code, we add some logs\nDemo # Start the server # Server log:\n20:29:52.904 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Create server listening to port 9999 20:29:52.906 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Waiting for client connection... We start a server, and server is blocked until a client connects to the server. This is the first blocking place in the server code. SocketChannel socketChannel = ssc.accept(); Start clientA # Server log:\n20:33:01.791 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Build connection with client /127.0.0.1:50632 20:33:01.791 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Start to read channel from client /127.0.0.1:50632 Client of port(50632) connects to the server\nServer code is blocked again until recevies any data from clientA\nchannel.read(buffer); ClientA log:\n20:33:01.777 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Connecting to server 20:33:01.780 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Connected with server Input: ClientA conects to the server and waiting for user to type in the terminal Send data from clientA to the server # ClientA log:\nInput: Hello Server from ClientA! 20:38:02.356 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Sending data [Hello Server from ClientA!] to server 20:38:02.358 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Sent data [Hello Server from ClientA!] to server Input: In clientA, we typed Hello Server from ClientA!. After hitting Enter key, the string we just typed was sent to the server. Server log:\n20:38:02.358 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server from ClientA! 20:38:02.359 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Complete reading data 20:38:02.359 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Waiting for client connection... Server receives the string Hello Server from ClientA!, and prints the string in the terminal Server is not blocked at reading data from channel, and code is executed again Server is blocked at waiting for connection from another client Send data again from clientA to the server # ClientA log:\nInput: Hello Server from ClientA Again! 20:42:28.083 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Sending data [Hello Server from ClientA Again!] to server 20:42:28.084 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Sent data [Hello Server from ClientA Again!] to server Input: ClientA sends a new string Hello Server from ClientA Again! to the server Server\nServer doesn\u0026rsquo;t have any new logs printed. Because it was blocked at waiting for client connection, no matter how much data sent from clientA, it won\u0026rsquo;t be handled by the server ClientB connects to the server # Client B log:\n20:45:17.107 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Connecting to server 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Connected with server Input: A new clientB connects to the server Server log:\n20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Build connection with client /127.0.0.1:50698 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Start to read channel from client /127.0.0.1:50632 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server from ClientA Again! 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Complete reading data 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Start to read channel from client /127.0.0.1:50698 ClientB from port(50698) connects to the srever. After new client connection, code is executed again. Server starts to read channels from clientA (port 50632) and prints the data sent some time ago Hello Server from ClientA Again Server is blocked again waiting for clientB sending data\u0026hellip; Summary # As we can see in the demo, blocking mode is low efficient and unable to hanle multiple client connections using one thread. To handle connection and receiving data from multiple clients properly, we have to create a new thread for each connection, which is unrealistic. In next post, we will discuss unblocking mode and its underlying problems.\n"},{"id":17,"href":"/docs/programming/backend/java/netty/channel/","title":"Channel \u0026 ChannelFuture","section":"Netty","content":" Concept # What is Channel? # Definition\nA Channel in Netty represents an open network connection, such as a socket. It\u0026rsquo;s a key abstraction that encapsulates the underlying network transport, such as TCP or UDP.\nRole\nData Communication: A Channel is used for reading data from and writing data to the network. State Management: It keeps track of the state of a network connection (e.g., whether it\u0026rsquo;s open, connected, etc.). What is ChannelFuture? # Definition\nA ChannelFuture represents the result of an asynchronous Channel I/O operation. It\u0026rsquo;s a promise-like mechanism that provides a way to be notified when an asynchronous operation completes.\nUsage # Create a ChannelFuture # ChannelFuture channelFuture = new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer\u0026lt;NioSocketChannel\u0026gt;() { @Override protected void initChannel(NioSocketChannel nioSocketChannel) throws Exception { nioSocketChannel.pipeline().addLast(new StringEncoder()); } }) .connect(new InetSocketAddress(9999)); Channel Connect # Let\u0026rsquo;s add a LoggingHandler to better track the lifecycle of the Channel\nChannelFuture channelFuture = new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer\u0026lt;NioSocketChannel\u0026gt;() { @Override protected void initChannel(NioSocketChannel nioSocketChannel) throws Exception { + nioSocketChannel.pipeline().addLast(new LoggingHandler()); nioSocketChannel.pipeline().addLast(new StringEncoder()); } }) .connect(new InetSocketAddress(9999)); Synchronous Way # Code\nlog.debug(\u0026#34;Start to connect...\u0026#34;); channelFuture.sync(); log.debug(\u0026#34;Connected\u0026#34;); Channel channel = channelFuture.channel(); channel.writeAndFlush(\u0026#34;Hello World\u0026#34;); Execution Result\n2023-12-31 11:16:55 [main] DEBUG c.w.netty_.component.channel.Client - Start to connect... 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2] REGISTERED 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2] CONNECT: 0.0.0.0/0.0.0.0:9999 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2, L:/127.0.0.1:49310 - R:0.0.0.0/0.0.0.0:9999] ACTIVE 2023-12-31 11:16:55 [main] DEBUG c.w.netty_.component.channel.Client - Connected 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2, L:/127.0.0.1:49310 - R:0.0.0.0/0.0.0.0:9999] WRITE: 11B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 57 6f 72 6c 64 |Hello World | +--------+-------------------------------------------------+----------------+ 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2, L:/127.0.0.1:49310 - R:0.0.0.0/0.0.0.0:9999] FLUSH Explain\nThe lifecycle of Channel and custom logs are printed the in the terminal Two thread: \u0026ldquo;main\u0026rdquo; and \u0026ldquo;worker\u0026rdquo; (nioEventLoopGroup-2-1) are involved Netty build the connection from the client to the server in a non-blocking asynchronous way. Main thread initiates the connection The connection then is handed over to the worker thread The Channel is REGISTERED -\u0026gt; CONNECT -\u0026gt; ACTIVE sync() stops the main thread until the Channel becomes CONNECT Then \u0026ldquo;Hello World\u0026rdquo; is written and flushed Although Netty builds the connection in a non-blocking and asynchronous way, because of using sync() method, the logs are printed in linear order like building the connection synchronously Asynchronous Way / Callback # Code\nchannelFuture.addListener(future -\u0026gt; ((ChannelFuture) future).channel().writeAndFlush(\u0026#34;Hello World\u0026#34;)); Execution Result:\n2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807] REGISTERED 2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807] CONNECT: 0.0.0.0/0.0.0.0:9999 2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807, L:/127.0.0.1:49450 - R:0.0.0.0/0.0.0.0:9999] WRITE: 11B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 57 6f 72 6c 64 |Hello World | +--------+-------------------------------------------------+----------------+ 2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807, L:/127.0.0.1:49450 - R:0.0.0.0/0.0.0.0:9999] FLUSH 2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807, L:/127.0.0.1:49450 - R:0.0.0.0/0.0.0.0:9999] ACTIVE How does it work?\nIn Netty, operations like reading, writing or connecting over a network channel are asynchronous. When you invoke such an operation, it doesn\u0026rsquo;t complete immediately.\nPerform the Asynchronous Operation: You initiate an operation, like channel connection, which returns a ChannelFuture Attach Listener to ChannelFuture: You use addListener to attach a ChannelFutureListener to this ChannelFuture Completion of the Operation: When the operation you listen to finishes, the callback method is invoked Channel Close # Synchronous Way # Code\nChannel channel = channelFuture.sync().channel(); channel.close(); ChannelFuture closeFuture = channel.closeFuture(); closeFuture.sync(); group.shutdownGracefully(); Execution result\n2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311] REGISTERED 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311] CONNECT: localhost/127.0.0.1:9999 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311, L:/127.0.0.1:45504 - R:localhost/127.0.0.1:9999] ACTIVE 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311, L:/127.0.0.1:45504 - R:localhost/127.0.0.1:9999] CLOSE 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311, L:/127.0.0.1:45504 ! R:localhost/127.0.0.1:9999] INACTIVE 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311, L:/127.0.0.1:45504 ! R:localhost/127.0.0.1:9999] UNREGISTERED Asynchronous Way # Code Channel channel = channelFuture.sync().channel(); channel.close(); channel.closeFuture().addListener(future -\u0026gt; { group.shutdownGracefully(); }); Channel Lifecycle # In the execution history of closing the Channel, we can see the lifecycle or state of a Channel\nREGISTERED: The Channel is registered with an EventLoop. This means it\u0026rsquo;s now bound to a particular thread for its I/O operations CONNECT: The Channel is trying to connect to a server ACTIVE: A Channel is considered active when it\u0026rsquo;s connected to a remote peer and ready for I/O operations CLOSE: The Channel closes the connection and releases all underlying resources. A closed Channel can\u0026rsquo;t be reopened INACTIVE: A Channel in the INACTIVE state has been disconnected from the remote endpoint but is not necessarily fully closed. It is potential for reuse UNREGISTERED: The Channel is unregistered from its EventLoop "},{"id":18,"href":"/docs/programming/backend/java/thread/common-methods/","title":"Common Methods","section":"Thread","content":" Common Methods of Java Thread # Sleep(long millis) # Static method that causes the currently executing thread to sleep (temporarily cease execution) for the specified number of milliseconds. If a thread is sleeping, its state is changed to TIMED_WAITING Sleep method could be interrupted by calling interrupt(). After interruption, sleep throws InterruptedException, and thread state is changed to RUNNABLE Example # Code Thread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { log.debug(\u0026#34;Interrupted\u0026#34;); log.debug(\u0026#34;t1 state: {}\u0026#34;, Thread.currentThread().getState()); } }); t1.setName(\u0026#34;t1\u0026#34;); t1.start(); Thread.sleep(50); log.debug(\u0026#34;t1 state: {}\u0026#34;, t1.getState()); log.debug(\u0026#34;Interrupting thread t1\u0026#34;); t1.interrupt(); Execution results 2024-01-06 12:55:33 [main] DEBUG c.w.thread_.commonmethods.Sleep_ - t1 state: TIMED_WAITING 2024-01-06 12:55:33 [main] DEBUG c.w.thread_.commonmethods.Sleep_ - Interrupting thread t1 2024-01-06 12:55:33 [t1] DEBUG c.w.thread_.commonmethods.Sleep_ - Interrupted 2024-01-06 12:55:33 [t1] DEBUG c.w.thread_.commonmethods.Sleep_ - t1 state: RUNNABLE join() # Waits for the thread to die or terminate. Use case # Code\n@Slf4j public class Join_ { static int result = 0; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(50); result++; } catch (InterruptedException e) { throw new RuntimeException(e); } }); t1.start(); log.debug(\u0026#34;result value: {}\u0026#34;, result); } } Execution Result\n2024-01-06 13:18:42 [main] DEBUG c.w.thread_.commonmethods.Join_ - result value: 0 result was printed in the main thread, but its value is not incremented in thread t1 yet, because the thread first sleeps 50ms Solution # Code\n@Slf4j public class Join_ { static int result = 0; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(50); result++; } catch (InterruptedException e) { throw new RuntimeException(e); } }); t1.start(); + t1.join(); log.debug(\u0026#34;result value: {}\u0026#34;, result); } } Flowchart\nExecution Result\n2024-01-06 13:18:42 [main] DEBUG c.w.thread_.commonmethods.Join_ - result value: 1 In above code snippet, we invoke t1.join() method, which is a blocking method. main thread will wait here until t1 thread terminates State # The thread that is waiting for another thread to terminate is in State WAITING\nCode\nThread t2 invokes t1.join() and waits for t1 to terminate Check t2 state while it is blocked at t1.join() Thread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(50); log.debug(\u0026#34;t1 terminates\u0026#34;); } catch (InterruptedException e) { throw new RuntimeException(e); } }); t1.setName(\u0026#34;t1\u0026#34;); Thread t2 = new Thread(() -\u0026gt; { try { log.debug(\u0026#34;t2 is waiting for t1 terminating\u0026#34;); t1.join(); log.debug(\u0026#34;t2 terminates\u0026#34;); } catch (InterruptedException e) { throw new RuntimeException(e); } }); t2.setName(\u0026#34;t2\u0026#34;); t1.start(); t2.start(); Thread.sleep(20); log.debug(\u0026#34;t2 state: {}\u0026#34;, t2.getState()); Execution result\n2024-01-06 15:36:16 [t2] DEBUG c.w.thread_.commonmethods.Join_ - t2 is waiting for t1 terminating 2024-01-06 15:36:16 [main] DEBUG c.w.thread_.commonmethods.Join_ - t2 state: WAITING 2024-01-06 15:36:16 [t1] DEBUG c.w.thread_.commonmethods.Join_ - t1 terminates 2024-01-06 15:36:16 [t2] DEBUG c.w.thread_.commonmethods.Join_ - t2 terminates interrupt() # The interrupt() method is used to signal a thread that it should interrupt its current operation. This method is part of the Thread class. When interrupt() is called on a thread:\nIf the thread is executing a blocking operation (like sleep(), wait(), or join()), it immediately throws an InterruptedException, which can be caught and handled. This exception CLEARS the interrupted status of the thread. If the thread is NOT in a blocking operation, it doesn\u0026rsquo;t immediately cause an effect other than setting the thread\u0026rsquo;s interrupted status. Example # Interrupt a blocking operation\nThread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, Thread.currentThread().isInterrupted()); } }, \u0026#34;t1\u0026#34;); t1.start(); Thread.sleep(50); log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, t1.isInterrupted()); t1.interrupt(); log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, t1.isInterrupted()); 2024-01-06 21:58:27 [main] DEBUG c.w.t.c.InterruptBlockedThread - t1 is interrupted false 2024-01-06 21:58:27 [main] DEBUG c.w.t.c.InterruptBlockedThread - t1 is interrupted true 2024-01-06 21:58:27 [t1] DEBUG c.w.t.c.InterruptBlockedThread - t1 is interrupted false Before interrupting t1 thread, its interrupted status is false After invoking interrupt() method, the interrupted status is set to true immediately Thread t1 is interrupted while it is sleeping, and the interruption throws InterruptedException. The exception clears the interrupted status Interrupt a running operation\nThread t1 = new Thread(() -\u0026gt; { while(true) { if (Thread.currentThread().isInterrupted()) { break; } } }, \u0026#34;t1\u0026#34;); t1.start(); Thread.sleep(50); log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, t1.isInterrupted()); t1.interrupt(); log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, t1.isInterrupted()); 2024-01-06 22:02:18 [main] DEBUG c.w.t.c.InterruptRunningThread - t1 is interrupted false 2024-01-06 22:02:18 [main] DEBUG c.w.t.c.InterruptRunningThread - t1 is interrupted true If a thread is interrupted while it is running an operation. Only interrupted status is set to true. Hence interrupt() method is a polite way to signal the thread to stop, and still give the thread to handle some logic and clear some resources before terminating the thread Gracefully Terminate Threads # Code\n@Slf4j public class GracefullyTerminateThread { Thread thread; public GracefullyTerminateThread() {} public void start() { thread = new Thread(() -\u0026gt; { while(!Thread.currentThread().isInterrupted()) { try { log.debug(\u0026#34;Hello World\u0026#34;); Thread.sleep(1000); } catch (InterruptedException e) { log.debug(\u0026#34;Interrupt sleep\u0026#34;); Thread.currentThread().interrupt(); } } log.debug(\u0026#34;Thread gracefully terminates\u0026#34;); }); thread.start(); } public void gracefullyStop() { if (thread != null) { thread.interrupt(); } } } Flowchart\nThe thread continuously performs its task (logging and sleeping) in a loop, constantly checking for an interrupt signal. When gracefullyStop() is called from outside (another thread or method), it signals the thread to stop by setting its interrupted status. Upon receiving the interrupt signal, the thread exits its normal operation, performs any necessary clean-up, and then terminates. Execution\nTest code\npublic class GracefullyTerminate { public static void main(String[] args) throws InterruptedException { GracefullyTerminateThread thread = new GracefullyTerminateThread(); thread.start(); Thread.sleep(3000); thread.gracefullyStop(); } } Result\n2024-01-07 14:18:39 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Hello World 2024-01-07 14:18:40 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Hello World 2024-01-07 14:18:41 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Hello World 2024-01-07 14:18:42 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Interrupt sleep 2024-01-07 14:18:42 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Thread gracefully terminates "},{"id":19,"href":"/docs/programming/system-design/technology/redis/data-presistence/","title":"Data Persistence","section":"Redis","content":" Redis Data Persistence # Redis Data Persistence: How Does It Work? # Redis is widely known for its blazing-fast in-memory operations, but it also provides robust mechanisms for ensuring data persistence, making it suitable for a wide range of use cases. Let’s dive into how Redis achieves data persistence and ensures durability.\n1. Snapshotting (RDB - Redis Database File) # Snapshotting is a point-in-time persistence mechanism where Redis saves the entire dataset to disk at regular intervals.\nHow it Works # Redis creates a binary dump of the dataset in the form of an RDB file by forking a child process. The child process handles the snapshot creation entirely, ensuring minimal impact on runtime performance as the main thread remains free to serve client requests. Configuration # Snapshotting can be configured using the save directives in the Redis configuration file (redis.conf). For example:\nsave 60 1000 # Save a snapshot every 60 seconds if at least 1000 changes occurred Advantages # Quick Recovery: Snapshots provide a complete and consistent dataset, enabling fast recovery if the server crashes. Minimal Performance Impact: The child process handles I/O, leaving the main thread responsive. Disadvantages # Potential Data Loss: Changes made after the last snapshot will be lost if the server crashes. Resource Intensive: Copying the entire dataset can be I/O heavy, especially for large datasets. 2. Append-Only File (AOF) # The Append-Only File is a more granular persistence mechanism that logs every write operation to a file in a write-ahead manner, similar to a write-ahead log (WAL) in traditional databases. This ensures that data changes are saved incrementally and can be reconstructed reliably during a restart.\nHow it Works # Redis logs each write operation (e.g., SET, DEL) in an append-only format, which can be configured to log either every single write or at a specified interval. During recovery, Redis replays these logged operations to reconstruct the dataset. Configuration # Enable AOF persistence with the following in redis.conf:\nappendonly yes # Redis provides three options for AOF synchronization: - appendfsync always: Every write operation is immediately flushed to disk. This ensures maximum durability but may impact performance. - appendfsync everysec: Write operations are flushed to disk every second, balancing durability and performance. - appendfsync no: The OS decides when to flush data to disk, prioritizing performance over durability. Advantages # Higher Durability: Logs every operation, reducing potential data loss. Disadvantages # Performance Overhead: Logging every operation (e.g., appendfsync always) can introduce significant disk I/O and latency. Large File Sizes: AOF files are generally larger than RDB snapshots. AOF Rewriting: To address growing file sizes, Redis compacts the AOF file by rewriting it with the minimal set of commands needed to rebuild the current dataset. This process, while efficient, introduces additional disk I/O. Slow Recovery: Incremental recovery from AOF files can take longer compared to RDB snapshots, especially for large datasets, as Redis replays every logged operation to reconstruct the dataset. "},{"id":20,"href":"/docs/programming/network/dns-hands-on/","title":"DNS Hands On","section":"Network","content":" Introduction # In dns post, I introduced hierarchy of domain names and how browser queries IP address for a domain name from domain name servers. This post we will query domain name servers step by step to get the IP address of domain google.com. Also, I will introduce a new concept record in domain name server\nEnvironment and Tools # I will use CLI dig to query domain name server in Linux system\nRecords # Records are resourced managed by domain name servers. They are a mapping of domain name to its various type of information. You can think records as entries in the DB managed by domain name servers. Records have below important fields\nName # The domain name\nType # Type of the record. Here are some common record types\nA (Address Record): Maps a domain name directly to its IPv4 address AAAA (Quad Record): Maps a domain name directly to its IPv6 address CNAME (Canonical Name Record): Maps a domain name to another domain name. It is used to alias one domain name to another NS (Name Server Record): Map a domain name to a domain name server TTL (Time To Live) # TTL specifies the duration in seconds that the record may be cached by the resolver. After this time, the resolver must query the name server again to ensure it has the current record\nValue # Value varies depending on the record type\nA/AAAA record: it contains the IP address of the domain name points to CNAME record: the target domain to which it alias to Hands On # Query Root Domain . # Command\ndig NS . dig: CLI command to look up records in domain name server NS: Query NS records .: The domain name to query. Here is root domain The query is sent to the root name server, and the router will select the nearest physical name server and route the query to it.\nResponse\n;; ANSWER SECTION: . 0 IN NS f.root-servers.net. . 0 IN NS g.root-servers.net. . 0 IN NS h.root-servers.net. . 0 IN NS i.root-servers.net. . 0 IN NS j.root-servers.net. . 0 IN NS k.root-servers.net. . 0 IN NS l.root-servers.net. . 0 IN NS m.root-servers.net. . 0 IN NS a.root-servers.net. . 0 IN NS b.root-servers.net. . 0 IN NS c.root-servers.net. . 0 IN NS d.root-servers.net. . 0 IN NS e.root-servers.net. a.root-servers.net. 0 IN A 198.41.0.4 a.root-servers.net. 0 IN AAAA 2001:503:ba3e::2:30 b.root-servers.net. 0 IN A 170.247.170.2 b.root-servers.net. 0 IN AAAA 2801:1b8:10::b c.root-servers.net. 0 IN A 192.33.4.12 c.root-servers.net. 0 IN AAAA 2001:500:2::c d.root-servers.net. 0 IN A 199.7.91.13 d.root-servers.net. 0 IN AAAA 2001:500:2d::d e.root-servers.net. 0 IN A 192.203.230.10 e.root-servers.net. 0 IN AAAA 2001:500:a8::e f.root-servers.net. 0 IN A 192.5.5.241 f.root-servers.net. 0 IN AAAA 2001:500:2f::f g.root-servers.net. 0 IN A 192.112.36.4 The command returns 13 NS records of root domain servers and A/AAAA records of each root name server.\nRoot Name Server # There are 13 logical root name servers around the world, naming from a to m and with .root-servers.net. as suffix. However, it doesn\u0026rsquo;t mean there are only 13 physical servers. Actually each logical root name server has hundreds of physical servers with same IP address using anycast addressing. When the router receives client\u0026rsquo;s query to root servers, it will select the nearest physical server with the least latency or other network conditions\nQuery Name Server of com. # Command\ndig @a.root-servers.net. NS com. Here I pick one root name server a.root-servers.net. and query the NS record for domain name com.\nResponse\n;; AUTHORITY SECTION: com. 172800 IN NS e.gtld-servers.net. com. 172800 IN NS b.gtld-servers.net. com. 172800 IN NS j.gtld-servers.net. com. 172800 IN NS m.gtld-servers.net. com. 172800 IN NS i.gtld-servers.net. com. 172800 IN NS f.gtld-servers.net. com. 172800 IN NS a.gtld-servers.net. com. 172800 IN NS g.gtld-servers.net. com. 172800 IN NS h.gtld-servers.net. com. 172800 IN NS l.gtld-servers.net. com. 172800 IN NS k.gtld-servers.net. com. 172800 IN NS c.gtld-servers.net. com. 172800 IN NS d.gtld-servers.net. ;; ADDITIONAL SECTION: e.gtld-servers.net. 172800 IN A 192.12.94.30 e.gtld-servers.net. 172800 IN AAAA 2001:502:1ca1::30 b.gtld-servers.net. 172800 IN A 192.33.14.30 b.gtld-servers.net. 172800 IN AAAA 2001:503:231d::2:30 j.gtld-servers.net. 172800 IN A 192.48.79.30 j.gtld-servers.net. 172800 IN AAAA 2001:502:7094::30 m.gtld-servers.net. 172800 IN A 192.55.83.30 m.gtld-servers.net. 172800 IN AAAA 2001:501:b1f9::30 i.gtld-servers.net. 172800 IN A 192.43.172.30 i.gtld-servers.net. 172800 IN AAAA 2001:503:39c1::30 f.gtld-servers.net. 172800 IN A 192.35.51.30 f.gtld-servers.net. 172800 IN AAAA 2001:503:d414::30 a.gtld-servers.net. 172800 IN A 192.5.6.30 a.gtld-servers.net. 172800 IN AAAA 2001:503:a83e::2:30 g.gtld-servers.net. 172800 IN A 192.42.93.30 g.gtld-servers.net. 172800 IN AAAA 2001:503:eea3::30 h.gtld-servers.net. 172800 IN A 192.54.112.30 h.gtld-servers.net. 172800 IN AAAA 2001:502:8cc::30 l.gtld-servers.net. 172800 IN A 192.41.162.30 l.gtld-servers.net. 172800 IN AAAA 2001:500:d937::30 k.gtld-servers.net. 172800 IN A 192.52.178.30 k.gtld-servers.net. 172800 IN AAAA 2001:503:d2d::30 c.gtld-servers.net. 172800 IN A 192.26.92.30 The response contains two sections of answers:\nAuthority Section: This section lists the authoritative name servers for the .com TLD Additional Section: This section lists the IPv4 and IPv6 addresses for the authoritative TLD name servers Query Name Server of google.com # Command\ndig @e.gtld-servers.net. NS google.com Send the query to the TLD name server e.gtld-servers.net.\nResponse\n;; AUTHORITY SECTION: google.com. 172800 IN NS ns2.google.com. google.com. 172800 IN NS ns1.google.com. google.com. 172800 IN NS ns3.google.com. google.com. 172800 IN NS ns4.google.com. ;; ADDITIONAL SECTION: ns2.google.com. 172800 IN AAAA 2001:4860:4802:34::a ns2.google.com. 172800 IN A 216.239.34.10 ns1.google.com. 172800 IN AAAA 2001:4860:4802:32::a ns1.google.com. 172800 IN A 216.239.32.10 ns3.google.com. 172800 IN AAAA 2001:4860:4802:36::a ns3.google.com. 172800 IN A 216.239.36.10 ns4.google.com. 172800 IN AAAA 2001:4860:4802:38::a ns4.google.com. 172800 IN A 216.239.38.10 The response also contains two sections: the authoritative name servers google.com and their IP addresses. Those records are NS records for google.com name servers instead of web servers. To get the IP address of the web server, we need to make an extra query to the Google\u0026rsquo;s name server\nQuery IP Address of google.com Web Server # Command\ndig @ns2.google.com. google.com Response\n;; ANSWER SECTION: google.com. 300 IN A 142.250.217.78 The response only contains one A record which is the IPv4 address of google.com web server. You can type the IP address in your browser, and you will be routed to website www.google.com\n"},{"id":21,"href":"/docs/programming/project/node-js-auth/","title":"Node Js Auth","section":"Project","content":" Introduction # This project is for learning purpose. It is a practice of\nhow to use JWT (Json Web Token) to authenticate user login with Google using OAuth2 Tech Stack # Node.js Express: Quickly start a local host MongoDB/Mongoose: Database to store users @hapi/joi: Package to validate parameters of objects bcryptjs: Hash confidential information including passwords in this project jsonwebtoken: JWT package to sign and verify a auth token axios: Send HTTP requests querystring: Package to parse and assembly query string in HTTP request Reference # Implement JWT using node.js and express: Very nice video which guides me step by step to build this project Google OAuth2 with node.js: Video that explains and implements OAuth2 flow from end to end OAuth 2.0 and OpenID Connect: Plain English explains the evolution of OAuth2 and OpenID Connect What Is JWT? # A JWT (JSON Web Token) is like a compact digital note or a small piece of data that web servers and clients (like your browser or a mobile app) use to communicate secure information. It\u0026rsquo;s like a tiny, encoded message.\nStructure of JWT # A JWT is made up of three parts, each encoded in base64 and separated by dots (.):\nHeader: The header typically consists of two parts: the type of the token, which is JWT, and the signing algorithm being used, such as SHA256 or RSA. Payload: The payload contains information of the identity, like id, isa(issuedAt), and etc\u0026hellip; Signature: This is the secure part of JWT. It is created by taking encoded header, encoded payload, a secret only known by the server, then running it through the signing algorithm mentioned in the header. If the algorithm is Symmetric algorithm (like HS256): The same secret is used for signing and verification Asymmetric algorithm (like RS256): A private key will be used for signing, and a corresponding public key will be used for verification Risk of JWT # Data is Not Encrypted The first two parts of a JWT token are only base64 encoded, but not encrypted, so everyone can decode them and read data directly. Hence, confidential information should not be stored in the payload Susceptible to Theft If a JWT is stolen, it could by used by unauthorized parties to gain access to the system No Revocation Mechanism Once issued, a JWT can\u0026rsquo;t be revoked before it expires Key Management Challenges The security of JWT depends on how secret key is managed by the server. If the secret is not well managed or securely stored, it could lead security vulnerabilities Best Practice # Use HTTPS to prevent interception of tokens. Keep expiration times as short as practical. Avoid storing sensitive data in the token. Implement token refresh mechanisms. Securely manage signing keys. API # Register # Url: /api/user/register Method: POST Request Body { name: string, email: string, password: string } Response Body { userId: string } Business logic Validate parameters in the request body Validate email is not registered in DB Insert a new user in DB Login # Url: /api/user/login Method: POST Request Body { email: string, password: string } Response Header { auth-token: string } Body { token: string } Business logic Verify if email and password match Sign a new JWT and return it Posts # Url: /api/posts Method: GET Request Header { auth-token: string } Response Body { posts: { title: string, description: string } } Business logic Verify the auth-token from request headers Return a hard-coded post object After users login, server signs a JWT to users. By include the JWT in the request headers, server could verify it to determine if user is authenticated (login)\nOAuth2 # What is OAuth2? # OAuth2 is an open standard for access delegation, which is commonly used as the way for internet users to grant a client or application access to their resources under another server without sharing their passwords\nWhy OAuth2? # Secure Delegation of Access: Users often need to grant a third party website or application access to their data on another service (like accessing your google account from a social media app). Sharing credentials for this purpose is highly insecure\nFine Grained Authorization: User may not want to give full access of their all data\nStandardization and Interoperability: With requirements of authentication and authorization from many internet service, a standard way is needed\nReducing Password Fatigue: Users are overwhelmed by the need to create username and password for each single service\nKey Components of OAuth2 # Resource Owner: The user who authorizes an application to access their account Client: The application that wants to access user\u0026rsquo;s account Resource Server: The server hosting user\u0026rsquo;s data Authorization Server: The server that authenticates user and issues access token to the application How OAuth2 works # Authorization Request\nThe client requests authorization to access user\u0026rsquo;s resources. This is usually done through a redirection, where client passes along its identity (client id) and the scope of the access it\u0026rsquo;s requesting User Authenticate and Consent\nThe user is asked to login to the authorization server and to approve the requested access by the client Authorization Grant\nUpon successful authentication and consent, authorization server issues an authorization grant to the client. The authorization grant can be of different type, like an authorization code or an implicit grant, depending on the OAuth flow being used Access Token Request (In case of authorization code grant)\nIf an authorization code is granted, then client exchanges the code for an access token. This is done by sent a request to authorization server\u0026rsquo;s token endpoint where client authenticates itself and presents the authorization code Issuance of Access Token\nThe authorization server authenticates the client validates the authorization grant and if valid issues an access token (and possibly a refresh token) Accessing the Resource\nThe client uses access to token to make a request to the resource server for the protected resources Resource Server Validates Token\nThe resource server validates the token and if valid serves the request Resource Deliver\nThe client receives the protected resources Why OpenID Connect # OAuth2 was originally designed for authorization, but not for authentication. Different companies have their own standards to authenticate users using OAuth2, in other words, OAuth2 is overused for authentication.\nOpenID Connect is an authentication layer built on top of OAuth2. It was developed to address the need for a standardized authentication process.\nHow OpendID Connect works # Authentication Request The client includes openid in the scope of the authorization request. That indicates that the client is requesting an ID token in addition to access token Token Response Access Token: Just like OAuth2, access token is issued by the authorization server. This token grants access to user\u0026rsquo;s resources ID Token: This is unique to OpenID Connect. The ID token is a JWT which contains claims about user\u0026rsquo;s identity and profile Token Validation Upon receiving the tokens, the client must validate the ID tokens to ensure its integrity and authenticity. This involves verifying JWT signature and the claims it contains "},{"id":22,"href":"/docs/programming/backend/python/re/","title":"RegEx","section":"Python","content":" re in Python # re.search() # The re.search() method is used to search a string for a match to a regular expression pattern. It scans through the string from left to right and returns the FIRST match it finds. If a match is found, it returns a match object; otherwise, it returns None.\nMethod Signature # search(pattern: str, string: str, flags: int=0) -\u0026gt; re.Match pattern: The regular expression pattern to search for. string: The string to search within. flags: Optional flags to modify the behavior of the pattern (default is 0). Match Object Methods # If a match is found, re.search() returns a match object. This object provides several useful methods and attributes:\n.group() -\u0026gt; str: Returns the string matched by the regular expression. .start() -\u0026gt; int: Returns the starting position of the match. .end() -\u0026gt; int: Returns the ending position of the match. .span() -\u0026gt; turple(int): Returns a tuple containing the (start, end) positions of the match. Example # import re pattern = r\u0026#34;\\d+\u0026#34; input_string = \u0026#34;There are 123 numbers in 456 text\u0026#34; match = re.search(pattern, input_string) print(f\u0026#34;`re.search()` type -\u0026gt; {type(match)}\u0026#34;) if match: print(f\u0026#34;`match.group()` type -\u0026gt; {type(match.group())}\u0026#34;) print(f\u0026#34;`match.group()` value -\u0026gt; {match.group()}\u0026#34;) print(f\u0026#34;`match.start()` type -\u0026gt; {type(match.start())}\u0026#34;) print(f\u0026#34;`match.start()` value -\u0026gt; {match.start()}\u0026#34;) print(f\u0026#34;`match.span()` type -\u0026gt; {type(match.span())}\u0026#34;) print(f\u0026#34;`match.span()` value -\u0026gt; {match.span()}\u0026#34;) else: pass Output\n`re.search()` type -\u0026gt; \u0026lt;class \u0026#39;re.Match\u0026#39;\u0026gt; `match.group()` type -\u0026gt; \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; `match.group()` value -\u0026gt; 123 `match.start()` type -\u0026gt; \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; `match.start()` value -\u0026gt; 10 `match.span()` type -\u0026gt; \u0026lt;class \u0026#39;tuple\u0026#39;\u0026gt; `match.span()` value -\u0026gt; (10, 13) re.match() # The re.match() method is used to determine if the regular expression pattern matches AT THE BEGINNING of the string. If a match is found at the beginning of the string, it returns a match object. Otherwise, it returns None.\nMethod Signature # match(pattern: str, string: str, flags: int=0) -\u0026gt; re.Match Example # import re pattern = r\u0026#34;word\u0026#34; word_at_beginning = \u0026#34;word is at the beginning of the sentence\u0026#34; word_not_at_beginning = \u0026#34;The word is not at the beginning of the sentence\u0026#34; match = re.match(pattern, word_at_beginning) non_match = re.match(pattern, word_not_at_beginning) if match: print(f\u0026#34;Matching string -\u0026gt; {match.group()}\u0026#34;) else: pass if non_match: pass else: print(f\u0026#34;word is not found at the beginning\u0026#34;) Output\nMatching string -\u0026gt; word word is not found at the beginning re.fullmatch() # The re.fullmatch() method is used to check if the entire string matches a regular expression pattern. It differs from re.match() in that re.fullmatch() requires the whole string to match the pattern, not just a prefix. If the entire string matches the pattern, it returns a match object; otherwise, it returns None\nMethod Signature # fullmatch(pattern: str, string: str, flags: int=0) -\u0026gt; re.Match Example # import re pattern = \u0026#34;word\u0026#34; full_match_string = \u0026#34;word\u0026#34; non_full_match_string = \u0026#34;words\u0026#34; match = re.fullmatch(pattern, full_match_string) non_match = re.fullmatch(pattern, non_full_match_string) if match: print(\u0026#34;Found full match\u0026#34;) else: pass if non_match: pass else: print(\u0026#34;No full match\u0026#34;) Output\nFound full match No full match re.findall() # Method Signature # The re.findall() method is used to find all occurrences of a pattern in a string. It returns a list of all matches found. If no matches are found, it returns an empty list. Unlike re.search() and re.match(), which return match objects, re.findall() directly returns the matched strings.\nfindall(pattern: str, string: str, flags: int=0) -\u0026gt; list(str) Example # import re pattern = r\u0026#34;\\bword\\b\u0026#34; text = \u0026#34;word in a wordy word sentence with words\u0026#34; matches = re.findall(pattern, text) print(f\u0026#34;Type of `re.findall()` -\u0026gt; {type(matches)}\u0026#34;) print(f\u0026#34;Type of item in matches -\u0026gt; {type(matches[0])}\u0026#34;) print(\u0026#34;All matches found:\u0026#34;, matches) Output\nType of `re.findall()` -\u0026gt; \u0026lt;class \u0026#39;list\u0026#39;\u0026gt; Type of item in matches -\u0026gt; \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; All matches found: [\u0026#39;word\u0026#39;, \u0026#39;word\u0026#39;] "},{"id":23,"href":"/docs/programming/system-design/design-data-intensive-applications/replication/","title":"Replication","section":"Design Data Intensive Applications","content":" Definition # Replication means keeping a copy of the same data on multiple machines that are connected via a network.\nAdvantages of Replication # Reduce Latency: To keep data geographically close to your users Increase Availability: To allow the system to continue working even if some of its parts have failed Increase Read Throughput: To scale out the number of machines that can serve read queries Principles of Replication # Data Consistency: Each node that stores a copy of the database is called a replica. Every write to the database needs to be processed by every replica; otherwise, the replicas would no longer contain the same data (Inconsistency). Single Leader Mode # The most common solution for Data Consistency is called leader-based replication (also known as active/passive or master–slave replication)\nHow Does It Work? # One of the replicas is designated the leader which takes all write requests\nThe other replicas are known as followers. All data written to the leader should be replicated to all followers in same order\nBoth leader and followers can take read requests\nSynchronous Versus Asynchronous Replication # Synchronous Replication # The Replication to follower 1 is synchronous: the leader waits until follower 1 has confirmed that it received the write before reporting success to the user, and before making the write visible to other clients.\nAdvantage: The follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. When the leader node fails suddenly, we are sure that the latest data is available on the follower Disadvantage: If the synchronous follower doesn\u0026rsquo;t respond for whatever reason, the leader has to block all writes until the follower is available again Semi-Synchronous Replication # It\u0026rsquo;s not realistic to make all followers synchronous because any one node outage would cause the whole system to halt. Sometimes you make one follower synchronous and the others asynchronous. If the synchronous node fails, one of the asynchronous node is made synchronous. That ensures at least two nodes (the leader and the synchronous nodes) have update-to-date data. The configuration is sometimes called semi-synchronous\nAsynchronous Replication # The Replication to follower 2 is asynchronous: the leader sends the message, but doesn’t wait for a response from the follower\nAdvantage: The system can continue to work (allow writes) even though all followers have failed and fallen behind\nDisadvantage: If the leader node fails and is not recoverable, any writes that have not yet replicated to followers are lost. This means that a write is not guaranteed to be durable\nSetting up New Followers # Steps # Take a snapshot of the leader\u0026rsquo;s database at some point in time\nCopy the snapshot to the new follower node\nStep 1 and 2 are not sufficient, because while copying the snapshot, clients are still writing new data to the leader database. When the copy process is complete, the new follower database actually doesn\u0026rsquo;t catch up the leader node. Hence, further steps are needed\nThe follower connects to the leader and requests all the data changes that have happened since the snapshot was taken. This requires that the snapshot is associated with an exact position in the leader’s replication log.\nThe new follower node catches up when the new node has process all backlog of data changes since the snapshot\nHandling Node Outages # High availability system should be running as a whole despite individual node failures or maintenance.\nFollower Failure # On its disk, each follower should keep a log of data changes it has received and processed from the leader. If a follower crashes and is restarted, from its log it knows the last transaction it has processed before the fault happened. Thus, it can connect to the leader and request all data changes that occurred during the time when the follower was disconnected\nLeader Failure: Failover # When the leader node fails,\nOne of the followers need to be promoted to be the new leader Clients should be reconfigured to send writes to the new leader Followers need to consumer data from the new leader Steps # Determine that the leader has failed\nCommon method is to use a timeout: nodes bounces messages back and forth between each other. If a node doesn\u0026rsquo;t respond for some period of time \u0026ndash; for example, 30 seconds \u0026ndash; it is assumed to be dead\nChoosing a new leader\nThe best candidate for the leadership is the replica with most up-to-date data changes from the old leader to minimize any data loss\nReconfiguring the system to use the new leader\nClients now need to send writes to the new leader The system needs to ensure that the old leader to be a follower and recognize the new leader when it is recovered Things can go wrong # If asynchronous replication is used, new leader may not have all writes from the old leader before it failed. If the old leader rejoins, what should happen to those writes? It is hard to merge them into new leader because it may have received and processed conflicting writes. For example, new entries whose key was auto-incremented were inserted to the old leader database, and insertions were not received by the new leader, so when new entries are inserted to the new leader database, same keys would be used. The most common solution is to discard the conflicting and not synchronized data changes which compromises system durability, and it is dangerous.\nSplit brain: two nodes both believe that they are the leader. If both nodes accept writes and there is no process to resolve conflict, data is likely to be lost or corrupted\nTimeout trade-offs: A longer timeout means a longer time to recover in case the leader fails. Too short timeout may cause unnecessary failover. A temporary load spike or a network glitch could cause a node\u0026rsquo;s slow response\nImplementation of Replication Logs # Statement-Based Replication # The leader logs every write request (statement) that it executes and sends that statement log to its followers. INSERT, UPDATE and DELETE statements for a relational database\nProblem: Nondeterministic functions, like NOW() or RAND() are likely to generate different values in each replica Write-Ahead Log (WAL) shipping # Storage engines usually append every write to a log. For example in B-Trees, every modification is written to a write-ahead log (WAL). The database then applies the operation from WAL to the B-Tree structure database. That ensures durability: changes are recoverable in case of a crash\nThe log is an append-only sequence of bytes containing all writes to the database. Besides writing the log to disk, the leader can also send the log to followers. When followers apply the log, it builds a copy of exact data structure as found on the leader\nShortage: Backward compatible\nFor WAL, the storage engine needs an internal conversion from the log to the actual modification of the database. The conversion process may not be backward compatible, in other words, different versions of storage engine may convert same log to different data changes on the database. If leader and followers are using different versions of software after maintenance, that may cause problem\nLogical (row-based) log replication # Logical log represents the data change of a write operation. Use relational database as example:\nFor an inserted row: the log contains new values of all columns For a deleted row: the log contains all necessary information \u0026ndash; for example, primary key \u0026ndash; to identify the deleted row For an updated row: the log contains enough information to identify the updated row, and new values of all columns or columns that have been modified A transaction that modifies several rows generates several such log records, followed by a record indicating that the transaction was committed.\nLogical log contains information of data that has been changed, so this technique is called change data capture. The advantage of logical log is that it is decoupled from the storage engine, so it\u0026rsquo;s more easy to keep it backward compatible. Also, it is easy for external applications to parse\nReplication Lag # Replication lag is the delay between a write happening on the leader and being reflected on a follower\nProblem of Replication Lag # Leader based replication allows all writes go through a single node and clients can read from any nodes in the system. If workloads in your application consist of mostly reads and only small percentage of write, then this is an attractive option. You can simply add more followers to server more reads (Scalability) w/o increasing latency.\nIn that case, it is not realistic to use synchronous replication because any node of failure can make the entire system unavailable for writing. Hence, the majority of followers should be asynchronous. That brings another problem, some followers that have fallen behind the leader (large replication lag) would respond inconsistent values to clients. If you stop writes, after some time, those followers would catch up leader. That is called eventually consistent.\nSolution 1: Read Your Writes # This is a guarantee that user would always see any updates they submitted, but it doesn\u0026rsquo;t make any promise about others\nHow to achieve?\nRead from leader for data that can only be modified by the user, like user\u0026rsquo;s profile, or user\u0026rsquo;s posts. If most things in the application are potentially modified by the user, that approach won\u0026rsquo;t work\nThe client can remember the timestamp for its most recent write. If the replica is not up-to-date, the query can be handled by another replica or wait until the replica has caught up. The timestamp could be logical, like log sequence number or version of the queried entry\nProblem of multiple devices read-your-write\nApproaches that require remembering the timestamp are more difficulty because one device doesn\u0026rsquo;t know the updates made on other devices. Some possible solutions:\nServer pushes timestamps of updates from any devices to all other connected devices Solution 2: Monotonic Reads # Monotonic reads is a guarantee that below kind of anomaly does not happen. It’s a lesser guarantee than strong consistency, but a stronger guarantee than eventual consistency.\nAnomaly\nUser can read things moving backward in time. That can happen when user first read from replica with little lag and then read from replica with greater lag\nThe first query returns a comment that was recently added by user 1234, but the second query doesn’t return anything because the lagging follower has not yet picked up that write\nAchieving monotonic reads\nOne possible way to achieve monotonic reads is to make sure all reads from one user are made to same replica (consistent hashing)\nMultiple Leader Replication # Multiple-leader replication allows more than one node to accept writes. A node that processes a write must forward the data change to ALL other nodes (Each leader in this model is simultaneously follower of other leaders)\nUse Cases for Multi-Leader Replication # Multi-datacenter operation # Within each datacenter, regular leader–follower replication is used; between datacenters, each datacenter’s leader replicates its changes to the leaders in other datacenters.\nComparison between single-leader and multiple-leader configuration in a multiple-datacenter deployment\nPerformance\nIn a single-leader configuration, every write must go over the internet to the datacenter with the leader. That can add significant latency to writes from other datacenters\nIn a multiple-leader configuration, every write can be processed in the local datacenter and asynchronously replicated to other data centers. The inner-datacenter delay is hidden from users\nTolerance of datacenter outage\nIn a single-leader configuration, if the datacenter with the leader fails, failover can promote follower from other datacenters to be the new leader\nIn a multiple-leader configuration, every datacenter works independently. If one datacenter fails, traffic can be handled by other datacenters\nTolerance of network outage\nWrites from other datacenters need to go through the public network and to be handled by the leader synchronously. Thus, A single-leader configuration is very sensitive to the problem of inter-datacenter network, which may be less reliable than local datacenter network\nA multiple-leader configuration with asynchronous replication tolerates network problem better. A temporary network issue won\u0026rsquo;t affect writes to be processed\nClients with Offline Operations # Assume you have a calendar app on you mobile phone, you are allowed to add meetings even though the app is offline. Not only on the mobile phone, you can also make changes in the calendar app on your laptop. When the app is online, the changes that you made should be synced to other devices\nAt an architectural point of view. The setup is same as multiple-leader configuration. The device is a \u0026ldquo;datacenter\u0026rdquo;, and the network between datacenters is not reliable (devices could be offline). Clients can still write to the datacenter when network outage happens\nCollaborative Editing # Real-time collaborative editing applications, like Google Docs, allow users to update same document simultaneously. It\u0026rsquo;s not a real multiple-leader datacenter problem, but they have a lot of commons\nOne way to achieve collaborative editing is to lock the document when any user is editing the file. Other users can only edit the file when previous user commits the change and the lock is released. This setup is like one-leader configuration, and only one client can write the database simultaneously\nFor multiple-leader configuration like setup, the application allows multiple users to edit simultaneously\nHandling Write Conflicts # The biggest problem with multi-leader replication is that write conflicts. For example, a document is edited by two users simultaneously on Google Drive. User one changes the title from A to B, and user two changes the title from A to C. Each user\u0026rsquo;s changes is processed by its local leader. However, when the changes are asynchronously replicated, a conflict is detected\nApproach 1: Synchronous Versus Asynchronous Conflict Detection # You can configure replication between datacenters synchronous \u0026ndash; i.e., wait for the write to be replicated to all replicas before telling the user the write was successful. However, by doing so, you will lose the main advantage of multiple-leader replication: allow each replica to accept writes independently\nApproach 2: Conflict Avoidance # You can force all writes to a record are processed by one leader, so the write conflict won\u0026rsquo;t occur.\nFor example, in applications that user can edit their own data, like user profiles, you can ensure that requests from a specific user are always routed to the leader in same datacenter for reads and writes. From user\u0026rsquo;s perspective, the configuration is single-leader\nProblems\nThis approach won\u0026rsquo;t work if the datacenter accepting writes fails and all requests are re-routed to other datacenters, or user has moved to a location which is closer to another datacenter\nApproach 3: Converging to A Consistent State # We don\u0026rsquo;t worry about wirte conflicts in single-leader database because all writes are in sequential order because of write lock. When multiple writes are replicated to followers, the last write should have the final value. This doesn\u0026rsquo;t apply to multiple-leader database because writes are made concurrently and independently in different datacenters. Thus, we need an approach to determine the convergent value when write conflicts happen\nGive each write a version ID, and write with the highest version wins\nPrioritize replica, write from replica with higher priority wins\nProblems\nBoth approaches may cause data loss\nMultiple-Leader Topologies # A replication topology describes the communication paths along which writes are propagated from one node to another\nCircular topology: Each node receives writes from one node and forwards those writes (plus any writes of its own) to one other node.\nPreventing infinite loop: Every write should be tagged with the identifier of all nodes that it has passed through Problem: A single node failure can interrupt the flow of replication. For example, the write flow is A -\u0026gt; B -\u0026gt; C -\u0026gt; D -\u0026gt; A, if node C fails, then writes from node A and B can\u0026rsquo;t be propagated to node D Star topology: A designated root node accepts write replication from other leaders and forwards the writes to all other leaders\nSingle point of failure: Root node failure would interrupt the replication to all other leaders All-to-all topology: Every leader sends it writes to all other leaders\nReverse causal order: Some network links are faster than others, write replication 2 that depends on write replication 1 may arrive at one node earlier Client A inserts a row into a table on leader 1, and client B updates that row on leader 3. However, leader 2 may receive the update first, which doesn\u0026rsquo;t make sense because it\u0026rsquo;s an update on a row that doesn\u0026rsquo;t exist in the database\nLeaderless Replication # Storage system that abandons the concept of a leader and allows any replica to directly accept writes from clients\nWriting to The Database When One Node Is Down # Imagine you have a database with three replicas, and one node is down. There is no failover in leaderless database because of no leader node.\nWrite\nLet\u0026rsquo;s see what will happen when one node is down. Client sends the write to all replicas in parallel, and two nodes accept the write and one node that is down misses it. It\u0026rsquo;s sufficient for two out of three replicas to acknowledge the write. After client receives two ok responses, we consider the write is successful.\nRead\nNow imagine that the unavailable node comes back online, and clients start reading from it. Any writes that happened while the node was down are missing from that node. To solve this problem, when client reads, it sends the request to several replicas. Client will receive the up-to-date data from one node and stale data from another. Version number will determine which value is newer.\nSolutions of Replication Lag # Read repair\nWhen a client makes a read request to several nodes in parallel, the client detects the stale data and write new value to the replica with stale data. This approach works well for data that is frequently read\nAnti-entropy process\nIn addition, some datastores have a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another.\nQuorums for reading and writing # In the example (writing to the database when one node is down), if we know that every successful write is guaranteed to be present on at least two out of three replicas, that means at most one replica can be stale. Thus, if we read from at least two replicas, we can be sure that at least one of the two is up-to-date. If the third replica is down or slow to respond, reads can nevertheless continue returning an up-to-date value.\nGeneral condition\nIf there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. (In our example, n = 3, w = 2, r = 2.) As long as w + r \u0026gt; n, we expect to get an up-to-date value when reading, because at least one of the r nodes we’re reading from must be up-to-date. Reads and writes that obey these r and w values are called quorum reads and writes\nNormally, reads and writes are always sent to all n replicas in parallel. The parameters w and r determine how many nodes we wait for—i.e., how many of the n nodes need to report success before we consider the read or write to be successful.\nLimitations\nWhen the quorum condition is fulfilled, you always expect to read the most up-to-date data. That is the case because the set of nodes to which you have written and the set of node from which you have read must overlap. However, even with w + r \u0026gt; n, there are likely to be edge cases where stale values are returned.\nSloppy quorum\nConcurrent read and write, the new value haven\u0026rsquo;t been reflected on some replicas\nConsider a system with 5 replicas, with a write quorum of 3 and a read quorum of 3. Imagine a scenario where a write operation updates replicas 1, 2 and 3, and a read operation queries replica 3, 4, and 5. If replica 3 has not yet finished updating while it is queried, it might still return the stale value\nIf a node carrying a new value fails, and its data is restored from a replica carrying an old value, the number of replicas carrying the new value may fall below w\nA write succeeded on fewer than w nodes, and the write is not rolled back on those succeeded replicas. That means the write is reported as failed, but the consequent read may return the new value (Old value is supposed to be returned because the new value is from a failed write)\nSloppy Quorum \u0026amp; Hinted Handoff # For some quorum configurations, there are some designated nodes to accept writes for a specific key. If some designated node fail, sloppy quorum allows other nodes to accept those writes and respond success to clients. When the failed designated nodes are available, values updated will be transferred from sloppy replicas to the designated replicas. This is call *hinted handoff\nAlthough condition w + r \u0026gt; n is still fulfilled in sloppy quorum, in some cases, stale values will be returned to clients. For example, a read operation happens before hinted handoff completes, and it contacts a set of nodes that don\u0026rsquo;t have the new value. Thus, sloppy quorum achieves high availability, which allows client to write when some designated nodes fail, by compromising consistency (eventual consistency)\nConcurrent Writes # Two clients update the same data with different values concurrently (write #1 and write #2). One node receives two writes in order write #1 and write #2, but another node receives same writes in reverse order. If each node just overwrites the data with latest write, then values of same data in different nodes are inconsistent. In order to become eventually consistent, the replicas should converge to the same value.\nLast write wins (discarding concurrent write)\nOne approach is to declare that each replica only store the most \u0026ldquo;recent\u0026rdquo; value and allow the \u0026ldquo;older\u0026rdquo; values to be overwritten and discarded. Then we need a way of unambiguously determining which write is more \u0026ldquo;recent\u0026rdquo;. We can use timestamp, as long as the two writes don\u0026rsquo;t happen at exactly same time, we can determine which one is more \u0026ldquo;recent\u0026rdquo; even with \u0026ldquo;0.000001s\u0026rdquo; difference in time. However, order in concurrently writes makes no sense, because the two writes don\u0026rsquo;t know and depend on each other.\nFor example, imagine one item is in the cart of someone\u0026rsquo;s Amazon account. One person increments the number of the item by 1, and at same time another person removes the item from the cart in another device. Two write operations update the item concurrently, and both don\u0026rsquo;t know each other. Then which one should win, 0 or 2? Use timestamp can achieve eventual consistency, but may confuse clients because both writes responded success, but one write is implicitly discarded\n"},{"id":24,"href":"/docs/programming/aws/message/sns/","title":"SNS","section":"Message","content":" Architecture # Pub-Sub Model # Publisher # Service or application that sends message to SNS Topic. SNS Topic allows multiple message publisher\nSNS Topic # SNS uses topics to logically separate messages into channels\nFanout The Fanout scenario is when a message published to an SNS topic is replicated and pushed to multiple endpoints, such as Kinesis Data Firehose delivery streams, Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing.\nFilter Policy After subscribing to an SNS topic, subscribers will receive all messages published by the topic. To receive a subset of messages, a subscriber must assign a filter policy to the topic subscription.\nRetry Policy Retry Policy is a JSON Object which allows SNS Topic to re-publish message to subscribers when message delivery fails\nDLQ You can set up an SQS DLQ (Dead Letter Queue) to receive messages failed to be delivered after all retries\nResource Policy It\u0026rsquo;s a policy attached to SNS Topic which defines WHO (Primary) can do WHAT (Actions) on WHICH resource\nSubscriber # To receive messages from an SNS topic, applications or receives should subscribe to the SNS topic. After subscription, every message sent to the SNS topic will be replicated and pushed to all subscribers\nSNS V.S. SQS # Push V.S. Pull # SNS topics push message to subscribers. SQS queues retain messages, and consumers need to pull messages from the queue\nMessage Retention # An SQS message is stored on the queue for up to 14 days until it is successfully processed by a consumer. SNS does not retain messages so if there are no subscribers for a topic, the message is discarded.\nMessage Duplication # SNS topics duplicate messages and fan out them to all subscribers. Although SQS queues can have multiple consumers, but one message is processed by only one consumer. The message goes back to the queue if consumers doesn\u0026rsquo;t delete the message before invisible timeout runs out\nMessage Processing # SNS only concerns with message delivery. Apart from message delivery, SQS also cares if the message is processed properly by the consumer. The messages are deleted by the consumer after message processing is finished\n"},{"id":25,"href":"/docs/programming/aws/message/sqs/","title":"SQS","section":"Message","content":" Architecture of SQS System # Three Main Components # Producers\nProducers in above system are the identities who send message to SQS queue\nSQS Queue\nSQS Queue is a buffer that stores messages and decouples producers and consumers in the system\nConsumers\nConsumers are identities in the system poll message from SQS queue\nHow to Use SQS Queue # Producer sends message\nProducer sens message to SQS Queue, and the message will be visible to all consumers\nMessage state - Visible: The message is able to be polled by any consumers. The message is in visible state when Message is sent to the queue and not pulled by any consumers Message is not deleted before the visibility timeout runs out Visibility timeout: The time that a message stays in the queue but is hidden to all consumers. Message is supposed to be polled and processed by only one consumer at a time. Hence, once a consumer polls the message, it should not be visible to all consumers Consumer polls message\nConsumer polls messages (up to 10 in one batch) from the queue and processes the messages. The message will be invisible until 1. Consumer who polls the message completes message processing and deletes the message 2. Or consumer doesn\u0026rsquo;t delete the message before visibility timeout runs out, and the message becomes visible again\nMessage state - Invisible: The message can\u0026rsquo;t be polled by any consumers. Consumer deletes message\nConsumer completes message processing and deletes the message from the queue\nSQS Encryption # SQS Access Policy # Policy that is attached to a SQS queue, defines WHO (the identity) can perform WHAT actions (APIs that are allowed) on this SQS queue\nCross Account Access Policy # { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;Account-B-ID\u0026gt;:root\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;SQS:SendMessage\u0026#34;, \u0026#34;SQS:ReceiveMessage\u0026#34;, \u0026#34;SQS:DeleteMessage\u0026#34;, \u0026#34;SQS:GetQueueAttributes\u0026#34;, \u0026#34;SQS:GetQueueUrl\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:\u0026lt;region\u0026gt;:\u0026lt;Account-A-ID\u0026gt;:\u0026lt;queue-name\u0026gt;\u0026#34; } ] } The access policy allows Account-B to access SQS queue in Account-A SQS Standard Queue # Attributes # Unlimited throughput and number of messages in queue Default retention of message: 4 days. Max: 14 days Low latency Limitation of 256 KB per message Can have duplicate messages Could be out of order DLQ (Dead Letter Queue) # If consumer doesn\u0026rsquo;t delete the message from the queue before visibility timeout runs out, the message will be visible again and receive count for that message increments.\nWe can set up a threshold of how many times a message could go back to the queue. If MaximumReceives threshold is exceeded, the message goes into a dead letter queue\nDLQ is mainly for debugging purpose. It also supports to re-drive failing messages which sends messages from DLQ to SQS queue\n"},{"id":26,"href":"/docs/programming/web/security/tls-handshake/","title":"TLS Handshake","section":"Security","content":" TLS/SSL and HTTPS # TLS (Transport Layer Security) and its predecessor, SSL (Secure Sockets Layer), are integral to HTTPS, which stands for Hypertext Transfer Protocol Secure. HTTPS is the secure version of HTTP, the primary protocol used for transmitting web pages over the internet. This post will introduce the process of TLS handshake, and the TLS version is TLS 1.2\nTLS Handshake # Client to Server: ClientHello\nThe client starts the handshake by sending a ClientHello message. This message includes\nthe TLS version the client supports, a list of cryptographic algorithms (cipher suites) it can use, and a random byte string (client random) for key generation purposes. Server to Client: ServerHello The server responds with a ServerHello message. This message indicates the chosen TLS protocol version (compatible with the client\u0026rsquo;s version), selects a cipher suite from the client\u0026rsquo;s list, and includes a server-generated random byte string (server random).\nServer to Client: Certificate The server sends its digital certificate to the client. This certificate, typically issued by a trusted Certificate Authority (CA), contains the server\u0026rsquo;s public key and is used to authenticate the server to the client.\nServer to Client: Server Key Exchange This message is sent by the server only if the server\u0026rsquo;s digital certificate does not contain enough data to allow the client to exchange a pre-master secret. It provides cryptographic parameters necessary for the client to establish the pre-master secret.\nServer to Client: Server Hello Done This message indicates the end of the ServerHello and associated messages. After sending this message, the server waits for the client\u0026rsquo;s response.\nClient to Server: Client Key Exchange, Change Cipher Spec, Encrypted Handshake Message\nClient Key Exchange: The client responds with key exchange data, which typically includes a pre-master secret encrypted with the server\u0026rsquo;s public key. Change Cipher Spec: The client then sends a message indicating that subsequent messages from the client will be encrypted using the newly agreed cipher suite and keys. Encrypted Handshake Message: This is a \u0026lsquo;Finished\u0026rsquo; message encrypted with the new encryption settings. It provides a cryptographic check that ensures the integrity of the handshake thus far. 7. Server to Client: Encrypted Handshake Message\nThe server decrypts the client\u0026rsquo;s \u0026lsquo;Finished\u0026rsquo; message and verifies it. Then, it sends its own \u0026lsquo;Finished\u0026rsquo; message, encrypted with the agreed cipher suite and keys. This message serves to confirm that the server part of the handshake is complete and that the server will begin to use the new security settings for all subsequent messages.\nSymmetric Encryption in TLS Handshake # Establishing a Session Key: After the asymmetric exchange of the pre-master secret, both the client and the server independently compute the master secret from the pre-master secret and the random numbers exchanged in the ClientHello and ServerHello messages.\nGeneration of Session Keys: From the master secret, both parties derive a set of symmetric session keys using a secure key derivation function. These keys include separate encryption and MAC (Message Authentication Code) keys for both client-to-server and server-to-client communication.\nData Encryption: Once the symmetric session keys are established, all subsequent data transmitted during the TLS session is encrypted using symmetric encryption. This includes the final part of the handshake, where both the client and server exchange \u0026lsquo;Finished\u0026rsquo; messages encrypted with the symmetric keys.\nReference # Bilibili Video\n"},{"id":27,"href":"/docs/programming/backend/java/thread/thread-lifecycle/","title":"Thread Lifecycle","section":"Thread","content":" Six Java Thread States # NEW State:\nWhen you create an instance of a Thread class (or a class that extends Thread), the thread is in the New state. At this point, the thread is not yet running. Example: Thread t = new Thread(); RUNNABLE State:\nWhen you invoke the start() method, the thread moves to the RUNNABLE state In a typical implementation, a Java thread in the RUNNABLE state corresponds to an OS thread that is eligible for running A RUNNABLE state means the thread is either running on the CPU or waiting for Scheduler to allocate CPU resource BLOCKED State:\nA thread enters the BLOCKED state when it tries to acquire a monitor lock (intrinsic lock) of an object but another thread already holds that lock. This commonly occurs in synchronized blocks or methods. WAITING State:\nA thread is in the WAITING state when it is waiting for another thread to perform a specific action. Unlike the BLOCKED state, where a thread is waiting to acquire a lock, in the WAITING state, a thread is waiting for another thread\u0026rsquo;s action without any lock competition How WAITING state occurs Object.wait() WITHOUT timeout Thread.join() WITHOUT timeout TIMED_WAITING State:\nA thread is in this state when it is waiting for another thread to perform an action for up to a specified waiting time. After the time expires, the thread will automatically return to the RUNNABLE state if it\u0026rsquo;s not blocked by other means How TIMED_WAITING state occurs Thread.sleep(long millis) Object.wait WITH timeout Thread.join WITH timeout TERMINATED State:\nA thread enters the TERMINATED state when it completes the execution of its run() method or if an exception occurs. Once terminated, a thread cannot be restarted. State Transition # Demo # Code\n@Slf4j public class SixStates { public static void main(String[] args) { Thread newThread = new Thread(() -\u0026gt; { log.debug(\u0026#34;Start a new thread\u0026#34;); }, \u0026#34;NEW-thread\u0026#34;); Thread runnableThread = new Thread(() -\u0026gt; { while(true) { } }, \u0026#34;RUNNABLE-thread\u0026#34;); runnableThread.start(); Thread timedWaitingThread = new Thread(() -\u0026gt; { try { Thread.sleep(1_000_000); } catch (InterruptedException e) { throw new RuntimeException(e); } }, \u0026#34;TIMED_WAITING-thread\u0026#34;); timedWaitingThread.start(); Thread waitingThread = new Thread(() -\u0026gt; { try { synchronized (SixStates.class) { runnableThread.join(); } } catch (InterruptedException e) { throw new RuntimeException(e); } }, \u0026#34;WAITING-thread\u0026#34;); waitingThread.start(); Thread blockedThread = new Thread(() -\u0026gt; { synchronized (SixStates.class) { log.debug(\u0026#34;BLOCKED thread\u0026#34;); } }, \u0026#34;BLOCKED-thread\u0026#34;); blockedThread.start(); Thread terminatedThread = new Thread(() -\u0026gt; { }, \u0026#34;TERMINATED-thread\u0026#34;); terminatedThread.start(); log.debug(\u0026#34;{}\u0026#34;, newThread.getState()); log.debug(\u0026#34;{}\u0026#34;, runnableThread.getState()); log.debug(\u0026#34;{}\u0026#34;, timedWaitingThread.getState()); log.debug(\u0026#34;{}\u0026#34;, waitingThread.getState()); log.debug(\u0026#34;{}\u0026#34;, blockedThread.getState()); log.debug(\u0026#34;{}\u0026#34;, terminatedThread.getState()); } } Execution 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - NEW 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - RUNNABLE 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - TIMED_WAITING 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - WAITING 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - BLOCKED 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - TERMINATED "},{"id":28,"href":"/docs/programming/aws/message/eventbridge/","title":"EventBridge","section":"Message","content":" What is AWS EventBridge? # AWS EventBridge is a serverless event bus service that is used to build event driven applications. EventBridge allows you to ingest, filter, transform and deliver events from sources to targets.\nReference # AWS EventBridge Workshop EventBridge Global Endpoint Message Services for Serverless Applications Architecture # Components and Concepts # Data Sources # Data sources or event publishers are applications that publish data to the EventBridge. They could be your own applications, Software-as-a-Service (SaaS) applications and AWS services. Data sources call PutEvent API to publish events to an event bus in the EventBridge.\nAWS services: EventBridge catches all AWS service events, but to utilize them, you need to create a rule to route event of interests to target Event Buses # Event buses receive events. They are data channels in EventBridge, and you can think them as topics in SNS. When applications publish events to EventBridge, it must specify the event bus name.\nRules # A Rule matches incoming events and routes them to targets for processing. A single rule can route matching events to multiple targets. Rules should be attached to an event bus.\nExample # EC2 Event\n{ \u0026#34;version\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;12345678-1234-1234-1234-123456789012\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;EC2 Instance State-change Notification\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;aws.ec2\u0026#34;, \u0026#34;account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2024-02-10T21:22:00Z\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-west-2\u0026#34;, \u0026#34;resources\u0026#34;: [ \u0026#34;arn:aws:ec2:us-west-2:123456789012:instance/i-1234567890abcdef0\u0026#34; ], \u0026#34;detail\u0026#34;: { \u0026#34;instance-id\u0026#34;: \u0026#34;i-1234567890abcdef0\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34; } } Rule to SQS Queue\nEC2StateChangeRule: Type: AWS::Events::Rule Properties: Description: \u0026#34;EventBridge rule to route EC2 state change events to SQS with transformation\u0026#34; EventBusName: default EventPattern: source: - \u0026#34;aws.ec2\u0026#34; detail-type: - \u0026#34;EC2 Instance State-change Notification\u0026#34; detail: state: - \u0026#34;pending\u0026#34; - \u0026#34;running\u0026#34; - \u0026#34;stopped\u0026#34; - \u0026#34;terminated\u0026#34; Targets: - Id: \u0026#34;TargetSQS\u0026#34; Arn: !GetAtt MySQSQueue.Arn InputTransformer: InputPathsMap: instanceId: \u0026#34;$.detail.instance-id\u0026#34; state: \u0026#34;$.detail.state\u0026#34; InputTemplate: \u0026#39;{\u0026#34;instanceId\u0026#34;: \u0026lt;instanceId\u0026gt;, \u0026#34;state\u0026#34;: \u0026lt;state\u0026gt;}\u0026#39; Important Features # Event Pattern # Only events matching the event pattern will be delivered to targets\nInput Transformer # Input transformer is a feature that can transform source event to the format you desire and pass the transformed event to target\nArchive and Replay # Archive events. Replay allows you to resend events from archive to event bus\nTo create an archive for an event bus, here are some key attributes you need to specify\nEvent Source: Event bus that you want to archive events Retention Period: Time that events will be retained in archive Event Pattern: Only events matching the event pattern will be archived Schema Registry # EventBridge can infer schemas based on events routed through an event bus by using schema discovery. This can be used to generate code bindings directly to your IDE for type-safe languages like Python, Java, and TypeScript. This can help accelerate development by automating the generation of classes and code directly from events.\nGlobal Endpoint # Global Endpoint allows you to publish events to event bus in secondary region when the event bus in primary region is down. It improves availability and reliability of your event-driven applications.\nServices Involved # Route53 Health check event bus in primary region and emit health metrics to CloudWatch Route PutEvent requests to event bus in primary or secondary region CloudWatch Alarm Once health check alarm is over threshold, the alarm notifies Route53 that primary event bus is down Event buses When primary event bus is healthy, event is routed to primary event bus and replicated to secondary event bus and vise verse when primary event bus is down How does it work # Publisher calls PutEvent to send events to EventBridge global endpoint hosted in Route53 The Event is routed to primary event bus and optionally replicated to secondary event bus in real time depending on your setup Once health check metrics are over threshold in CloudWatch alarm, route53 is notified, and it starts failover which routes all events to secondary event bus until the alarm is inactive. You have multiple options when events are routed to secondary event bus Implement same process solution to process the event Archive all events routed to secondary event bus and replay them to primary event bus once it is healthy again Loss of Global Endpoint # More costs Event duplication All fields are same for original and replicated events except event id. Consumers should be designed with idempotency (de-duplication). SNS v.s. EventBridge # Commons # They are important components in event driver application to decouple data sources and data consumers Push data to consumers Unlike SQS queues, they don\u0026rsquo;t retain data They can have multiple data sources and consumers Support retry and DLQ Similar components SNS Topic - Event Bus Subscription with filter policy - Rule Differences # EventBridge is integrated with SaaS which allows them to put events directly to event buses EventBridge allows schema registry "},{"id":29,"href":"/docs/programming/backend/python/format-string/","title":"format-string","section":"Python","content":" Format String in Python # f-strings # f-strings provide a concise and readable way to embed expressions inside string literals. They are prefixed with the letter f or F and use curly braces {} as placeholder for expressions\nQuick Start\nname = \u0026#34;Alice\u0026#34; age = 30 greeting = f\u0026#34;Hello, my name is {name} and I am {age} years old\u0026#34; print(greeting) Output\nHello, my name is Alice and I am 30 years old Usage # Expressions\nresult = f\u0026#34;The sum of 2 and 3 is {2 + 3}.\u0026#34; print(result) Output\nThe sum of 2 and 3 is 5. Functions\ndef greet(name): return f\u0026#34;Hello, {name}!\u0026#34; message = f\u0026#34;{greet(\u0026#39;Alice\u0026#39;)}\u0026#34; print(message) Output\nHello, Alice! .format() # .format() is a method of str. It uses curly braces {} as placeholders within the string, which are replaced by the values passed to the format method.\nQuick View\nname = \u0026#34;Alice\u0026#34; age = 30 greeting = \u0026#34;Hello, my name is {} and I am {} years old.\u0026#34;.format(name, age) print(greeting) Output\nHello, my name is Alice and I am 30 years old. Usages # Positional Arguments: Values are inserted into the placeholders in the order they appear. Values can be used more than once.\nstring = \u0026#34;{0} is {1} years old. {0} wants to eat banana\u0026#34;.format(\u0026#34;Tom\u0026#34;, 30) print(string) Output\nTom is 30 years old. Tom wants to eat banana Named Arguments: You can use named placeholders and pass values using keyword. Values can be reused also.\nstring = \u0026#34;{name} is {age} years old. {name} wants to eat banana\u0026#34;.format(name=\u0026#34;Tom\u0026#34;, age=30) print(string) Output\nTom is 30 years old. Tom wants to eat banana Modifiers # Modifiers in Python string formatting allow you to control the way your data is presented in strings. Both f-strings and the .format() method support a variety of modifiers for formatting numbers, strings, dates, and more.\nCommon Modifiers\nWidth and Alignment Precision Type Specifiers Width and Alignment # These modifiers control the width of the field and the alignment of the content within the field.\nSyntax\n{value:width} # width syntax {value:\u0026lt;} # left alignment {value:\u0026gt;} # right alignment {value:^} # center alignment Example\nAlignment modifier :\u0026lt; need to be used before width :width\nprint(f\u0026#34;{\u0026#39;Alice\u0026#39;:\u0026lt;10}\u0026#34;) # \u0026#34;Alice \u0026#34; print(f\u0026#34;{\u0026#39;Alice\u0026#39;:\u0026gt;10}\u0026#34;) # \u0026#34; Alice\u0026#34; print(f\u0026#34;{\u0026#39;Alice\u0026#39;:^10}\u0026#34;) # \u0026#34; Alice \u0026#34; Precision # Precision is mainly used with floating-point numbers to control the number of digits after the decimal point.\nSyntax\n{value:precisionf} **Example\nprint(f\u0026#34;{3:.2f}\u0026#34;) # \u0026#34;3.00\u0026#34; print(\u0026#34;{:.2f}\u0026#34;.format(3)) # \u0026#34;3.00\u0026#34; # Combined with width and alignment print(f\u0026#34;{3:\u0026lt;10.2f}\u0026#34;) # \u0026#34;3.00 \u0026#34; print(f\u0026#34;{3:\u0026gt;10.2f}\u0026#34;) # \u0026#34; 3.00\u0026#34; Type Specifier # Type specifiers allow you to format numbers in different bases (binary, octal, hexadecimal), percentages, and more.\nExample\n# Integer types number = 255 print(f\u0026#34;{number:b}\u0026#34;) # Binary: \u0026#34;11111111 print(f\u0026#34;{number:o}\u0026#34;) # Octal: \u0026#34;377\u0026#34; print(f\u0026#34;{number:x}\u0026#34;) # Hexadecimal: \u0026#34;ff\u0026#34; print(\u0026#34;{:b}\u0026#34;.format(number)) # Binary: \u0026#34;11111111\u0026#34; print(\u0026#34;{:o}\u0026#34;.format(number)) # Octal: \u0026#34;377\u0026#34; print(\u0026#34;{:x}\u0026#34;.format(number)) # Hexadecimal: \u0026#34;ff\u0026#34; # Percentage percentage = 0.85 print(f\u0026#34;{percentage:.2%}\u0026#34;) # \u0026#34;85.00%\u0026#34; print(\u0026#34;{:.2%}\u0026#34;.format(percentage)) # \u0026#34;85.00%\u0026#34; "},{"id":30,"href":"/docs/programming/network/networking/","title":"IP","section":"Network","content":" 🌐 Understanding IP Addresses: The Identity System of the Internet # Every device on a network needs an identity — a way for other devices to find it and communicate with it. In computer networking, that identity is an IP address.\n📌 What Is an IP Address? # An IP (Internet Protocol) address is a unique number assigned to each device connected to a network. It functions much like a home address — directing data to the right recipient.\nThere are two main versions in use:\nIPv4: A 32-bit address, written as four decimal numbers separated by dots (e.g., 192.168.0.1). IPv6: A 128-bit address, written in hexadecimal and separated by colons (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334). While IPv4 remains the most common, IPv6 adoption is growing due to the exhaustion of IPv4 address space.\n🧱 Structure of an IP Address # An IP address is composed of two parts:\nThe network portion: identifies the larger network segment. The host portion: identifies the specific device within that segment. For example, in the IPv4 address 192.168.1.10/24:\n192.168.1 identifies the network. .10 identifies the host (device). The /24 (in CIDR notation) means the first 24 bits are reserved for the network portion. This separation helps routers and switches direct traffic efficiently.\n🌍 Public vs. Private IP Addresses # Type Purpose Example Ranges Public Internet-facing devices Assigned by ISPs Private Internal local networks 192.168.0.0/16, 10.0.0.0/8, 172.16.0.0/12 Private IP addresses are not routable on the public internet. To enable internet access, routers use NAT (Network Address Translation) to map private IPs to a shared public IP.\n⚙️ Static vs. Dynamic IP Addresses # Static IP Address: Manually configured and does not change over time. Commonly used for servers, printers, and critical infrastructure. Dynamic IP Address: Assigned automatically by a DHCP (Dynamic Host Configuration Protocol) server. These may change over time and are typically used for desktops, laptops, and mobile devices. In most home networks, dynamic IPs are used by default.\n✅ Key Takeaways # Every device on a network is assigned a unique IP address. IPv4 and IPv6 are the two major formats — with IPv6 created to solve the limitations of IPv4. IP addresses are split into network and host portions for routing purposes. Private IPs are used within local networks; public IPs are globally unique and used on the internet. Devices can use either static or dynamic IP addresses, depending on their role. Understanding IP addresses is foundational to everything in networking — from\n"},{"id":31,"href":"/docs/programming/system-design/design-data-intensive-applications/partition/","title":"Partition","section":"Design Data Intensive Applications","content":" Partition # In the context of databases, partitioning refers to the process of dividing a database or its elements into smaller, more manageable pieces called partitions. Each piece of data (each record, row, or document) belongs to an exact partition.\nThe main benefit for partition is scalability. Different partitions can be placed on different nodes. Thus, a large dataset can be distributed across many disks, and the query loads can be distributed across many processors.\nPartitioning and Replication # Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes. This means that, even though each record belongs to exactly one partition, it may still be stored on several nodes for fault tolerance.\nPartitioning of Key-Value Data # Our goal of partitioning is to spread the data and the query load evenly across nodes.\nIf the partitioning is unfair, some partitions would have more data or queries than others. A partition with disproportionately high load is called a hot spot.\nPartitioning by Key Range # One way of partitioning is to assign a continuous range of keys to each partition. If you know the boundaries between the ranges, you can easily determine which partition contains a given key. If you also know which partition is assigned to which node, then you can make your request directly to the appropriate node\nThe ranges of keys are not necessarily evenly spaced, because your data may not be evenly distributed. In order to distribute the data evenly, the partition boundaries need to adapt to the data\nEven Distribution V.S. Fast Query # When partitioning a dataset, there is often a trade-off between achieving an even distribution of data and ensuring fast query performance.\nFast Query\nRange Scans: Key range partitioning allows for efficient range scans. For instance, storing sensor data with timestamps as keys means that measurements within a specific time period are stored in contiguous partitions. Example: Queries for data within a month can be quickly executed as the relevant data is stored in a few partitions due to the sorted order of timestamps. Even Distribution\nHot Spots: Key range partitioning can lead to hot spots where certain partitions become overloaded. This happens when all measurements within a time period are directed to a single node. Solution: To distribute data more evenly, the key can be prefixed with a sensor ID. This spreads the measurement data across multiple nodes, balancing the load more effectively. Trade-Off: While this approach improves data distribution and reduces hot spots, it complicates queries. Fetching data for multiple sensors within a time range now requires separate range queries for each sensor, potentially increasing query complexity and time. Partitioning by Hash of Key # Because of the risk of hot spots, many distributed datastores use a hash function to determine the partition for a given key. A good hash function takes skewed data and makes it uniformly distributed.\nOnce you have a suitable hash function for keys, you can assign each partition a range of hashes, and every key whose hash falls within a partition\u0026rsquo;s range will be stored in that partition\nLost\nBy using the hash of the key for partitioning we lose a nice property of key-range partitioning: the ability to do efficient range queries. Keys were once adjacent are now scattered across all the partitions, so their sort order is lost\nCompound primary key\nCompound primary key consists of several columns. First part of the key is called hashing key or partitioning key, which determines the partition. Other columns are used as an index for sorting the data. A query can\u0026rsquo;t search for a range of values within the first column of a compound key, but if it specifies a fixed value for the first column, it can perform an efficient range scan over the other columns of the key\nSkewed Workloads and Relieving Hot Spots # As discussed, hashing a key to determine its partition can help reduce hot spots. However, it can’t avoid them entirely: in the extreme case where all reads and writes are for the same key, you still end up with all requests being routed to the same partition. For example, a celebrity with millions of followers in a social media application\nOne solution is to add a random number to the beginning or end of the key. It could split the queries evenly to different partitions. However, that brings problem when reading data from those keys as you have to query the data from many partitions and then combine it\nPartitioning and Secondary Indexes # Secondary Index # A secondary index is an additional data structure that allows you to query the database more efficiently on columns other than the primary key.\nAssume you have a employees table with following columns\nemployee_id (Primary Key) name department salary hire_date Without Secondary Index\nSELECT * FROM employees WHERE department = \u0026#39;Sales\u0026#39;; Without a secondary index, the database might have to perform a full table scan, checking each row to find matches for the department column. This can be inefficient, especially for large tables\nWith Secondary Index\nYou can create a secondary index for department column. Database will create an additional data structure that maps department names to the rows where they appear. When you run the same query, the database can use the secondary index to quickly locate rows whose department is Sales without scanning the entire table\nData Structure\nYou can conceptually think the data structure of secondary index as\nMap\u0026lt;column_value, List\u0026lt;row_pointer\u0026gt;\u0026gt;\nSecondary index key: the value of the column that the secondary index is based on. For example, distinct department names\nList of rows: the index maintains a list of POINTERS (not rows) to the rows in the table where the column value matches the key. These pointers could be row ID or primary key values that uniquely identify each row\nLocal Secondary Index # A local secondary index is a type of secondary index in partitioned databases where the index is partitioned in the same manner as the base table. Each partition of the index corresponds directly to a partition of the base table, and the index entries within each partition only reference the rows within that partition.\nDisadvantages of Local Indexes\nSecondary index query could be expensive when you need to access data distributed across many or all partitions. For example, the employee table is partitioned by primary key employee_id, and you create a local index for department. When you need to query all employees whose department is Sales, you need to query all partitions\nGlobal Secondary Index # Global secondary index covers entire database regardless how the database is partitioned. That means the index entries can reference rows across multiple partitions\nPartitioning Global Secondary Index\nGlobal secondary indexes (GSIs) can also be partitioned. When a global secondary index is partitioned, it means that the index itself is divided into smaller segments or partitions, each of which can be managed independently. This allows the index to scale more effectively with the underlying data and distribute the indexing loads across multiple nodes.\nIndex update overheads\nThe downside of a global index is that writes are slower and more complex, because a write to a single row or document may affect multiple partitions of the index (a table has many secondary indexes and different secondary index entries may in different partitions)\nIn practice, updates to global secondary indexes are often asynchronous. That won\u0026rsquo;t increase write latency, but the index may not be up-to-date\nRebalancing Partitions # The process of moving loads from one node in the cluster to another is called rebalancing.\nMinimum Requirements of Rebalancing\nAfter rebalancing, the loads (data storage, read and write requests) should be shared fairly between the nodes in the cluster\nWhile rebalancing is happening, the database should continue accepting reads and writes\nNo more data than necessary should be moved between nodes\nStrategies for Rebalancing # Not Recommended: Hash Mod N # Assume we have N nodes. Hash(key) mod N would return a number between 0 and N-1. We can number nodes from 0 to N-1, and then use mod to assign the key to corresponding node.\nThe problem with mod N approach is that if the number of nodes N changes, most of the keys will need to be moved from one node to another node (including both existing nodes and new nodes). Such frequent moves make rebalancing excessively expensive.\nThis approach moves data around more than necessary.\nFix Number of Partitions # Create many more partitions than nodes, and assign several partitions to each node. For example, we have 10 nodes, and we create 100 partitions, and assign 10 partitions to each node.\nIf a new node is added to the cluster, the new node can \u0026ldquo;steal\u0026rdquo; a few partitions from every existing nodes until partitions are fairly distributed once again.\nIn this configuration, it’s possible to split and merge partitions, but a fixed number of partitions is operationally simpler, so many fixed-partition databases choose not to implement partition splitting.\nThus, the number of partitions configured at the outset is the maximum number of nodes you can have, so you need to choose it high enough to accommodate future growth.\nEach partition contains a fixed fraction of the total data, the size of each partition grows proportionally to the total amount of data in the cluster. If partitions are very large, rebalancing and recovery from node failures become expensive.\nHowever, each partition also has management overhead, so it’s counterproductive to choose too high a number.\nDynamic Partitioning # For databases that use key range partitioning, a fixed number of partitions with fixed boundaries would be very inconvenient when you need to reconfigure the partition boundaries\nDynamic partitioning would split a partition exceeding a configured size into two partitions and merge shrunk partitions into one partition.\nEach partition is assigned to one node, and each node can handle multiple partitions. After a large partition has been split, one of its two halves can be transferred to another node in order to balance the load.\nDynamic partitioning is not only suitable for key range-partitioned data, but can equally well be used with hash-partitioned data.\nPartitioning Proportionally to Nodes # With dynamic partitioning, the number of partitions is proportional to the size of the dataset, since the splitting and merging processes keep the size of each partition between some fixed minimum and maximum. On the other hand, with a fixed number of partitions, the size of each partition is proportional to the size of the dataset. In both of these cases, the number of partitions is independent of the number of nodes.\nA third option is to make the number of partitions proportional to the number of nodes \u0026ndash; in other words, to have fixed number of partitions per node.\nThe size of each partition grows proportionally to the dataset size while the number of nodes remains unchanged\nWhen you increase the number of nodes, the partitions become smaller again\nNew Node Joins\nWhen a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of on half of each of those split partitions while leaving the other half of each partition in place\nRequest Routing # Service Discover: When a client wants to make a request, how does it know which node to connect to?\nAllow clients to send the request to any node, and the node either handles the request directly or forward it to appropriate node\nSend all requests from clients to a routing layer, like load balancer\nRequire that clients be aware of the partitioning and the assignment of partitions to nodes. In this case, clients can connect directly to the appropriate node\n"},{"id":32,"href":"/docs/programming/backend/java/netty/pipeline-handler/","title":"Pipeline \u0026 Handler","section":"Netty","content":" Handler # Role # A Handler in Netty is a component that contains the business logic for processing inbound and outbound data as well as various network (channel) events.\nTypes # ChannelInboundHandler:\nPurpose: Deals with inbound data and events. It processes incoming data and reacts to channel events Key Methods: channelRead, channelActive, channelInactive, channelRegistered, channelUnregistered. Usage: You override these methods to perform actions like reading data from a network socket, reacting to channel activation or deactivation, etc. ChannelOutboundHandler:\nPurpose: Responsible for handling outbound operations, such as writing data to the network or closing the channel. Key Methods: write, flush, close, bind, connect. Usage: These methods are overridden to intercept and execute operations that modify the state of the channel or write data to it. Pipeline # What is pipeline? # A Netty pipeline is an ordered list of handlers. It represents a sequence of operations that are applied to inbound and outbound data.\nStructure and Flow # ChannelPipeline: Each Channel in Netty has its own ChannelPipeline. This pipeline is automatically created when the channel is created. Ordered Sequence: Handlers are placed in the pipeline in a specific order, and the data or events are processed in this sequence. Data Flow: Inbound Data: Flows head-to-tail through the pipeline. Each inbound handler processes the data and passes it to the next handler. Outbound Data: Flows tail-to-head. Each outbound handler processes the data and passes it back towards the head of the pipeline. Example # Server # 1. EventLoopGroup boss = new NioEventLoopGroup(); 2. EventLoopGroup worker = new NioEventLoopGroup(2); 3. ChannelInboundHandlerAdapter inboundHandler1 = new ChannelInboundHandlerAdapter() { 4. @Override 5. public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { 6. ByteBuf buf = (ByteBuf) msg; 7. String message = buf.toString(StandardCharsets.UTF_8); 8. log.debug(\u0026#34;InboundHandler_1 receives msg: [{}]\u0026#34;, message); 9. super.channelRead(ctx, message); 10. } 11. }; 12. ChannelInboundHandlerAdapter inboundHandler2 = new ChannelInboundHandlerAdapter() { 13. @Override 14. public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { 15. log.debug(\u0026#34;InboundHandler_2 receives msg: [{}]\u0026#34;, msg); 16. ctx.channel().writeAndFlush(ctx.alloc().buffer().writeBytes(\u0026#34;Hello Client\u0026#34;.getBytes())); 17. super.channelRead(ctx, msg); 18. } 19. }; 20. ChannelOutboundHandlerAdapter outboundHandler1 = new ChannelOutboundHandlerAdapter() { 21. @Override 22. public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception { 23. log.debug(\u0026#34;OutboundHandler_1 writes msg\u0026#34;); 24. super.write(ctx, msg, promise); 25. } 26. }; 27. ChannelOutboundHandlerAdapter outboundHandler2 = new ChannelOutboundHandlerAdapter() { 28. @Override 29. public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception { 30. log.debug(\u0026#34;OutboundHandler_2 writes msg\u0026#34;); 31. super.write(ctx, msg, promise); 32. } 33. }; 34. new ServerBootstrap() 35. .group(boss, worker) 36. .channel(NioServerSocketChannel.class) 37. .childHandler(new ChannelInitializer\u0026lt;NioSocketChannel\u0026gt;() { 38. @Override 39. protected void initChannel(NioSocketChannel nioSocketChannel){ 40. ChannelPipeline pipeline = nioSocketChannel.pipeline(); 41. nioSocketChannel.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); 42. pipeline.addLast(inboundHandler1); 43. pipeline.addLast(inboundHandler2); 44. pipeline.addLast(outboundHandler1); 45. pipeline.addLast(outboundHandler2); 46. } 47. }) 48. .bind(9999); Line 1-2. EventLoopGroups Creation: Two EventLoopGroup instances are created. boss handles accepting new connections, and worker (with 2 threads) handles actual I/O operations for the connections. Line 3-11. First Inbound Handler: A custom inbound handler (inboundHandler1) that reads incoming data (as ByteBuf), converts it to a string, logs it, and forwards it down the pipeline. Line 12-19. Second Inbound Handler: Another inbound handler (inboundHandler2) that logs the received message and writes a response back to the client. Line 20-26. First Outbound Handler: A custom outbound handler (outboundHandler1) that logs a debug message when it writes data. Line 27-33. Second Outbound Handler: Another outbound handler (outboundHandler2) with similar functionality as the first outbound handler, logging when data is written. 34-48. Bootstrap a server. In the channel pipeline, five handlers are added, including a logging handler and the two inbound and two outbound handlers Client # EventLoopGroup group = new NioEventLoopGroup(); ChannelFuture channelFuture = new Bootstrap() .group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer\u0026lt;NioSocketChannel\u0026gt;() { @Override protected void initChannel(NioSocketChannel nioSocketChannel) { nioSocketChannel.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); nioSocketChannel.pipeline().addLast(new StringEncoder()); } }) .connect(new InetSocketAddress(\u0026#34;localhost\u0026#34;, 9999)); Channel channel = channelFuture.sync().channel(); channel.writeAndFlush(\u0026#34;Hello World\u0026#34;); Client connects to the server Then client sends a message to the server Execution # Start the server Start the client Execution Result # Server log\n2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] REGISTERED 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] ACTIVE 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] READ: 11B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 57 6f 72 6c 64 |Hello World | +--------+-------------------------------------------------+----------------+ 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG c.w.n.c.pipeline_handler.Server - InboundHandler_1 receives msg: [Hello World] 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG c.w.n.c.pipeline_handler.Server - InboundHandler_2 receives msg: [Hello World] 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG c.w.n.c.pipeline_handler.Server - OutboundHandler_2 writes msg 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG c.w.n.c.pipeline_handler.Server - OutboundHandler_1 writes msg 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] WRITE: 12B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 43 6c 69 65 6e 74 |Hello Client | +--------+-------------------------------------------------+----------------+ 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] FLUSH 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] READ COMPLETE Client log\n2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712] REGISTERED 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712] CONNECT: localhost/127.0.0.1:9999 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] ACTIVE 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] WRITE: 11B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 57 6f 72 6c 64 |Hello World | +--------+-------------------------------------------------+----------------+ 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] FLUSH 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] READ: 12B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 43 6c 69 65 6e 74 |Hello Client | +--------+-------------------------------------------------+----------------+ 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] READ COMPLETE Pipeline Virtualization # Double-Linked List:\nEach handler in the pipeline is a node in a double-linked list, maintaining references to its previous and next handlers. This allows for efficient traversal in both directions – towards the head for outbound events and towards the tail for inbound events. Dynamic Modification:\nThe double-linked list structure allows for handlers to be dynamically added, removed, or replaced at runtime. This is crucial for Netty\u0026rsquo;s flexibility, enabling the pipeline to adapt to different protocol requirements or runtime conditions. Ordered Processing:\nThe order of handlers in the list determines the order of processing. Inbound data flows through the pipeline from the first (head) to the last (tail) handler, while outbound data flows in the opposite direction. Contextual Linkage:\nEach handler is associated with a ChannelHandlerContext, which provides the handler with its position (context) in the pipeline. This context is used to interact with the pipeline, such as forwarding events, accessing the channel, or modifying the pipeline itself. Head and Tail Contexts:\nSpecial HeadContext and TailContext nodes are typically present in the beginning and at the ends of the pipeline. These nodes handle some standard operations, like the initial entry of events into the pipeline or the termination of event propagation. How Does It Work? # Client\nClient Registration, Connection, and Activation The client registers a new channel (REGISTERED), connects to the server (CONNECT), and becomes active (ACTIVE). Server\nServer Registration and Activation The server registers a new channel (REGISTERED) and then becomes active (ACTIVE). This indicates that the server accepts client\u0026rsquo;s connection Client\nSending Data to Server: The client writes (WRITE) the \u0026ldquo;Hello World\u0026rdquo; message (11 bytes) and then flushes it (FLUSH) to the server. Server\nReceiving Client Data:\nThe server reads (READ) 11 bytes of data from the client. This data corresponds to the string \u0026ldquo;Hello World\u0026rdquo;. Inbound Handler Processing:\nInboundHandler_1 on the server receives the \u0026ldquo;Hello World\u0026rdquo; message, logs it, and passes it along. InboundHandler_2 also receives the \u0026ldquo;Hello World\u0026rdquo; message, logs it, and then writes a response (\u0026ldquo;Hello Client\u0026rdquo;) back to the client. Outbound Handler Processing:\nOutboundHandler_2 and OutboundHandler_1 log the message as it passes through the outbound pipeline. The server then writes 12 bytes, which is the \u0026ldquo;Hello Client\u0026rdquo; response, and flushes it (FLUSH) to the client. Completion of Read Operation:\nThe READ COMPLETE event signifies the end of the read operation for this round of communication. Client\nReceiving Server Response: The client reads (READ) 12 bytes of data, which is the \u0026ldquo;Hello Client\u0026rdquo; message sent by the server. Completion of Read Operation: Similar to the server, the READ COMPLETE event on the client side indicates the end of the read operation. "},{"id":33,"href":"/docs/programming/system-design/technology/redis/pub-sub/","title":"Redis Pub/Sub","section":"Redis","content":" Redis Pub/Sub # Redis Pub/Sub (Publish/Subscribe) is a lightweight messaging system that allows clients to send and receive messages in real time. It’s designed for event-driven communication where publishers broadcast messages to channels, and subscribers listening to those channels receive the messages instantly.\nKey Components # Channels: Channels are message hubs in Redis, responsible for receiving and broadcasting messages. Publishers: Clients that send messages to channels. Subscribers: Clients that listen to channels and receive messages. How It Works # Subscribers Subscribe to Channels:\nA subscriber issues the SUBSCRIBE command to start listening to one or more channels. Example: # Subscriber listens to \u0026#39;news\u0026#39; and \u0026#39;sports\u0026#39; channels SUBSCRIBE news sports Publishers Publish Messages:\nA publisher sends messages to a specific channel using the PUBLISH command. Example: # Publisher sends a message to the \u0026#39;news\u0026#39; channel PUBLISH news \u0026#34;Breaking news! Redis is awesome.\u0026#34; Message Broadcast:\nRedis instantly delivers the message to all subscribers of the channel. Example: # Subscribers receive the message in real time \u0026#34;news: Breaking news! Redis is awesome.\u0026#34; Dynamic Subscriptions:\nClients can subscribe or unsubscribe from channels at any time without interrupting other operations. Example: # Dynamically unsubscribe from \u0026#39;sports\u0026#39; channel UNSUBSCRIBE sports Key Properties of Redis Pub/Sub # Push-Based Messaging:\nMessages are pushed to subscribers as soon as they are published, unlike pull-based systems like Kafka. Synchronous Communication:\nMessages are broadcasted to all active subscribers immediately after being published. No Acknowledgment:\nRedis Pub/Sub does not require explicit acknowledgment of message delivery. No Message Persistence:\nMessages are only delivered to active subscribers. If a subscriber is offline, it will miss messages sent during its downtime. Dynamic Subscribe/Unsubscribe:\nClients can dynamically subscribe or unsubscribe from channels at runtime without interfering with other subscribers or the overall Pub/Sub system. "},{"id":34,"href":"/docs/programming/web/security/session-vs-token/","title":"Session vs Token","section":"Security","content":" Introduction # Both sessions and tokens are used for user authentication and maintaining user state across multiple HTTP requests in a web application.\nProcess # Session-Based Authentication Process # 1-2: User Login Attempt\nThe user submits their login credentials (usually username and password) through the client (e.g., a web browser). 3-4: Credentials Verification\nThe server receives the credentials and verifies them against its user database or authentication source. 5-6: Session Creation\nUpon successful verification, the server creates a new session. This session is stored in the server\u0026rsquo;s memory or a session store (like a database). 7: Sending Session ID to Client\nThe server sends this session ID back to the client, typically as a cookie. This cookie is stored in the user\u0026rsquo;s browser. Client Stores Session ID\nThe browser stores this session ID and sends it along with every subsequent request to the server. Server Session Validation\nFor each new request from the client, the server reads the session ID from the cookie, looks up the corresponding session in its session store, and validates it. Token-Based Authentication Process (Using JWT) # 1-2: User Login Attempt\nThe user submits their login credentials (usually username and password) through the client (e.g., a web browser). 3-4: Credentials Verification\nThe server receives the credentials and verifies them against its user database or authentication source. Token Generation\nUpon successful verification, the server generates a token (like a JWT). This token includes encoded user information and is digitally signed by the server. 5: Server sends Token to Client\n6: Client Stores Token\nToken Sent with Requests\nFor every subsequent request, the client attaches this token, typically in the HTTP Authorization header. Server Token Validation\nThe server validates the token on each request. This involves verifying the token’s integrity and possibly checking against a list of revoked tokens. Comparison # Session-Based Authentication # User Information Storage:\nIn session-based authentication, user information is stored on the server. This can include user identity, authentication status, user preferences, and other session-related data. Server Memory and Resources:\nSince the data is stored server-side, it consumes server memory and resources. This can be significant, especially with a large number of concurrent users. Client-Server Interaction:\nDuring subsequent requests, the client sends this session ID back to the server. The server uses this ID to retrieve the user\u0026rsquo;s session information and authenticate the request. Security:\nThe actual user data is not exposed to the client, which can be a security advantage. However, the session ID needs to be protected to prevent session hijacking. Scalability Concerns:\nStoring session data for each user can impact the scalability of the application, especially in distributed systems where session data might need to be shared across servers. Token-Based Authentication (e.g., JWT) # User Information Storage:\nIn token-based systems like JWT, user authentication information is encoded and optionally encrypted within the token itself. This can include user ID, roles, permissions, and other claims. Client-Side Storage:\nThe token is stored on the client side, typically in the browser\u0026rsquo;s local storage, session storage, or as an HTTP-only cookie. Stateless Operation:\nEach request from the client includes the token, allowing the server to authenticate the request statelessly, without needing to store user session data. Security:\nWhile the user information in the token can be read by the client, sensitive data should never be stored in a token payload unless encrypted. The token is also susceptible to different types of attacks (like XSS), and thus must be handled securely. Scalability and Performance:\nSince the server does not store session state, token-based authentication can be more scalable, reducing the load and memory requirements on the server. "},{"id":35,"href":"/docs/programming/backend/java/nio/unblocking-mode/","title":"Unblocking Mode","section":"Nio","content":" Unblocking Mode # In last post, we discussed blocking mode and its problems. In this post, we will discuss unblocking mode and its problems\nCode Example # Server\n@Slf4j public class Server { public static void main(String[] args) throws IOException { ByteBuffer buffer = ByteBuffer.allocate(32); ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(9999)); ssc.configureBlocking(false); List\u0026lt;SocketChannel\u0026gt; channels = new ArrayList\u0026lt;\u0026gt;(); while (true) { log.debug(\u0026#34;Server connecting\u0026#34;); SocketChannel sc = ssc.accept(); if (sc != null) { log.debug(\u0026#34;Connect to client \u0026#34; + sc.getRemoteAddress()); sc.configureBlocking(false); channels.add(sc); } for (SocketChannel channel : channels) { log.debug(\u0026#34;Server reading\u0026#34;); int read = channel.read(buffer); if (read != 0) { buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); log.debug(\u0026#34;Read data from client \u0026#34; + channel.getRemoteAddress()); } } } } } Comparing with server code of blocking mode, we set blocking configuration of the ServerSocketChannel and SocketChannel to false ssc.configureBlocking(false); sc.configureBlocking(false); Also we add a conditional check before adding the SocketChannel to the list and reading data from the channel. That is because in unblocking mode, method scc.accept() will not be blocked, instead it returns a SocketChannel object or null depending on if a client connects to the server. Hence, we should only add the SocketChannel to the list if it is not null, in other words, a client connects to the server. Method channel.read(buffer) will always read from the Channel no matter clients send data or not. What the method returns is the length of bytes it reads. Here we only handle the data if len \u0026gt; 0 if (sc != null) { ... } if (read != 0) { ... } Client\n@Slf4j public class Client { public static void main(String[] args) throws IOException { SocketChannel sc = SocketChannel.open(); log.debug(\u0026#34;Connecting to server\u0026#34;); sc.connect(new InetSocketAddress(InetAddress.getLocalHost(), 9999)); log.debug(\u0026#34;Connected with server\u0026#34;); while (true) { String input = Scanner_.scanLine(\u0026#34;Input: \u0026#34;); log.debug(\u0026#34;Sending data [{}] to server\u0026#34;, input); sc.write(StandardCharsets.UTF_8.encode(input)); log.debug(\u0026#34;Sent data [{}] to server\u0026#34;, input); } } } Client code has no difference with that code in blocking mode Demo # Start the server # Server log:\n... 09:26:19.287 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:26:19.287 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:26:19.287 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:26:19.287 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting ... The log Server connecting is printed infinitely because of unblocking mode Client connects to the server # Server log:\n... 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server reading 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server reading 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting ... Logs are continuously printed, but this time, new log Server Reading is printed also because after the client connects to the server, the SocketChannel is added in the list, and the server unblockingly reads the channel Client log:\n09:28:33.989 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Connecting to server 09:28:33.993 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Connected with server Input: Client sends data to the server # To better demonstrate unblocking mode, I removed some logs in the server code to avoid logs from continuously being printed. Then I restarted the server, and connects to the client\nClient log:\n09:40:28.973 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Connecting to server 09:40:28.976 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Connected with server Input: Hello Server from ClientA! 09:40:45.721 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Sending data [Hello Server from ClientA!] to server 09:40:45.722 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Sent data [Hello Server from ClientA!] to server Input: Hello Server from ClientA Again! 09:40:57.604 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Sending data [Hello Server from ClientA Again!] to server 09:40:57.605 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Sent data [Hello Server from ClientA Again!] to server Input: A client connects to the server and sends two strings to the server. In blocking mode, the server should only receive first string and receive second string until another client connects to the server Server log\n09:40:28.989 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Connect to client /127.0.0.1:50930 09:40:45.723 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server from ClientA! 09:40:45.723 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Read data from client /127.0.0.1:50930 09:40:57.605 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server from ClientA Again! 09:40:57.605 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Read data from client /127.0.0.1:50930 The server connects to the client Further more, it receives all strings from the client. As we can see here, because the server is running in unblocking mode, it can handle client connection and data communication concurrently Summary # In unblocking mode, the blocking methods get executed, so the server can handle multiple connections and receive data from multiple clients concurrently. However, different from blocking mode in which the CPU is blocked for most time, CPU in unblocking mode is busy and always running. That\u0026rsquo;s the problem of unblocking mode and it is a waste of resource cause in most time, CPU doesn\u0026rsquo;t do any useful work but run the while loop\u0026hellip; In the next post, we will discuss how NIO uses Selector to handle this issue "},{"id":36,"href":"/docs/programming/backend/java/thread/synchornization/","title":"Synchronization","section":"Thread","content":" Thread Interference # Imagine you have an object that maintains a hit count for a website. If two threads increment the hit counter at the same time, they might read the same value, say 100. Both threads then increment it and set it back to the object. The result should be 102 hits, but because there was no synchronization, you only get 101 - one hit is lost. This is a simple example of a race condition.\npublic class ThreadInterference { public static void main(String[] args) throws InterruptedException { InterferenceThread thread1 = new InterferenceThread(); InterferenceThread thread2 = new InterferenceThread(); thread1.start(); thread2.start(); thread1.join(); thread2.join(); int count = InterferenceThread.getCount(); System.out.println(\u0026#34;The value of count in InterferenceThread class is supposed to be 2_000_000, but the real value is: \u0026#34; + count); } } class InterferenceThread extends Thread { static int count = 0; int loopCount = 0; public static int getCount() { return count; } @Override public void run() { while (loopCount++ \u0026lt; 1_000_000) { count++; } } } /* * Executing result: * The value of count InterferenceThread class is supposed to be 2_000_000, but the real value is: 1563643 * */ Introduction to Synchronization # In Java, synchronization is a mechanism that allows us to control the access of multiple threads to any shared resource. It\u0026rsquo;s critical because it helps prevent thread interference and consistency problems.\nWhy do we need synchronization? # To avoid thread interference which occurs when multiple threads try to access and modify the shared resource concurrently. To prevent consistency problems by making sure that the shared resource is in a consistent state. To achieve thread safety when creating applications that are intended to be used in a multithreaded environment. The Concept of a Monitor (Intrinsic Locks) # Every Java object has an intrinsic lock associated with it. When a thread wants to execute a synchronized method on an object, it first needs to obtain the intrinsic lock. Here\u0026rsquo;s how it works:\nWhen a synchronized method is called, the thread automatically acquires the lock before executing the method. Other threads that attempt to call a synchronized method on the same object are blocked until the first thread exits the method and releases the lock. Synchronized Methods/Block # Synchronized methods allow us to create thread-safe operations by ensuring that only one thread can execute a synchronized method on an instance at a time. When a thread is executing a synchronized method, all other threads that invoke synchronized methods on the same instance will be blocked until the first thread exits the method.\nClass-Level Synchronization # You can synchronize static methods or code snippets containing static fields, which locks the Class object associated with the class. This means it locks at the class level, and only one thread can execute any of the synchronized static methods for that class.\npublic class ClassLevelSynchronization { public static void main(String[] args) throws InterruptedException { ClassLevelSyncThread thread1 = new ClassLevelSyncThread(); ClassLevelSyncThread thread2 = new ClassLevelSyncThread(); thread1.start(); thread2.start(); thread1.join(); thread2.join(); int count = ClassLevelSyncThread.getCount(); System.out.println(\u0026#34;The value of count in InterferenceThread class is supposed to be 2_000_000, but the real value is: \u0026#34; + count); } } class ClassLevelSyncThread extends Thread { static int count = 0; int loopCount = 0; public static int getCount() { return count; } @Override public void run() { while (loopCount++ \u0026lt; 1_000_000) { synchronized (ClassLevelSyncThread.class) { count++; } } } } /* * The value of count in InterferenceThread class is supposed to be 2_000_000, but the real value is: 2000000 * */ In above example, code count++ is wrapped by a synchronized block, and the lock for this block is SynchronizedThread.class, which means at any time only one SynchronizedThread thread can execute count++ code.\nWhy use SynchronizedThread.class as the lock? That is because count is a static field, and its value is associated to SynchronizedThread class instead of any concrete objects. If we use a concrete object as the lock like this, cause the lock belongs to different objects, the scenario that multiple threads access the same variable at a moment will happen again. You can replace the SynchronizedThread.class with this and check the executing results.\nInstance-Level Synchronization # When you synchronize an instance method, you are locking the instance for which the method was called. Only one thread per instance can execute any of the synchronized instance methods.\npublic class InstanceLevelSynchronization { public static void main(String[] args) throws InterruptedException { InstanceLevelSyncThread instanceLevelSyncThread = new InstanceLevelSyncThread(); Thread thread1 = new Thread(instanceLevelSyncThread); Thread thread2 = new Thread(instanceLevelSyncThread); Thread thread3 = new Thread(instanceLevelSyncThread); thread1.start(); thread2.start(); thread3.start(); thread1.join(); thread2.join(); thread3.join(); int count = instanceLevelSyncThread.getCount(); System.out.println(\u0026#34;The value of count in InterferenceThread class is supposed to be 3_000_000, but the real value is: \u0026#34; + count); } } class InstanceLevelSyncThread implements Runnable { int count = 0; @Override public void run() { for (int loopCount = 0; loopCount \u0026lt; 1_000_000; loopCount++) { synchronized (this) { count++; } } } public int getCount() { return count; } } /* * Executing results: * The value of count in InterferenceThread class is supposed to be 3_000_000, but the real value is: 3000000 * */ Here we initiate a InstanceLevelSyncThread instance and create three threads with the instance passed in. The lock for the synchronized block here is this, which is the instance we create.\nWe craete three threads, why are they synchronized by InstanceLevelSyncThread instance?\n/* What will be run. */ private Runnable target; public Thread(Runnable target) { this(null, target, \u0026#34;Thread-\u0026#34; + nextThreadNum(), 0); } @Override public void run() { if (target != null) { target.run(); } } Let\u0026rsquo;s first look at the source code of Thread. Thread has a Runnable filed target and a constructor taking Runnable instance as parameter. Look at the run() method in Thread, the code what will be run is not the Thread itself but the Runnable instance target. In above example, alother we create three Thread instance, but we pass same target to them, so the codes run by each thread are all from same instance, which is target. That\u0026rsquo;s why, count is synchronized by this.\nWhy do we use for loop here, any problem using while loop with loopCount? loopCount is a instance-level variable which is a shared field to different threads. If we use a while loop with loopCount like below, cause it is not synchronized, its value could be implemented by different threads at same moment. In other words, loopCount is not thread safe.\nwhile (loopCount++ \u0026lt; 1_000_000) { synchronized (this) { count++; } } However, when using a for loop, we create a new int varibale each time when run() method is executed, which won\u0026rsquo;t be shared by multiple threads, so it is thread safe.\nfor (int loopCount = 0; loopCount \u0026lt; 1_000_000; loopCount++) { synchronized (this) { count++; } } "},{"id":37,"href":"/docs/programming/network/cird/","title":"CIRD","section":"Network","content":" 📏 CIDR Explained: The Flexible Backbone of IP Addressing # One of the most powerful tools in modern networking is CIDR — Classless Inter-Domain Routing. It replaces the old class-based IP system and gives us a flexible, scalable way to organize and route traffic across networks.\nThis post introduces what CIDR is, why it was created, and how to read and use CIDR notation.\n🧠 Why CIDR Was Introduced # Before CIDR, IP addresses were divided into rigid classes:\nClass A: x.0.0.0/8 (16 million IPs) Class B: x.x.0.0/16 (65,000 IPs) Class C: x.x.x.0/24 (256 IPs) This system caused two major problems:\nWasted IP addresses — Because the class-based system allocated IPs in large fixed blocks (e.g., Class A gave 16 million IPs), many organizations received far more IPs than they actually needed. For example, a company that needed just a few hundred IPs might be assigned a Class B block with over 65,000 IPs, leaving the rest unused and wasted.\nHuge routing tables — Since each classful network block had to be listed individually in routers\u0026rsquo; routing tables, ISPs and backbone routers ended up managing thousands of separate entries. This made routing tables large and inefficient, slowing down routing decisions and requiring more memory and processing power in networking hardware.\nCIDR solves both by letting us define IP ranges of any size, and aggregate multiple ranges into one route.\n🔢 What Is CIDR? # CIDR stands for Classless Inter-Domain Routing. It expresses IP address ranges using this format:\n\u0026lt;IP address\u0026gt;/\u0026lt;prefix length\u0026gt; Example: # 192.168.1.0/24 The first 24 bits are the network portion (192.168.1). The remaining 8 bits are for hosts (0–255). CIDR allows you to describe both large and small networks with precision.\n🧮 How to Read CIDR Notation # CIDR IP Range Size Usable IPs Notes /30 4 IPs 2 usable For point-to-point links /24 256 IPs 254 usable Common for home/office LANs /16 65,536 IPs 65,534 For larger enterprise blocks /8 ~16 million ~16 million Very large, rarely used today The number of usable IPs is always 2 less than the total range: one reserved for the network address, and one for the broadcast address.\n🧭 CIDR in Routing Tables # Routers use CIDR to make decisions about where to forward packets:\nEach rule in the routing table is a CIDR block.\nRouters match the destination IP to the longest prefix (most specific match). For example, if a router sees two matching entries in its routing table — one for 192.168.0.0/16 and another for 192.168.1.0/24 — and the destination IP is 192.168.1.45, it will choose the /24 route because it is more specific (i.e., it matches more bits of the IP address).\nBefore CIDR, if a router needed to support four separate Class C networks (each with 256 IP addresses), it required four distinct routing table entries:\n192.168.0.0/24 192.168.1.0/24 192.168.2.0/24 192.168.3.0/24 Each of these had to be handled independently, adding complexity and overhead to routing operations.\nWith CIDR, we can aggregate these four networks into a single route:\n192.168.0.0/22 This is possible because the first 22 bits of the IP addresses in all four networks are identical. Route aggregation significantly reduces the number of routing table entries, leading to faster lookups and better scalability across large networks and the internet.\nCIDR makes the internet\u0026rsquo;s routing tables smaller, faster, and more scalable.\n"},{"id":38,"href":"/docs/programming/system-design/technology/redis/replication-and-sentinel/","title":"Redis Replication \u0026 Sentinel","section":"Redis","content":" Redis Replication \u0026amp; Sentinel # High availability is critical for any modern system, and Redis achieves this through its master-slave architecture and Redis Sentinel. Let’s explore how these components work together to ensure seamless operation, even in the face of failures.\nMaster and Slave Architecture # The master-slave architecture is a classic replication model commonly used in distributed systems. Its primary purpose is to enhance system scalability and fault tolerance by replicating data from a central master node to multiple slave nodes.\nRedis supports a master-slave replication model, where:\nMaster Node:\nHandles all write operations. Automatically propagates data changes to the slave nodes. Slave Nodes:\nServe as read replicas, improving scalability by handling read operations. Act as backups for the master in case of failure. What Happens If the Master is Down?\nWhen the master node goes down, the entire system can become unusable for write operations. This raises the critical question: How does Redis handle master failures?\nThis is where Redis Sentinel comes into play.\nRedis Sentinel: The Key to High Availability # Redis Sentinel is a system designed to monitor Redis nodes and ensure high availability. It:\nMonitors Redis Nodes:\nSentinels continuously check the health of the master and slave nodes by sending PING commands. If a node doesn’t respond within the configured timeout (down-after-milliseconds), it is marked as subjectively down (SDOWN). Performs Automatic Failover:\nIf a master is declared objectively down (ODOWN) by a quorum of Sentinels, Sentinel promotes one of the slaves to master. It reconfigures the other slaves to replicate from the new master. Acts as a Configuration Provider:\nSentinel informs clients about the new master, ensuring they always connect to the correct instance. What Happens If a Sentinel Instance is Down?\nSentinel is designed to operate as a cluster of multiple Sentinel nodes. Even if one or more Sentinel instances fail, as long as the remaining Sentinels meet the quorum requirement, they can continue to monitor the Redis nodes and perform failovers.\nHow Does Sentinel Ensure Clients Always Discover the Latest Master? # Sentinel achieves consistency using quorum, which is a leaderless replication mechanism. It ensures clients always connect to the latest master by leveraging quorum-based decision-making and state propagation. To understand this, let’s break the process down step-by-step.\n1. Detecting Master Failure # Each Sentinel node periodically sends PING commands to the master, slaves, and other Sentinels to monitor their health. If a Sentinel node does not receive a response within the configured down-after-milliseconds time, it marks the master as subjectively down (SDOWN). Sentinels communicate this status with each other using a gossip protocol, exchanging their view of the cluster state. 2. Declaring the Master Objectively Down (ODOWN) # A quorum of Sentinels (e.g., 3 out of 5 nodes) must agree that the master is unresponsive to declare it objectively down (ODOWN). It’s important to note that quorum is only used for the failover decision, not for updating clients with the new master information. Once quorum is achieved, the failover process begins. 3. Electing a Leader Sentinel for Failover # During failover, one Sentinel is temporarily elected to coordinate the failover process. 4. Promoting a New Master # The leader Sentinel selects the best candidate slave based on: Replication offset (to ensure minimal data loss). Connection health. Priority configuration (replica-priority). The selected slave is promoted to master, and other slaves are reconfigured to replicate from the new master. 5. Synchronizing the New Master # Once the failover is complete, the Sentinel leader synchronizes the new master node information to other Sentinels using the gossip protocol. As the state propagates through the Sentinel cluster, it is important to note that the Sentinels involved in the failover decision (write quorum) do not necessarily have the latest master information immediately, as the information is propagated asynchronously after the failover process. 6. Communicating the New Master to Clients # Clients query Sentinels to discover the current master node.\nWhile some Sentinels may have stale information during this brief propagation period, clients can query multiple Sentinels to get the latest master information, making a best-effort attempt to ensure accurate and consistent data.\n"},{"id":39,"href":"/docs/programming/backend/java/nio/selector/","title":"Selector","section":"Nio","content":" Selector in Java NIO # A Selector in Java NIO is a special type of object that can check one or more NIO channels and determine which channels are ready for data operations (such as reading or writing). This is crucial in scenarios like servers handling multiple client connections.\nKey Features: # Multiplexing: A single selector can handle multiple channels. Non-blocking Mode: Channels registered with a selector are usually in non-blocking mode. Efficiency: Instead of using multiple threads to handle channels, one thread can handle multiple channels using a selector. How Selectors Work: # Create a Selector: Obtain a selector using the Selector.open() method. Register Channels with Selector: Channels (like SocketChannel) need to be registered with the selector. Selection: Use the select() method to check which channels are ready. This method blocks until at least one channel is ready. Below is the code example of server. Let\u0026rsquo;s read the code line by line to understand how selector works\n@Slf4j public class Server { public static void main(String[] args) throws IOException { Selector selector = Selector.open(); log.debug(\u0026#34;Create a selector\u0026#34;); ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(9999)); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT, null); log.debug(\u0026#34;Register ServerSocketChannel under selector\u0026#34;); while (true) { log.debug(\u0026#34;Start selecting\u0026#34;); selector.select(); log.debug(\u0026#34;Selected\u0026#34;); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = selector.selectedKeys().iterator(); while(keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); log.debug(\u0026#34;Connected to client {}\u0026#34;, sc.getRemoteAddress()); } } } } Create a Selector\nSelector selector = Selector.open(); Here we create a new Selector which has two important collections, keys and selectedKeys. At this time, the two collections are still empty. Let\u0026rsquo;s go ahead to understand what those two collection hold Register a channel with the selector\nServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(9999)); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT, null); We create a new ServerSocketChannel Then register the ServerSocketChannel under the selector In register method, we pass in the selector and interested operation. For the ServerSocketChannel, it only interests in ACCEPT operation. We will introduce more operations later After registration, a new SelectionKey is created and added into collection keys Select channels with event\nselector.select(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = selector.selectedKeys().iterator(); After invoking select method, the selector will monitor all registered channels. The method will be blocked here until channels have event Once channels have event happened, the selector will add those channels to another collection selectedKeys Handle channel operations\nSelectionKey key = keyIterator.next(); ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); Only one channel is registered under the selector, so I cast the channel to ServerSocketChannel and let it accept client\u0026rsquo;s connection Demo # Start the server\n08:45:22.261 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Create a selector 08:45:22.268 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register ServerSocketChannel under selector 08:45:22.268 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Start selecting The selector starts to monitor registered channels. select method is blocked until event happens in the channel ClientA connects\n08:46:01.420 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 08:46:01.421 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47074 08:46:01.422 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Start selecting ClientB connects\n08:46:31.762 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 08:46:31.762 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47080 08:46:31.762 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Start selecting "},{"id":40,"href":"/docs/programming/system-design/design-data-intensive-applications/transactions/","title":"Transactions","section":"Design Data Intensive Applications","content":" Transaction # A transaction is a way for an application to group several reads and writes together into a logical unit. Either the entire transaction succeeds (commits) or it fails (aborts, rollbacks). This can eliminate partial failure\u0026ndash;i.e., the case where some operations succeed and some fail\nThe Meaning of ACID # ACID, which stands for Atomicity, Consistency, Isolation, and Durability, are the safety guarantees provided by transactions\nAtomicity # Definition\nThe ability to abort a transaction on error and have all writes from that transaction discarded\nIf the writes are grouped together into an atomic transaction, and the transaction cannot be completed (committed) due to a fault, then the transaction is aborted, and the database must discard or undo any writes it has made so far in that transaction.\nConsistency # Consistency: Database should be in valid states before and after transactions\nInvariants: Rules or conditions that define what constitutes a valid state in the database\nInvariant Examples\nUnique and valid email address Non-negative product price Non-negative account balance Application Property\nMany consistency rules are specific to the business logic of the application. For example, ensuring that a customer’s order is valid according to various business rules (e.g., inventory availability, discount application, shipping constraints) often requires logic that is best handled in the application layer. Thus, the letter C doesn\u0026rsquo;t really belong to ACID\nIsolation # Concurrently running transactions shouldn’t interfere with each other. For example, if one transaction makes several writes, then another transaction should see either all or none of those writes, but not some subset.\nDurability # Durability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten.\nIn a single node system, it means that data has been written to nonvolatile storages\nIn a replicated system, it means that data has been copied to some number of nodes\nSingle-Object and Multi-Object Operations # Single-Object Operations # Atomicity and isolation are essential requirements for single-object operations.\nThe need for multiple-object transactions # Many distributed databases have abandoned multiple-object transactions because they are difficult to implement across partitions. However, there are still some scenarios requiring multiple-object transactions\nWhen updating denormalized information in a document data model, you need to update multiple documents in one go\nIn databases with secondary indexes (almost used everywhere except pure key-value stores), the indexes also need to be updated every time you change a value. These indexes are different database objects from a transaction point of view\nSecondary Index: In many databases, particularly relational databases, secondary indexes are used to improve the performance of queries. Unlike the primary index, which is typically based on the primary key, secondary indexes are built on other columns. Whenever you change a value in the database that is a part of a secondary index, the secondary index needs to be updated to reflect this change\nHandling Errors and Aborts # Retry Principles\nIt is only worth retrying after transient errors (for example due to deadlock, isolation violation, temporary network interruptions, and failover). A retry for permanent error (e.g., constraint violation) is pointless\nIf the error is due to overload, retrying the transactions will make the problem worse. In this case, consider limiting the number of retries, and use exponential backoff\nWeak Isolation Levels # Read Committed # The most basic level of transaction isolation. It makes two guarantees:\nWhen reading from the database, you will only see data that has been committed (no dirty reads).\nWhen writing to the database, you will only overwrite data that has been committed (no dirty writes).\nDirty Reads # Definition\nA transaction READS data that has been modified by another transaction but not yet committed\nRead committed isolation\nAny writes by a transaction only become visible to others when that transaction commits\nDirty Writes # Definition\nA transaction WRITES data that has been modified by another transaction but not yet committed.\nDirty writes happen when two transactions modify same data concurrently\nLimitation\nRead committed only works for two direct writes, but it can\u0026rsquo;t prevent race condition between two read-modify-write operations.\nFor example, there is a counter whose value is 1 in a table, and client 1 and client 2 read its value simultaneously. Then they increment the counter value concurrently. Even with read committed, the final value of the counter is 2 instead of 3\nImplementation # No dirty writes\nUsing row-level lock. When a transaction wants to modify a particular object (a row or a document), it must first acquire a lock on that object. It must then hold the lock until the transaction is committed or aborted. Only one transaction can hold the lock\nNo dirty reads\nUsing lock to prevent dirty reads is not realistic, cause one long-running writes would block all reads to that object\nThe workaround is that for every object that is written, the database remembers both the old committed value and the new value set by the transaction that is currently holding the write lock. While the transaction is ongoing, any other transactions that read the object are simply given the old value\nSnapshot Isolation and Repeatable Read # Non-Repeatable Read # A transaction reads the same data item more than once and gets different values each time. This inconsistency happens because another concurrent transaction modifies the data between the reads\nExample\nInitial State:\nAccount balance: $1000 Alice Transaction:\nStarts and reads the balance of account Reads balance as $1000 Transfer Transaction:\nStarts and updates the balance of account (e.g., deposits $100). New balance: $1100. Commits the change. Alice Transaction:\nReads the balance of account Now reads balance as $1100. Read committed can\u0026rsquo;t solve above problem, because in second read of Alice transaction, transfer transaction has been committed. No dirty read happens\nSnapshot Isolation # The transaction sees all data that was committed in the database at the start of the transaction. Any changes made after that time are not visible to the read transaction\nImplementing Snapshot Isolation # Writes\nImplementation of snapshot isolation normally uses write lock to prevent dirty writes, which means transaction that makes a write can block the progress of another transaction that writes to the same object\nReads\nMVCC (multi-version concurrency control): The database must potentially keeps several committed versions of an object to allow various in-progress transactions to see the state of the database at different points in time.\nThe benefit of MVCC is that readers won\u0026rsquo;t block any writers and vise verse.\nComparison with read committed\nRead committed only needs to keep two versions of an object: latest committed version and modified-but-not-yet-committed version, but snapshot isolation requires multiple versions of an object\nDatabase can also use MVCC to support read committed isolation. Read committed uses a separate snapshot for each QUERY (one transaction may contain multiple queries), while snapshot isolation uses a separate snapshot for each TRANSACTION.\nImplementation Example # Each transaction has a unique transaction ID (txid).\nEach row in the table has a created_by field, containing the ID of the transaction that inserted the row into the table\nMoreover, each row has a deleted_by field, which is initially is empty. If a transaction deletes a row, the row is not deleted from the table, but it is marked for deletion by setting the field deleted_by to the ID of the transaction that requested for deletion. When it is certain that no queries would access the deleted data, a garbage collection process removes rows that are marked for deletion\nAn update is translated to a create and a delete. For example, in above picture, transaction 13 update balance of the account to $1100. Then the row with balance of $1000 is marked as deleted by transaction 13 and a new row with balance of $1100 is created by transaction 13\nVisibility rules\nAt the start of each transaction, the database makes a list of all other transactions that are in progress at that time. Any writes made by those transactions are ignored\nAny writes made by aborted transactions are ignored\nAny writes made by transactions with a later txid are ignored\nAll other writes are visible\nEquivalent rules\nWhen reader transaction queries the database, below rows are visible:\nCreated rows (not deleted): the transactions that created the rows had already committed at the time when reader transaction started\nDeleted rows: the transaction that deleted the row are NOT yet committed at the time when reader transaction started\nIndexes of Snapshot Isolation # One option is to have the index simply point to all versions of an object and require an index query to filter out any object versions that are not visible to the current transaction. When garbage collection removes old object versions that are no longer visible to any transaction, the corresponding index entries can also be removed\nPreventing Lost Updates # The read committed and snapshot isolation levels we’ve discussed so far have been primarily about the guarantees of what a read-only transaction can see in the presence of concurrent writes.\nWe have only discussed dirty writes. There are several other kinds of conflicts that can occur between concurrently writing transactions:\nLost updates # Definition\nLost updates can occur when an application reads some value from the database, modifies it and writes back the modified value (a read-modify-write cycle). If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification\nScenarios\nIncrementing a counter (requires querying the current value, calculating the new value and writing back the updated value)\nMaking a change to a JSON document (requires parsing the document, making the change and writing back the modified document)\nTwo users editing a wiki page at the same time\nSolutions # Atomic write operations # An atomic write operation ensures that the entire read-modify-write sequence is executed as a single, indivisible unit. This means that once a transaction starts modifying a value, no other transaction can see or affect the intermediate states (blocks both writes and reads)\nImplementation Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied.\nExplicit Locking # If the built-in atomic operations can\u0026rsquo;t handle the use case, the application explicitly lock the object when it starts a read-modify-write cycle. If any other transactions try to concurrently read the object, they are forced to wait until the first read-modify-write cycle has completed\nAutomatically Detecting Lost Updates # This approach doesn\u0026rsquo;t lock the object being updated and allow in parallel updates. If the transaction manager detects a lost update, it aborts the transaction and force it to retry the read-modify-write cycle\nCompare-and-Set # Compare-and-set operation allows an update to happen only the value has NOT changed since you last read it.\nLimitation\nIf the value read is from an old snapshot, it may not prevent lost updates, because this condition may be true even though another concurrent write is occurring (this updates is not visible to the transaction)\nLost Updates in Replicated System # Locks and compare-and-set operations don\u0026rsquo;t work in a replicated system, because they can only prevent lost updates in a single node\nWe discussed before that replicated system allows concurrent writes in different nodes, and we can use application code to resolve and merge those conflicts\nAtomic, commutative operations can work well in a replicated context\nCommutative operations are those where the order of execution does not affect the final result. Examples include incrementing a counter or adding elements to a set\nFor instance, if you increment a counter by 1 on Replica A and by 1 on Replica B, the final result is the same regardless of the order in which these increments are applied when the replicas synchronize.\nWrite Skew and Phantoms # In previous sections, we discussed dirty writes and lost updates, two kinds of race conditions can occur when different transactions try to write to the SAME objects concurrently.\nWrite Skew Example # Consider a hospital shift management system where two doctors should be on call for every 12 hours. On call doctors can take a break but at least one doctor should be on call, in other words, at most one doctor can take a break\nInitial state:\nAlice\u0026rsquo;s status: on_call Bob\u0026rsquo;s status: on_call Transaction A (Alice):\nReads the number of doctors who are on call (database returns 2) Plans to take a break Transaction B (Bob):\nReads the number of doctors who are on call (database returns 2) Plans to take a break Transaction A (Alice):\nUpdates Alice\u0026rsquo;s status to in_break and commits Transaction B (Bob):\nUpdates Bob\u0026rsquo;s status to in_break and commits Final state:\nAlice\u0026rsquo;s status: in_break Bob\u0026rsquo;s status: in_break The rule that at least one doctor should be on call is violated\nWrite Skew # Write skew pattern:\nA query checks whether a condition is satisfied. In above example, the number of on call doctors should be larger than 1\nIf the condition is satisfied and the application decides to go ahead, it then makes a write to other objects in the database. In above example, doctor\u0026rsquo;s status is updated from on_call to in_break\nPhantom\nDifferent transactions query same data. When the condition is satisfied, all transactions make a write to different objects. If the write to the objects breaks the condition in the query step, then write skew will occur.\nIn above example, no matter Alice or Bob updates their status from on_call to in_break, the condition that number of on call doctors should be larger than 1 is not satisfied anymore. In that case, the write made later should be aborted instead be committed.\nThis effect, where a write in one transaction changes the result of a search query in another transaction, is called a phantom\nWrite Skew Solution # Serializable isolation\nLocks all objects that are queried in the first step\nUnfortunately, this approach doesn\u0026rsquo;t work if the search query if checking the absence of rows matching some search conditions, and the write adds a row matching the same condition.\nFor example, in a meeting room booking system, before booking a room in the requested time spots, you need to search if any entries with the same room ID and time spots exist in the database, if not, then you insert a new entry with the room ID and time spots. In this example, you can\u0026rsquo;t lock any entries in the search query because no entries are matched.\nSerializability # Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency.\nActual Serial Execution # The simplest way of avoiding concurrency problems is to remove the concurrency entirely: to execute only one transaction at a time, in serial order, on a single thread.\nA system designed for single-threaded execution can sometimes perform better that a system that supports concurrency, because it can avoid the coordination overhead of locking.\nEncapsulating transactions in stored procedures # Interactive multiple-statement transaction\nAn interactive multiple-statement transaction is a type of transaction where multiple SQL statements are executed as part of a single transaction, and these statements are sent interactively from application code over the network to the database server. This means that there is significant network overhead involved, as each statement requires a round trip between the application and the database server.\nIn the interactive style of transactions, a lot of time is spent in network communication between the application and the database. If a database disallow concurrency, then interactive transactions will make the throughput and the performance dreadful.\nStored procedure\nFor this reason (Multiple network roundtrips in one transaction overloads the database server), systems with single-threaded serial transaction processing disallow interactive multiple-statement transactions. Applications need to store the transactional codes in the database ahead of time, as stored procedure, which contains multiple read or write statements and business logics. This approach \u0026ndash; one request is sent from the application to the database server, and then database executes the stored procedure \u0026ndash; eliminates the overhead of network communications.\nExample of stored procedure\nWrite Overheads # Systems with Single-threaded serial transactions can perform well for applications with high read throughput and low write throughput. Compared with read operations, write operations bring more overheads:\nData modifications and persistence\nData modified by write operations need to be durable. That typically involves writing to disk or other permanent storage.\nMost databases use write-ahead logging (WAL) to ensure durability and atomicity. Each write operation generate log entries which must be written to disk before the actual data modification.\nIndex maintenance\nWrite operations require updates to one or more indexes to keep them in sync with the underlying data Resource utilization\nWrite operations often involve advanced operations like constraint checking, trigger execution, and logging Error handling and rollback\nIf a write operation encounters an error, the database must roll back the entire change and retry in some cases Concurrent writes\nConflict resolution: Concurrent writes can lead to conflicts that the database must detect and resolve Locking: Write operations often require locking on the data modified to support concurrent writes Partitioning # Considering so many write overheads (concurrent writes are not applied here), high write throughput would overwhelm single-threaded systems. One solution is to partition your data, so you can scale your system linearly with the number of partitions.\nLimitation\nThis approach only works well if applications read and write the data in one single partition. If cross-partition transactions are required, then linear scalability is not realistic\nTwo-Phase Locking (2PL) # Two-phase locking requires strong locking on objects. It allows multiple transactions to read same objects, but if any transactions want to write an object, exclusive access is required\nReaders block other writers\nWriters block other readers and writers\nImplementation of Two-Phase Locking # Every object in the system has a lock. The lock could either be in shared mode or in exclusive mode\nIf a transaction wants to read an object, it must acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if a transaction already has an exclusive lock on the object, these transactions must wait\nIf a transaction wants to write, it must acquire the lock in exclusive mode. No other transaction can hold the lock at the same time (either in shared or in exclusive mode). In other words, readers block writes\nIf a transaction first reads and then writes an object, it must upgrade its lock from shared mode to exclusive mode. The upgrade works the same as getting an exclusive lock directly\nAfter a transaction has the acquired lock, it must hold it until the transaction commits or aborts\nPoor Performance of Two-Phase Locking # Overhead of acquiring and releasing locks. The isolation level is too strong. If two transactions try to do anything that may in any way result in a race condition, one transaction has to wait for the other to complete\nDeadlock. Because every object has a lock, and both read and write operations need to acquire the lock, the deadlock can happen quite easily that transaction A is waiting for transaction B to release the lock and vise verse.\nFor example, in the on call shift management system\nDoctor Alice and Bob are on call Transaction A (Alice) searches doctors that are on call, and it acquires the locks of two entries: Alice and Bob\u0026rsquo;s on call entries Transaction B (Bob) searches doctor that are on call, and it also acquires locks in shared mode for the same entries When either transaction tries to upgrade its lock from shared mode to exclusive mode, it will be blocked by the other transaction, and vise verse. Here, the deadlock occurs. When database detects this, it must abort all other transactions to allow only one transaction to acquire the exclusive lock\nPredicate Locks # In some types of phantoms, the rows in the query is absent, serializable isolation should potentially handle this problem. In the meeting room booking example, the condition of booking a room is no entries with same room ID and time spots. How could two-phase locking achieve this?\nPredicate locks\nPredicate locks are a type of database lock that lock search conditions or predicates instead of specific data objects (like rows or tables). For below search statements:\nSELECT * FROM bookings WHERE room_id = 123 AND end_time \u0026gt; \u0026#39;2018-01-01 12:00\u0026#39; AND start_time \u0026lt; \u0026#39;2018-01-01 13:00\u0026#39;; The transaction must first acquire shared lock on the search conditions. If a transaction already has the exclusive lock on the same search conditions, others have to wait until the lock is released\nLimitation of predicate locks\nGranularity and lock explosion\nWhen search conditions are highly specific, each unique condition potentially requires a separate lock. That may lead to a situation known as \u0026ldquo;lock explosion\u0026rdquo;, where many locks are created\nLock management and checking\nThe database must track each lock, which consumes memory and processing power. Each operation must be checking against all relevant predicate locks to ensure no conflicts\nIndex-range locks # The root cause of above limitations is that the search conditions associated with predicate locks are too granular causing too many locks are created. The optimization is to lock against less specific conditions.\nIndex-range locks simplify a predicate by making it match a greater set of object. For example, if you have a predicate lock for bookings of room 123 between noon and 1 p.m., you can approximate it by locking bookings for room 123 at any time, or you can approximate it by locking all rooms (not just room 123) between noon and 1 p.m. This is safe, because any write that matches the original predicate will definitely also match the approximations\nIn the room booking system, you may have an index on the room_id, and database uses this index to find existing bookings for room 123. Now the database can simply attach a shared lock to this index entry, indicating that a transaction has searched for bookings of room 123.\nA lock is attached to the index. When a transaction wants to make a write to an entry, it must update the index. In the processing of doing so, it will encounter a shared lock, and it will be forces to wait until the lock is released\nTrade-offs\nIndex-range locks are less precise as predicate locks would be, so they lock a larger range of objects, but it creates less number of locks\nSerializable Snapshot Isolation # Pessimistic Versus Optimistic Concurrency Control # Two-phase locking is a so-called pessimistic concurrency control mechanism. Its principle is that if anything might go wrong (as indicated by a lock held by another transaction), it\u0026rsquo;s better to wait until the situation is safe again before doing anything\nSerial execution is pessimistic to the extreme: it is equivalent to each transaction having an exclusive lock on the entire database (or one partition of the database) for the duration of the transaction\nserializable snapshot isolation\nBy contrast, serializable snapshot isolation is an optimistic concurrency control technique. If anything could go wrong, instead of blocking, transactions continue anyway, in the hope that everything will turn out all right. When a transaction wants to commit, the database checks if anything bad happened; if so, the transaction is aborted and has to be retried.\nSSI is based on snapshot isolation \u0026ndash; that is, all reads within the transaction are from a consistent snapshot of the database. This is the main difference compared to optimistic concurrency control technique.\nDecision Based on An Outdated Premise # Write skew pattern in snapshot isolation: a transaction reads some data from the database, examines the result of the query, and decide to take some action based on the results. However, the result from the original query may not be up-to-date by the time the transaction commits, because the data may have been modified in the meantime by other transactions\nHow does the database know if a query result might have changed? There are cases to consider:\nDetecting reads of a stale MVCC object version (uncommitted write occurred before the read)\nDetecting writes that affect prior reads (the write occurs after the right)\nDetecting stale MVCC reads\nSnapshot isolation is usually implemented by multi-version concurrency control (MVCC). When a transaction reads from a consistent snapshot, it ignores writes that were made but not yet committed at the time when the snapshot was taken.\nIn order to prevent this anomaly, the database needs to track writes that were ignored by the transaction due to MVCC visibility rule. When the transaction wants to commit, the database check if any of the ignored writes have now been committed. If so, the transaction must be aborted\nDetecting writes that affect prior reads\nWhen a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the modified data. The process is similar to acquire a write lock, but rather than blocking until the readers have committed, the transaction continues its process and notify other transaction the data they read is not up-to-date after it commits\nPerformance of Serializable Snapshot Isolation # As always, many engineering details affect how well an algorithm works in practice. For example, one trade-off is the granularity at which transactions’ reads and writes are tracked. If the database keeps track of each transaction’s activity in great detail, it can be precise about which transactions need to abort, but the bookkeeping overhead can become significant. Less detailed tracking is faster, but may lead to more transactions being aborted than strictly necessary.\nCompared to two-phase locking, the big advantage of serializable snapshot isolation is that read-only queries can run on a consistent snapshot without requiring any locks, which is very appealing for read-heavy workloads.\n"},{"id":41,"href":"/docs/programming/backend/java/nio/selector-read/","title":"Selector Read","section":"Nio","content":" Read Operation in Selector # In last post, we discussed Selector and how it works, also introduced an interested operation OP_ACCEPT. In this post, I will introduce another interested operation OP_READ and how to gracefully read data from SocketChannel and handle closed socket channels\nServer Code # @Slf4j public class Server { public static void main(String[] args) throws IOException { ByteBuffer buffer = ByteBuffer.allocate(32); Selector selector = Selector.open(); log.debug(\u0026#34;Create a selector\u0026#34;); ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(9999)); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT, null); log.debug(\u0026#34;Register ServerSocketChannel with selector\u0026#34;); while (true) { Print.printDelimiter(); log.debug(\u0026#34;Listen to events from selection keys\u0026#34;); selector.select(); Set\u0026lt;SelectionKey\u0026gt; selectionKeys = selector.selectedKeys(); log.debug(\u0026#34;Selected {} key(s)\u0026#34;, selectionKeys.size()); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = selectionKeys.iterator(); while(keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); sc.configureBlocking(false); log.debug(\u0026#34;Connected to client {}\u0026#34;, sc.getRemoteAddress()); SelectionKey scKey = sc.register(selector, 0, null); scKey.interestOps(SelectionKey.OP_READ); log.debug(\u0026#34;Register SocketChannel with selector\u0026#34;); } else if (key.isReadable()) { SocketChannel channel = (SocketChannel) key.channel(); log.debug(\u0026#34;Read from client {}\u0026#34;, channel.getRemoteAddress()); channel.read(buffer); buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); } keyIterator.remove(); } } } } Compared with server code in post, the main change is inside the second while loop. Let\u0026rsquo;s analyze line by line to see the difference.\nConnect to clients\nif (key.isAcceptable()) { ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); ... } Here I first check if the key is an acceptable operation. If so, then I cast the channel from the selected key to ServerSocketChannel and let server accepts client\u0026rsquo;s connection Register read operation for ServerSocket\nif (key.isAcceptable()) { ... sc.configureBlocking(false); SelectionKey scKey = sc.register(selector, 0, null); scKey.interestOps(SelectionKey.OP_READ); } After creating a new SocketChannel, I register read operation for the channel with the selector by invoking SelectionKey.interestOps(int ops) method A new SelectionKey is added into keys Client sends messages\nelse if (key.isReadable()) { SocketChannel channel = (SocketChannel) key.channel(); log.debug(\u0026#34;Read from client {}\u0026#34;, channel.getRemoteAddress()); channel.read(buffer); buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); } Assume a client sends messages to the server. In that case, the SocketChannel key is selected and added to selectedKeys The data is read from the channel to the ByteBuffer and messages are printed in the terminal Demo # Start the server 23:25:01.524 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Create a selector 23:25:01.531 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register ServerSocketChannel with selector ---------------------------------------------------- 23:25:01.531 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys A client connects 23:25:25.658 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 23:25:25.660 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47346 23:25:25.660 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register SocketChannel with selector ---------------------------------------------------- 23:25:25.660 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys The client sends messages 23:26:32.911 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 23:26:32.911 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Read from client /127.0.0.1:47346 23:26:32.911 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server! ---------------------------------------------------- 23:26:32.912 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys Open Questions: Why need to remove selected keys? # keyIterator.remove(); select method only adds items to SelectedKeys, but it doesn\u0026rsquo;t remove items. Hence, each time after handling event, we need to remove it. That\u0026rsquo;s why I use Iterator instead of for loop here. The better explain the problem if items are not removed, let\u0026rsquo;s comment the remove code and run above demo again.\nStart the server\n20:44:26.948 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Create a selector 20:44:26.955 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register ServerSocketChannel with selector ---------------------------------------------------- 20:44:26.956 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys When starting the server, there is one key created, but no keys is added to SelectedKeys Client connects\n21:16:52.573 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 21:16:52.582 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47438 21:16:52.582 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register SocketChannel with selector ---------------------------------------------------- 21:16:52.582 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys A client connects to the server, the ssc key is added to SelectedKeys. select method is not blocked. A new ServerSocket is created and a new SelectionKeys is registered with the selector Cause this time we didn\u0026rsquo;t remove the ssc key, so it is still in the SelectedKeys Client sends messages\n21:23:20.865 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 2 key(s) Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at com.whatsbehind.netty_.nio.selector_.Server.main(Server.java:39) The client sends a message to the server, the sc (server socket) key is added to SelectedKeys, and two keys are selected When handle Accept event in the ssc key, this time there is no connection from clients, and cause the ServerSocketChannel is set as non-blocking mode, in line #39, what channel.accept() returns is null. Hence line #39 throws NullPointerException #37 ServerSocketChannel channel = (ServerSocketChannel) key.channel(); #38 SocketChannel sc = channel.accept(); #39 sc.configureBlocking(false); Clients disconnect # When clients disconnect, no matter it is normal SocketChannel close or exit with code -1, the connected SocketChannelin the server will receive a read event, but length of the message body is 0. Hence, current implementation channel.read(buffer); can\u0026rsquo;t handle client disconnection properly, and selector will keep selecting key of that channel and it causes infinite loop.\nMethod channel.read(buffer) returns an int value which is length of the byte read from the channel. When the read event is client disconnection, the method returns -1, so we can modify our code like below\nSocketChannel channel = (SocketChannel) key.channel(); int len = channel.read(buffer); if (len == -1) { key.cancel(); } else { buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); } We explicitly check the returned int from channel.read(buffer). If the value is -1, then we invoke key.cancel() method to deregister the SocketChannel from the selector After cancel, the selector stops listening to event from that channel, in other words, the key is removed from the keys Let\u0026rsquo;s take a look at the execution results:\n09:57:38.118 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Create a selector 09:57:38.128 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register ServerSocketChannel with selector ---------------------------------------------------- 09:57:38.129 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys 09:57:40.136 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 09:57:40.137 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47628 09:57:40.137 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register SocketChannel with selector ---------------------------------------------------- 09:57:40.137 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys 09:57:42.913 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 09:57:42.913 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Read from client /127.0.0.1:47628 09:57:42.913 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Client /127.0.0.1:47628 is closed ---------------------------------------------------- 09:57:42.914 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys "},{"id":42,"href":"/docs/programming/backend/java/thread/thread-safety/","title":"Thread Safety","section":"Thread","content":" Example and Best Practice # Shared Mutable State # Unsafe Use public class Counter { private int count = 0; public void increment() { count++; } public int getCount() { return count; } } Better Practice public class Counter { private int count = 0; public synchronized void increment() { count++; } public synchronized int getCount() { return count; } } - Adding `synchronized` on the methods ensure each method "},{"id":43,"href":"/docs/programming/backend/java/nio/io-model-summary/","title":"I/O  Model Summary","section":"Nio","content":" I/O Model # We have discussed some I/O models, like blocking I/O model and non-blocking I/O model. These two I/O models have their own problems which are low efficient.\nThen I introduce another I/O model Multiplexing which utilizes Selector to monitor registered keys and handle operations in a batch. This model provides high efficiency and handles multiple operations like accept (client connection) and read concurrently.\nIn this post, I will introduce two more I/O models, synchronous and asynchronous.\nSynchronous I/O Model # Definition # In the synchronous I/O model, the thread initiates the input or output operations and then it is blocked until the operation is completed. The thread remains inactive and can not perform any other tasks during this time. The thread resumes its executions after the operation has finished\nKey factors # Only one thread is involved to initiate the operation and get the result The thread is idle or blocked when waiting for the operation completing Linear execution flow Example # Code\npublic static void main(String[] args) throws IOException { FileChannel fc = FileChannel.open(Paths.get(\u0026#34;/path/to/file/data.txt\u0026#34;), StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(16); fc.read(buffer); buffer.flip(); ByteBufferReader.readAll(buffer); } Execution result\n16:18:51.605 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello World! The example of synchronous I/O model is quite straight forward and easy. Only one thread is involved and the thread is blocked at blocking method fc.read(buffer) until data is loaded from the disk and copied to memories\nAsynchronous I/O Model # Definition # In asynchronous I/O model, when a thread initiates an I/O operation, it is not blocked and wait until the operation completes. Instead, the thread continues to execute, and another thread works on the I/O operation. Once the operation is complete, the initiating thread will be notified through mechanism like callbacks\nKey factors # Multiple threads are involved.\nOne thread is called main thread, which initiates the I/O operation, and it is not blocked during the operation. Instead, it continues to execute other tasks. Another thread is called working thread, its main task is to complete the I/O operation and calls back the main thread when the operation is complete. Main thread is not blocked\nConcurrency: Main thread can handle multiple I/O operations concurrently\nExample # Code\npublic static void main(String[] args) throws IOException, InterruptedException { AsynchronousFileChannel afc = AsynchronousFileChannel.open(Paths.get(\u0026#34;/path/to/file/data.txt\u0026#34;), StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(16); log.debug(\u0026#34;Start reading...\u0026#34;); afc.read(buffer, 0, buffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override public void completed(Integer integer, ByteBuffer buffer) { buffer.flip(); ByteBufferReader.readAll(buffer); } @Override public void failed(Throwable throwable, ByteBuffer buffer) { throwable.printStackTrace(); } }); log.debug(\u0026#34;Finish reading...\u0026#34;); Thread.sleep(1000); } In afc.read(...) method of the async file channel, I pass in a callback object which has two methods completed and failed which are invoked by the working thread when the read operation completes or throws. I insert three logs before and after the read method and inside the completed method In the end of the code snippet, I force the main thread to sleep 1s. Because the working thread is a daemon thread, it terminates when main thread finishes all tasks. Sleeping 1s allows the work thread to complete Execution result\n16:34:14.824 [main] DEBUG com.whatsbehind.netty_.nio.asynchronous_.CallBackFileReader - Start reading... 16:34:14.826 [main] DEBUG com.whatsbehind.netty_.nio.asynchronous_.CallBackFileReader - Finish reading... 16:34:14.828 [Thread-0] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello World! Two threads are involved, main thread (main) and working thread (Thread-0) Logs Start reading... and Finish reading... are printed first and the thread is main thread. That means after initiating the read operation afc.read(...), the read operation is allocated to the working thread and the main thread is not blocked but continues to execute other tasks The data read from the disk later is printed and its executing thread is the working thread. That means the two threads are running concurrently Is Future asynchronous? # Code\npublic static void main(String[] args) throws IOException, InterruptedException, ExecutionException { AsynchronousFileChannel afc = AsynchronousFileChannel.open(Paths.get(\u0026#34;/path/to/file/data.txt\u0026#34;), StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(16); log.debug(\u0026#34;Start reading...\u0026#34;); Future\u0026lt;Integer\u0026gt; future = afc.read(buffer, 0); // Blocking method: main thread is blocked here until read operation completes future.get(); buffer.flip(); ByteBufferReader.readAll(buffer); log.debug(\u0026#34;Finish reading...\u0026#34;); Thread.sleep(1000); } Instead of utilizing callbacks to notify the main thread. We can use Future object to get the data after the read operation After calling afc.read(...), I invoke a blocking method future.get() to let the main thread wait here until the read operation completes. During this time, the working thread is working on the read operation and the main thread is blocked Execution result\n16:46:47.948 [main] DEBUG com.whatsbehind.netty_.nio.asynchronous_.FutureFileReader - Start reading... 16:46:47.952 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello World! 16:46:47.952 [main] DEBUG com.whatsbehind.netty_.nio.asynchronous_.FutureFileReader - Finish reading... The above code works as synchronous I/O model. Codes are executed like a linear way and all logs are printed in the main thread Is it asynchronous?\nIn my opinion, NO! This way has two threads involved, but comparing with synchronous I/O model, main thread only transfers its read job to the working thread. Main thread is still blocked at method future.get(), and it can\u0026rsquo;t execute other tasks. Further more, comparing with synchronous I/O model, this way has one more thread involved but doesn\u0026rsquo;t allow main to execute multiple tasks concurrently.\n"},{"id":44,"href":"/docs/programming/backend/java/nio/multiple-threads/","title":"Multiple Threads","section":"Nio","content":"In our previous posts of using Selector for connection and communication (read/write) between server and clients, we only utilize only one thread. Multiplexing is highly efficient, but some time-consumed tasks would affect the overall performance.\nIn this post, I will mimic some functions from Netty which allocates tasks to different threads to improve the performance\nComponent # We will focus on the components in the server side\nBoss # Boss runs under one thread Boss maintains one selector which only listens to ACCEPT events, in other words, boss is only responsible to accept client connections Worker # Worker runs in a separate thread Each worker has one selector which listens to READ and WRITE events After client\u0026rsquo;s connection, the newly created SocketChannel will be registered with one worker Architecture # Connect\nA client connects to the server Boss accepts the connection and creates a SocketChannel Register\nBoss registers the SocketChannel with one worker Worker starts to listen to READ and WRITE events from the newly registered channel Boss is decoupled with the channel Binding After registration, the SocketChannel is bound to the worker, and decoupled from the boss I/O operations including READ and WRITE are monitored and handled by the worker Implementation # Worker # @Getter @Setter @Slf4j public class Worker implements Runnable { private Thread thread; private Selector selector; private String name; private boolean start; public Worker(String name) { this.name = name; } public void register(SocketChannel sc) throws IOException { if (!start) { start = true; selector = Selector.open(); thread = new Thread(this, this.name); thread.start(); log.debug(\u0026#34;Start selector and thread in {}\u0026#34;, name); } sc.configureBlocking(false); sc.register(selector, SelectionKey.OP_READ, null); log.debug(\u0026#34;Register client [{}] with {}\u0026#34;, sc.getRemoteAddress(), name); } @Override public void run() { while (true) { try { log.debug(\u0026#34;{} is listening...\u0026#34;, name); selector.select(); Iterator\u0026lt;SelectionKey\u0026gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); if (key.isReadable()) { SocketChannel sc = (SocketChannel) key.channel(); log.debug(\u0026#34;Reading message from client [{}]\u0026#34;, sc.getRemoteAddress()); ByteBuffer buffer = ByteBuffer.allocate(16); sc.read(buffer); buffer.flip(); ByteBufferReader.readAll(\u0026#34;Message: \u0026#34;, buffer); } } } catch (IOException e) { throw new RuntimeException(e); } } } } Field Selector: Monitors and handles read/write operations from registered channels Thread: Task executor Method run(): Worker implements Runnable. This method only does one thing: selector.select(): Listens to channel\u0026rsquo;s events Handle events register(): First time call: Create the Selector Create and start the thread Register the channel with the Selector Problems: # Let\u0026rsquo;s look at the two main operations of worker:\nmonitor (selector.select()); registration (sc.register(selector, SelectionKey.OP_READ, null);) Monitor is executed by the worker thread, cause method register() is invoked by other thread, so registration is executed by other thread (boss).\nWhile worker thread is blocked at line selector.select(), even though boss registers a new channel, the worker can\u0026rsquo;t start to monitor events from the new channel immediately until next round of monitor\nFix:\n@Getter @Setter @Slf4j public class Worker implements Runnable { private Thread thread; private Selector selector; private String name; private boolean start; + private ConcurrentLinkedQueue\u0026lt;Runnable\u0026gt; tasks = new ConcurrentLinkedQueue\u0026lt;\u0026gt;(); public Worker(String name) { this.name = name; } public void register(SocketChannel sc) throws IOException { if (!start) { start = true; selector = Selector.open(); thread = new Thread(this, this.name); thread.start(); log.debug(\u0026#34;Start selector and thread in {}\u0026#34;, name); } sc.configureBlocking(false); - sc.register(selector, SelectionKey.OP_READ, null); - log.debug(\u0026#34;Register client [{}] with {}\u0026#34;, sc.getRemoteAddress(), name); + tasks.add(() -\u0026gt; { + try { + sc.register(selector, SelectionKey.OP_READ, null); + } catch (IOException e) { + throw new RuntimeException(e); + } + }); + selector.wakeup(); } @Override public void run() { while (true) { try { log.debug(\u0026#34;{} is listening...\u0026#34;, name); selector.select(); + if (!tasks.isEmpty()) { + tasks.poll().run(); + log.debug(\u0026#34;Register client with {}\u0026#34;, name); + } Iterator\u0026lt;SelectionKey\u0026gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); iterator.remove(); if (key.isReadable()) { SocketChannel sc = (SocketChannel) key.channel(); log.debug(\u0026#34;Reading message from client [{}]\u0026#34;, sc.getRemoteAddress()); ByteBuffer buffer = ByteBuffer.allocate(16); ByteBuffer buffer = ByteBuffer.allocate(32); sc.read(buffer); buffer.flip(); ByteBufferReader.readAll(\u0026#34;Message: \u0026#34;, buffer); } } } catch (IOException e) { throw new RuntimeException(e); } } } } I create a Queue as the bridge between two threads\nInstead let other thread execute the registration, I add a Runnable task in the queue, and let the worker thread execute the registration\nTo fix the bug that the selector can\u0026rsquo;t immediately monitor newly registered channels while it is monitoring (blocked at selector.select()), I invoke method selector.wakeup() which unblocks the selector and let it continue to execute codes after adding the task to the queue,\nBefore handling events from selected keys, worker will first check if there are any registration tasks in the queue and do the registration if yes\n"},{"id":45,"href":"/docs/programming/backend/java/net/tcp-socket/","title":"Tcp Socket","section":"Net","content":" What Is a Socket? # Server socket listens to a port Normally, a server runs on a specific computer and has a socket that is bound to a specific port number. The server just waits, listening to the socket for a client to make a connection request.\nCient connects to the server with server ip and port On the client-side: The client knows the hostname of the machine on which the server is running and the port number on which the server is listening. To make a connection request, the client tries to rendezvous with the server on the server\u0026rsquo;s machine and port. The client also needs to identify itself to the server so it binds to a local port number that it will use during this connection. This is usually assigned by the system.\nServer accepts connection and creates a new socket If everything goes well, the server accepts the connection. Upon acceptance, the server gets a new socket bound to the same local port and also has its remote endpoint set to the address and port of the client. It needs a new socket so that it can continue to listen to the original socket for connection requests while tending to the needs of the connected client.\nSocket is successfully created on client On the client side, if the connection is accepted, a socket is successfully created and the client can use the socket to communicate with the server.\nThe client and server can now communicate by writing to or reading from their sockets. [1]\nExamples # Create Sockets # Server\npublic class Server_ { public static void main(String[] args) throws IOException { // A server socket listens to port 8888 ServerSocket serverSocket = new ServerSocket(8888); // The program is blocked here waiting for client connecting Socket socket = serverSocket.accept(); // Your business logic ... // Close both sockets to release system resource socket.close(); serverSocket.close(); } } First we create a ServerSocket listening to port 8888 Before accepting any connections, the program is blocked at line Socket socket = serverSocket.accept(); A client requests to connect to the server. After the server accepts the request, a new socket will be created in the server After your business logic, close the socket and server socket to release system resources Client\npublic class Client_ { public static void main(String[] args) throws IOException { // Create a new socket and connect to port 8888 on the server Socket socket = new Socket(InetAddress.getByName(\u0026#34;\u0026lt;host-name\u0026gt;\u0026#34;);, 8888); // Your business logic ... // Close the socket to release system resource socket.close(); } } First we create a socket connecting to port 8888 of the host Once the connection is successfully created, a socket will be created After your business logic, close the socket to release system resources Upload File from Client to Server # Client\npublic class Client_ { public static void main(String[] args) throws IOException { // Create a new socket and connect to a specific port of the server Socket socket = new Socket(InetAddress.getLocalHost(), 8888); // Read file from disk String filePath = \u0026#34;/path/to/the/file\u0026#34;; FileInputStream bis = new FileInputStream(filePath); byte[] data = inputStreamToByteArray(bis); bis.close(); System.out.println(\u0026#34;Client reads a file from disk\u0026#34;); // Send byte data of the file to the server OutputStream os = socket.getOutputStream(); os.write(data); // You need to shut down the output stream, otherwise the server will hang up // It\u0026#39;s a signal let server know that you\u0026#39;re done sending data // TODO: What\u0026#39;s the corresponding TCP packet for this? socket.shutdownOutput(); System.out.println(\u0026#34;Client uploads a file to server\u0026#34;); // Close streams and socket os.close(); socket.close(); } } Client reads a file in the disk to a FileInputStream, and invoke an utility method inputStreamToByteArray to convert InputStream to a byte array Then client writes the byte data into its output stream and sends it to the server Close all streams and socket to release system resources Utility method:\npublic class InputStreamUtil { public static byte[] inputStreamToByteArray(InputStream is) throws IOException { byte[] buf = new byte[1024]; ByteArrayOutputStream bos = new ByteArrayOutputStream(); int len; while((len = is.read(buf)) != -1) { bos.write(buf, 0, len); } byte[] result = bos.toByteArray(); bos.close(); return result; } } The utility method creates a ByteArrayOutputStream as a buffer to store data read from the input stream Contantly reads data from the input stream and writes the data into the ByteArrayOutputStream Invokes toByteArray method to get a byte array holding the data Server\npublic class Server_ { public static void main(String[] args) throws IOException { // A server socket waits for requests to come in over the network. ServerSocket serverSocket = new ServerSocket(8888); // Listens to a connection and accepts it. Returns a new socket Socket socket = serverSocket.accept(); // Read data from input stream InputStream is = socket.getInputStream(); byte[] data = inputStreamToByteArray(is); System.out.println(\u0026#34;Server received file uploaded from client\u0026#34;); // Save the file to local disk String filePath = \u0026#34;/path/to/save/the/file\u0026#34;; BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(filePath)); bos.write(data); bos.close(); // Close streams and socket is.close(); socket.close(); serverSocket.close(); } } Server socket listens to port 8888 in local host Creates a new socket to accept connection from the client Reads data from socket\u0026rsquo;s input stream Writes byte data to disk through FileOutputStream Close all streams and sockets to release system resources "},{"id":46,"href":"/docs/programming/backend/java/io/decorator-pattern-in-java-io/","title":"Decorator Pattern in Java IO","section":"Io","content":" Node Streams (Low-Level Streams) # Definition:\nNode Streams connect directly with the source of the data They read data from or write data to a specific location (like a file, memory, or network socket). Example: FileInputStream is a node stream that reads byte data from a file.\nFile file = new File(\u0026#34;example.txt\u0026#34;); FileInputStream fis = new FileInputStream(file); Here, FileInputStream is directly reading the bytes from the file \u0026ldquo;example.txt\u0026rdquo;. It\u0026rsquo;s a direct connection between the Java program and the file.\nProcessing Streams (High-Level Streams) # Definition:\nProcessing Streams are built on top of node streams to provide additional functionality, like buffering, filtering, reading objects, etc. They don\u0026rsquo;t write or read data directly to or from data source, but delegate this job to node streams. Example: BufferedInputStream is a processing stream that adds buffering capabilities to another input stream, such as FileInputStream.\nFileInputStream fis = new FileInputStream(\u0026#34;example.txt\u0026#34;); BufferedInputStream bis = new BufferedInputStream(fis); BufferedInputStream does not connect directly to the file. Instead, it wraps the FileInputStream and adds buffering to it. When you read from a BufferedInputStream, it retrieves data from the buffer, and when the buffer is empty, it reads another large chunk of data from the FileInputStream and fills the buffer. This minimizes the number of interactions with the file system, which is much slower than reading from a buffer in memory.\nCombining Node and Processing Streams # Typically, you chain processing streams together to get both their benefits. For instance, wrapping a FileInputStream with a BufferedInputStream.\nFile file = new File(\u0026#34;example.txt\u0026#34;); InputStream is = new BufferedInputStream(new FileInputStream(file)); ... is.close(); In this example, we use FileInputStream to connect to the file and BufferedInputStream to add buffering. The data still comes from the file, but it passes through the buffer, which can be read from more quickly than the file.\nDecorator Pattern in Java IO Stream # Component (Node Stream) # Definition: The component is the primary or underlying object that has the original behavior, which is to read data from data source for node streams. In Java IO: FileInputStream and FileReader are example of components. They provide the basic functionality for byte and character stream handling. Decorator (Processing Stream) # Definition: Decorators implement or extend the same interface as the component they are going to decorate. They compose the instance of component and provide an enhanced interface with added responsibilities.- In Java IO: BufferedInputStream, BufferedOutputStream, BufferedReader, and BufferedWriter are examples of decorators. They add buffering to the streams they decorate. Key Features of Decorator # Delegation: The decorator delegates the work to the component it decorates and then adds its own behavior before or after delegating. This is a key feature because it means the decorator itself doesn\u0026rsquo;t handle the primary operations; it relies on the component to do so.\npublic class BufferedInputStream extends InputStream { private InputStream inner; public BufferedInputStream(InputStream inner) { this.inner = inner; } public int read() throws IOException { ... // own behavior of decorator return inner.read(); } } Add Features Over Component Dynamically: Because decorators implement or extend the same interface as the component they are going to decorate, that means they can be decorated by other decorators. Decorators can be nested and combined in any order to add multiple behaviors.\nInputStream input = new FileInputStream(\u0026#34;file.txt\u0026#34;); InputStream buffered = new BufferedInputStream(input); InputStream dataInput = new DataInputStream(buffered); Q \u0026amp; A of Decorator Pattern # What\u0026rsquo;s the main difference between component and decorator?Component and decorator both implement or extend same interface or superclass. However, components don\u0026rsquo;t compose an instance of the interface or superclass, so it can only perform the original behavior. Decorators compose an instance of the interface or superclass, which can be delegated to perform the original behavior. What\u0026rsquo;s the difference between strategy and decorator pattern?Key feature for decorator and strategy pattern is delegation. However, decorators extend or implements same interface or superclass it decorates, which mean decorators themselves can be decorated also, which add extra features dynamically during runtime. "},{"id":47,"href":"/docs/programming/backend/java/io/object-input-and-output-stream/","title":"ObjectInputStream and ObjectOutputStream","section":"Io","content":"This post will introduce two new processing streams, ObjectInputStream and ObjectOutputStream, which are used to deserialize and serialize objects and primitive data.\nObjectInputStream # Purpose: To deserialize objects and primitive data written using ObjectOutputStream. It allows you to read bytes from a source (like a file or network socket) and reconstructs objects from those bytes.\nKey Features: Processing stream: reads serialized objects from an underlying InputStream.\nCommon Use Case: Commonly used in networking (for sending objects across a network) or for persisting objects to files.\nExample of ObjectInputStream # Deserialize objects from a file\nFileInputStream fis = new FileInputStream(\u0026#34;example.ser\u0026#34;); ObjectInputStream ois = new ObjectInputStream(fis); MyClass o = (MyClass) ois.readObject(); ois.close(); Deserialize objects from network\nSocket socket = new Socket(\u0026#34;example.com\u0026#34;, 8080); InputStream is = socket.getInputStream(); ObjectInputStream ois = new ObjectInputStream(is); MyClass o = (MyClass) ois.readObject(); ois.close(); ObjectOutputStream # Purpose: To serialize objects and primitive data types to an OutputStream. It converts objects into a byte stream that can then be written to a file, sent over a network, etc.\nKey Features: Processing stream: serializes objects and primitives to an underlying OutputStream.\nCommon Use Case: Used for saving object states, caching objects, or sending objects over a network.\nExample of ObjectInputStream # Serialize objects to a file\nFileOutputStream fos = new FileOutputStream(\u0026#34;example.ser\u0026#34;); ObjectOutputStream oos = new ObjectOutputStream(fos); MyClass o = MyClass(); oos.writeObject(o); oos.close(); Serialize objects to network\nSocket socket = new Socket(\u0026#34;example.com\u0026#34;, 8080); OutputStream os = socket.getOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(os); MyClass o = MyClass(); oos.writeObject(o); oos.close(); Serializable Interface # public interface Serializable {} Purpose: The Serializable interface is a marker interface (it has no methods) that indicates the implementor can be serialized by ObjectOutputStream. "},{"id":48,"href":"/docs/programming/backend/java/io/stream-reader-bridge-of-byte-and-char/","title":"Stream Reader: Bridge of Byte and Char","section":"Io","content":"There are two special readers in Java IO package, they are InputStreamReader and OutputStreamWriter which serve as bridge between byte data and character data\nInputStreamReader # Important constructor\npublic class InputStreamReader extends Reader { public InputStreamReader(InputStream in, Charset cs); } Key feature (Bridge from byte to char)\nCharacter Encoding: Data stored in files or transmitted over networks is often in the form of bytes. When such data represents text, it needs to be decoded using a specific character encoding (like UTF-8, ISO-8859-1, etc.) to be converted into characters that can be processed by the program. InputStreamReader facilitates this by decoding the byte stream into characters according to the specified or default charset.\nExample # // A socket connection to a server that sends text data Socket socket = new Socket(\u0026#34;example.com\u0026#34;, 80); InputStream input = socket.getInputStream(); // Use InputStreamReader to decode the byte stream from the socket // Assume the server sends data encoded in UTF-8 InputStreamReader reader = new InputStreamReader(input, StandardCharsets.UTF_8); int data; while ((data = reader.read()) != -1) { char character = (char) data; // Process the character } reader.close(); OutputStreamWriter # Bridge from Character to Byte: It bridges Writer (character-oriented abstraction) with OutputStream (byte-oriented stream). Customizable Encoding: Allows specifying a charset for encoding characters, making the character writing process customizable and flexible. "},{"id":49,"href":"/docs/programming/backend/java/io/io-stream/","title":"Java IO Stream","section":"Io","content":" What is stream? # Java Input/OutputStream and Reader/Writer are essential components of the Java I/O (Input/Output) library, designed to facilitate reading and writing data in various forms from different sources within Java applications. These classes serve as a bridge between your application and external data sources, making it easier to perform I/O operations efficiently and consistently. In this post, we will explore what Input/OutputStream and Reader/Writer are, their relationships, and how they classify based on the type of data and data source.\nBelow image illustrates the relationship among stream in Java IO, application and data source: Classification # Stream in Java IO could be classified by data type and data flow\n| Stream flow | Byte Stream | Character Stream | |-----------------------------|------------------|------------------| | input: data source -\u0026gt; app | InputStream | Reader | | output: app -\u0026gt; data source | OutputStream | Writer | Data type\nByte data: InputStream/OutputStream Character data: Reader/Writer Data flow\nInput (data source -\u0026gt; app): InputStream/Reader Output (app -\u0026gt; data source): OutputStream/Writer Class Hierarchy # java.io │ ├── Byte Stream │ ├── InputStream (abstract) │ │ ├── ByteArrayInputStream │ │ ├── FileInputStream │ │ ├── FilterInputStream │ │ │ ├── BufferedInputStream │ │ │ ├── DataInputStream │ │ │ └── PushbackInputStream │ │ ├── ObjectInputStream │ │ └── PipedInputStream │ │ │ └── OutputStream (abstract) │ ├── ByteArrayOutputStream │ ├── FileOutputStream │ ├── FilterOutputStream │ │ ├── BufferedOutputStream │ │ ├── DataOutputStream │ │ └── PrintStream │ ├── ObjectOutputStream │ └── PipedOutputStream │ └── Character Stream ├── Reader (abstract) │ ├── BufferedReader │ ├── CharArrayReader │ ├── FileReader │ ├── FilterReader │ │ ├── BufferedReader │ │ └── PushbackReader │ ├── InputStreamReader │ │ └── FileReader │ ├── PipedReader │ ├── StringReader │ └── FilterReader │ └── Writer (abstract) ├── BufferedWriter ├── CharArrayWriter ├── FileWriter ├── FilterWriter ├── OutputStreamWriter │ └── FileWriter ├── PipedWriter ├── PrintWriter └── StringWriter There are more than 40 classes under Java IO, and with a lot of different types of Input/Output Stream and Writer/Reader. However, IO Stream has a well defined hierarchy, here are some of properties of the it:\nJava IO has four abstract stream classes, they are InputStream OutputStream Reader Writer Every abstract stream class has multiple subclasses, and every subclass uses the name of its superclass as its suffix, for example: FileInputStream is subclass of InputStream, its suffix is InputStream, which is superclass name All subclasses under one abstract stream superclass (e.g. InputStream) can be classifed as two types Node Streams: These are the fundamental streams that directly interact with the specific data sources or destinations. They are the building blocks for reading or writing data and connect directly to the source, such as a file, an array in memory, or a network socket. Example FileInputStream ByteArrayInputStream Processing Streams: Also known as wrapper streams or filter streams, these streams do not directly read or write data to the data source. Instead, they are used to wrap around other streams (node streams or other processing streams) to provide additional functionalities like buffering, data conversion, or performance enhancement. Example BufferedInputStream DataInputStream The concept of node stream and processing stream may be hard to understand now. We will dive deeper into them with code examples and introduce the decorator design pattern underlying processing stream.\nInputStream # FileStreamInput # FileStreamInput is a subclass of InputStream. It\u0026rsquo;s a node steam whose data source is files from disk\nExample\nFile file = new File(\u0026#34;example.txt\u0026#34;); InputStream is = new FileInputStream(file); int content; while ((content = is.read()) != -1) { // process the content } is.close(); Read binary files\nFileInputSteam can not only read text files. More importantly, cause it is a byte stream, it is normally used to read binary files, like images, videos and any non-text files. Actually, we should only use InputStream to read binary files. Here is the main problem with using Reader to reading binary files:\nReader classes are deisnged to read character data. When you use a Reader to read a binary file, it may attempt to interpret non-character bytes as character, leading to data curruption and loss of imformation\nBufferedInputStream # BufferedInputStream is a processing stream\n"},{"id":50,"href":"/docs/programming/backend/java/io/file/","title":"Java File","section":"Io","content":"The File class in Java, found in the java.io package, is not used for file content manipulation (reading/writing) but for file and directory pathnames operations. It\u0026rsquo;s used to obtain or manipulate the information associated with a file or directory, such as metadata, permissions, and path details.\nCommonly Used APIs in File Class for Files and Directories # File Handling # Create a New File\ncreateNewFile(): Creates a new file if it does not exist. File myFile = new File(\u0026#34;myfile.txt\u0026#34;); if (myFile.createNewFile()) { System.out.println(\u0026#34;File created.\u0026#34;); } else { System.out.println(\u0026#34;File already exists.\u0026#34;); } Delete a File\ndelete(): Deletes the file or directory. if (myFile.delete()) { System.out.println(\u0026#34;File deleted.\u0026#34;); } else { System.out.println(\u0026#34;Failed to delete the file.\u0026#34;); } Check if File Exists\nexists(): Checks if the file or directory exists. if (myFile.exists()) { System.out.println(\u0026#34;The file exists.\u0026#34;); } else { System.out.println(\u0026#34;The file does not exist.\u0026#34;); } Read File Attributes\ncanRead(), canWrite(), isHidden(): Check read/write permission or if the file is hidden. length(): Returns the file size in bytes. lastModified(): Returns the last modified timestamp. Directory Handling # Create a Directory\nmkdir(): Creates the directory named by this abstract pathname. File dir = new File(\u0026#34;mydir\u0026#34;); if (dir.mkdir()) { System.out.println(\u0026#34;Directory created.\u0026#34;); } else { System.out.println(\u0026#34;Failed to create directory.\u0026#34;); } List Files and Directories\nlist(): Returns an array of strings naming the files and directories in the directory. String[] files = dir.list(); for (String file : files) { System.out.println(file); } Check Directory Status\nisDirectory(): Checks if the File instance is a directory. isFile(): Checks if the File instance is a regular file. File Path Operations\ngetPath(), getAbsolutePath(), getCanonicalPath(): Various ways to get the file/directory path. "},{"id":51,"href":"/docs/programming/backend/java/reflection/","title":"Reflection","section":"Java","content":" Introduction to Java Reflection # Java Reflection is a powerful feature that allows runtime introspection of classes, objects, and their members. It enables Java programs to manipulate internal properties and methods of classes dynamically. Reflection is especially useful in scenarios where the program needs to interact with classes and objects whose properties are not known at compile time.\nClass Object # The heart of Java\u0026rsquo;s reflection mechanism. It\u0026rsquo;s an instance that represents classes and interfaces in a running Java application. Every class, including primitive types and arrays, has a corresponding Class object.\nAcquiring Class Objects # Class.forName(\u0026quot;ClassName\u0026quot;): Loads the class dynamically. ClassName.class: Directly accesses the class if it\u0026rsquo;s known at compile time. object.getClass(): Retrieves the runtime class of an object. ClassLoader: For more complex scenarios, especially in modular applications. For primitive types: int.class or Integer.TYPE. // Using Class.forName() Class cls1 = Class.forName(\u0026#34;com.example.model.Person\u0026#34;); // Using .class syntax Class cls2 = Person.class; // From an instance of the class Person person = new Person(); Class cls3 = person.getClass(); // Using ClassLoader ClassLoader classLoader = Person.class.getClassLoader(); Class cls4 = classLoader.loadClass(\u0026#34;com.example.model.Person\u0026#34;); // Primitive type class Class intCls = int.class; Uniqueness and Singleton Nature of Class Object in Java Reflection # Singleton Pattern in Class Objects # The singleton pattern ensures that a class has only one instance and provides a global point of access to that instance. In the context of Java\u0026rsquo;s Class objects, the JVM enforces this pattern.\nHow It Works # Unique Instance: When a class is loaded into the JVM, a single instance of Class is created to represent it. This instance contains all the metadata about the class, including its methods, fields, constructors, and superclass. Global Accessibility: This unique Class instance is globally accessible through different means like Class.forName(), ClassName.class, and object.getClass(). JVM Management: The JVM manages these Class objects, ensuring that for each class loaded, only one Class object exists. Benefits # Consistency: Since there\u0026rsquo;s only one Class object per class, it guarantees consistency across the application. All parts of the program refer to the same metadata of a class. Efficiency: Reduces memory overhead by avoiding multiple instances of metadata for the same class. Example # Class cls1 = String.class; Class cls2 = \u0026#34;Hello, World!\u0026#34;.getClass(); Class cls3 = Class.forName(\u0026#34;java.lang.String\u0026#34;); // All of these Class objects are the same boolean areSame = (cls1 == cls2) \u0026amp;\u0026amp; (cls2 == cls3); // true Loading of Variables in a Class # In Java, variables are loaded and initialized based on their type (static or instance) and the sequence of their declaration in the class.\nStatic Variables # Initialization: Static variables are initialized when the class is first loaded into the JVM. Order of Execution: Static blocks and static variables are executed in the order they appear in the class. public class MyClass { static String staticVar = \u0026#34;Static Variable\u0026#34;; static { System.out.println(staticVar); // Accessed in static block staticVar = \u0026#34;Modified in Static Block\u0026#34;; } } // When MyClass is loaded, the static block is executed after staticVar is initialized. Instance Variables # Initialization: Instance variables are initialized when an instance of the class is created. Constructor Execution: They are typically initialized before the constructor\u0026rsquo;s execution or within it. Field # Represents the fields of a class. You can use Field objects to get and set field values dynamically.\nField field = cls.getField(\u0026#34;fieldName\u0026#34;); field.setAccessible(true); // For private fields Object value = field.get(objectInstance); field.set(objectInstance, newValue); Method # Represents a method of a class. You can invoke methods dynamically using Method objects.\nMethod method = cls.getMethod(\u0026#34;methodName\u0026#34;, parameterTypes); method.setAccessible(true); // For private methods method.invoke(objectInstance, arguments); Constructor # Represents a constructor of a class. Constructors can be used to create new instances of a class dynamically.\nConstructor constructor = cls.getConstructor(parameterTypes); Object newInstance = constructor.newInstance(initargs); TODO # For static variable loading, show the example with static variables declared before and after static block Better example about Method, Field and Constructor, like method getDeclaredMethod or getDeclaredField, and how to get a specific constructor if the class has multiples ones Reference # Perfect Video in Chinese\n"},{"id":52,"href":"/docs/programming/os/linux/linux-manual/","title":"Linux Manual","section":"Linux","content":" Vim: Three Modes # Normal mode Insert mode Command mode Command Mode # Type : to enter.\nw (write): save. q: quit. x (==wq): save and quit. set nu: show line numbers. set nonu: do not show line numbers. Normal Mode # y (yank): copy. p: paste. 5yy: copy 4 lines. dd: delete. 4dd: delete 4 lines. /word_to_search + Enter: search a word. n: next. G: last line. gg (go to the top\u0026hellip;): first line. u: undo. Ctrl + r: redo. 20gg / 20G: go to line 20. User Login and Logout Commands # Command: su - \u0026lt;user\u0026gt;: switch user. logout: If you are the root user, you will be switched to the standard user. If you are not the root user, you will be logged out of the Linux system. adduser \u0026lt;user\u0026gt;: add a user under /home directory. -d \u0026lt;home_dir\u0026gt;: specify the home directory for the new user. passwd: set the password for the root user. \u0026lt;user\u0026gt;: set the password for the user. User # User home directory: /home/\u0026lt;user\u0026gt;. File and Directory Commands # ls: List files and directories in the current directory. Example: ls Common options: -h: make the file sizes in the output more human-readable cd: Change the current directory. Example: cd /path/to/directory pwd: Print the current working directory. Example: pwd mkdir: Create a new directory. Example: mkdir new_directory rmdir: Remove a directory (only if it\u0026rsquo;s empty). Example: rmdir empty_directory rm: Remove files or directories. Example: rm file.txt or rm -r directory cp: Copy files and directories. Example: cp file.txt /path/to/destination mv: Move or rename files and directories. Example: mv file.txt new_name.txt or mv file.txt /path/to/destination touch: Create an empty file. Example: touch new_file.txt cat: Display the content of a file. Options: -n: show line number Example: cat file.txt more and less: Display file content page by page. Example: more file.txt or less file.txt head and tail: Display the beginning or end of a file. Example: head file.txt or tail file.txt chmod: Change file permissions. Example: chmod 755 file.txt chown: Change file ownership. Example: chown user:group file.txt ln: Create symbolic links or hard links to files or directories. Example: ln -s target link_name (for symbolic links) or ln target link_name (for hard links) history: Display latest executed commands in current shell session Use case: history !100: re-execute executed command in line 100 \u0026gt; \u0026amp; \u0026gt;\u0026gt;: In Linux and Unix-like operating systems, \u0026gt; and \u0026raquo; are used as operators for redirecting output from commands. They are often used in the command line to control where the output of a command is sent Difference: \u0026gt; overwrite existed content, but \u0026gt;\u0026gt; append command output to to the output of a file Example: echo \u0026quot;Hello, World!\u0026quot; \u0026gt; output.txt echo \u0026quot;Appended text\u0026quot; \u0026gt;\u0026gt; output.txt echo file1.txt \u0026gt;\u0026gt; file2.txt ls -l \u0026gt;\u0026gt; output.txt Date Command # Date Search \u0026amp; Find Commands # find: Search for files and directories. Basic syntax: find [path] [options] [expression] Common options: -name \u0026lt;pattern\u0026gt;: Search for files and directories with a specific name or pattern. -type \u0026lt;type\u0026gt;: Specify the type of file (e.g., f for regular files, d for directories). -mtime \u0026lt;days\u0026gt;: Search for files modified within a certain number of days. -size \u0026lt;size\u0026gt;: Search for files of a specific size (e.g., +10M for files larger than 10 megabytes). -user \u0026lt;username\u0026gt;: Search for files owned by a specific user. -group \u0026lt;groupname\u0026gt;: Search for files in a specific group. -exec \u0026lt;command\u0026gt; {} \\;: Execute a command on each matching file or directory. -print: Display the path of each matching file or directory. Example: find . -name \u0026quot;example.txt\u0026quot; find ~ -type d -user john find /path/to/directory -type f -mtime +7 -exec rm {} ; find . -type f -size +100M find /path/to/directory -type f -name \u0026quot;*.log\u0026quot; locate: Quick find location of a file. It uses pre-built database (updatedb command to create index for dirs and files) to provide fast search results Usage: sudo yum install mlocate: install locate in CentOS updatedb: create a db to store index of all files in your linux system locate example.txt: find example.txt files Reminder Keep in mind that locate is a powerful tool for quickly finding files and directories on your system. However, it doesn\u0026rsquo;t search for files in real-time, so you need to ensure that the database is updated regularly using the updatedb command for accurate results. grep: filter and find "},{"id":53,"href":"/docs/programming/web/glossary/cors/","title":"CORS","section":"Glossary","content":" What\u0026rsquo;s CORS # CORS is a mechanism to stop you from accessing resource in one origin from another origin. For example, there is an image img.jpg from origin images.com, if you don\u0026rsquo;t have CORS set properly, you can\u0026rsquo;t access the img.jpg from other origins like yourOrigin.com.\nWhy needs CORS # CORS is mainly for security usage. Image that your browser stores credential cookies of domain bank.com which is the website of you bank account, and a hacking website hacking.com want to access your bank information and make a transaction from your bank. If without CORS, the javascript script from hacking.com is able to get the cookies under domain bank.com and make requests to bank.com. CORS can protect your website from malicious requests\nAccess-Control-Allow-Origin Header # Issue Demonstration # Start a local host\nconst express = require(\u0026#34;express\u0026#34;); const app = express(); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.json({ name: \u0026#34;Hello\u0026#34;, value: \u0026#34;World\u0026#34;}) }); app.listen(3000); In your browser, navigate to http://localhost:3000. Open console and make a GET request to http://localhost:3000\nfetch(\u0026#34;http://localhost:3000\u0026#34;).then((req) =\u0026gt; req.json()).then(console.log); You will receive a successful response\n{name: \u0026#39;Hello\u0026#39;, value: \u0026#39;World\u0026#39;} You didn\u0026rsquo;t see any CORS issues, cause you were making request from http://localhost:3000 to same origin.\nOpen a new tab in your browser, navigate to www.google.com, and make a same request. You will see error like:\nAccess to fetch at \u0026#39;http://localhost:3000/\u0026#39; from origin \u0026#39;https://www.google.com\u0026#39; has been blocked by CORS policy: No \u0026#39;Access-Control-Allow-Origin\u0026#39; header is present on the requested resource. If an opaque response serves your needs, set the request\u0026#39;s mode to \u0026#39;no-cors\u0026#39; to fetch the resource with CORS disabled. Solution # Add www.google.com in allowed origins in your CORS config\nconst express = require(\u0026#34;express\u0026#34;); const app = express(); const cors = require(\u0026#34;cors\u0026#34;); app.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34; })); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.json({ name: \u0026#34;Hello\u0026#34;, value: \u0026#34;World\u0026#34;}) }); app.listen(3000); Make a same request again to local host, and the error is gone and you will see response printed in console:\n{name: \u0026#39;Hello\u0026#39;, value: \u0026#39;World\u0026#39;} Access-Control-Allow-Methods Header # Problem Demonstration # To demonstration the problem with Access-Control-Allow-Methods header\nI add GET and POST as allowed methods in CORS config, which limits that clients can only use GET or POST methods in their requests to the server Also, I changed the method provided by the server from GET to PUT app.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;], })); app.put(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.json({ name: \u0026#34;Hello\u0026#34;, value: \u0026#34;World\u0026#34;}) }); Then let\u0026rsquo;s make request in the console under www.google.com domain\nfetch(\u0026#34;http://localhost:3000\u0026#34;, {method: \u0026#34;PUT\u0026#34;}).then((req) =\u0026gt; req.json()).then(console.log); We will see error:\nwww.google.com/:1 Access to fetch at \u0026#39;http://localhost:3000/\u0026#39; from origin \u0026#39;https://www.google.com\u0026#39; has been blocked by CORS policy: Method PUT is not allowed by Access-Control-Allow-Methods in preflight response. Solution # Add PUT in allowed methods\napp.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;], })); Make the same request from www.google.com again, and this time the error was gone and we receive correct response.\n{name: \u0026#39;Hello\u0026#39;, value: \u0026#39;World\u0026#39;} Checking console, we can see two requests, the first request, which is called preflight request, and its response header is lik below:\nHTTP/1.1 204 No Content Access-Control-Allow-Origin: https://www.google.com Vary: Origin, Access-Control-Request-Headers Access-Control-Allow-Methods: GET,POST,PUT Content-Length: 0 Connection: keep-alive Keep-Alive: timeout=5 The headers tell the browser that cross-origin request to \u0026lsquo;http://localhost:3000/\u0026rsquo; is allowed for origin https://www.google.com, but only applies to three methods: GET, POST and PUT, which are same as what we defined in our CORS config.\nAnother request is the real cross-origin request, with PUT method, and server returns response with content.\nPreflight Request # We demonstrate Access-Control-Allow-Methods problem, but what is preflight response in above error\nPreflight request is an OPTIONS request automatically sent by browser before a cross origin request. Preflight request is used to determine if a cross origin request is safe. Preflight request including header Origin, Access-Control-Request-Method and Access-Control-Request-Headers (Optional) How Does Preflight Request Work # Browser sends an OPTIONS request to the server with headers mentioned above OPTIONS / HTTP/1.1 Access-Control-Request-Method: PUT Host: localhost:3000 Origin: https://www.google.com Sec-Fetch-Mode: cors Sec-Fetch-Site: cross-site Server receives the preflight request and check its CORS configuration. It determines if the real cross-origin request has proper permission by checking origin, method and headers in the preflight request\nIf the real request is allowed, server responds to the preflight request with appropriate CORS headers, including allowed origins, methods and headers.\nAfter receiving preflight response from the server, if the server allows the request, then browser will send the real cross-origin request to the server. If not, browser will block the request and prevent it from reaching the server\nIf we include headers in the request\nfetch(\u0026#34;http://localhost:3000\u0026#34;, { method: \u0026#34;PUT\u0026#34;, headers: { \u0026#34;Customer-Header\u0026#34;: \u0026#34;key=value\u0026#34;, \u0026#34;X-Customer-Header\u0026#34;: \u0026#34;key=value\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;value\u0026#34; } }).then((req) =\u0026gt; req.json()).then(console.log); Here we include two customer headers: Customer-Header and X-Customer-Header, and the preflight request and response are like below\nRequest OPTIONS / HTTP/1.1 Access-Control-Request-Headers: customer-header,x-customer-header Access-Control-Request-Method: PUT Host: localhost:3000 Origin: https://www.google.com Some headers are omitted, but you can see here preflight request tells server which headers are included in the cross-origin request.\nResponse HTTP/1.1 204 No Content Access-Control-Allow-Origin: https://www.google.com Access-Control-Allow-Methods: GET,POST,PUT Access-Control-Allow-Headers: customer-header,x-customer-header Access-Control-Allow-Credentials Header # The Access-Control-Allow-Credentials response header tells browsers whether to expose the response to the frontend JavaScript code when the request\u0026rsquo;s credentials mode (Request.credentials) is include.\nIssue Demonstration # Make below call in your browser console under Google domain\nfetch(\u0026#34;http://localhost:3000\u0026#34;, { method: \u0026#34;PUT\u0026#34;, credentials: \u0026#34;include\u0026#34; }).then((req) =\u0026gt; req.json()).then(console.log); You will see error message:\nAccess to fetch at \u0026#39;http://localhost:3000/\u0026#39; from origin \u0026#39;https://www.google.com\u0026#39; has been blocked by CORS policy: Response to preflight request doesn\u0026#39;t pass access control check: The value of the \u0026#39;Access-Control-Allow-Credentials\u0026#39; header in the response is \u0026#39;\u0026#39; which must be \u0026#39;true\u0026#39; when the request\u0026#39;s credentials mode is \u0026#39;include\u0026#39;. That is because you want to include credentials, including cookies, authorization headers, or TLS client certificates, in your request.\nRemember if you don\u0026rsquo;t explicitly specify credentials: \u0026quot;include\u0026quot; in your request, cookies will not be added in your request\u0026rsquo;s headers\nSolution # Set credentials to true in server\u0026rsquo;s CORS config\napp.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;], credentials: true })); Access-Control-Allow-Headers Header # Used in response to a preflight request to indicate which HTTP headers can be used when making the actual request.\nProblem Demonstration # Limit Content-Type as the only header which server allowed\napp.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;], allowedHeaders: [\u0026#39;Content-Type\u0026#39;] })); Then make a call from Google domain with two customer headers\nfetch(\u0026#34;http://localhost:3000\u0026#34;, { method: \u0026#34;PUT\u0026#34;, headers: { \u0026#34;Customer-Header\u0026#34;: \u0026#34;key=value\u0026#34;, \u0026#34;X-Customer-Header\u0026#34;: \u0026#34;key=value\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;value\u0026#34; } }).then((req) =\u0026gt; req.json()).then(console.log); The request was denied with below error message\nAccess to fetch at \u0026#39;http://localhost:3000/\u0026#39; from origin \u0026#39;https://www.google.com\u0026#39; has been blocked by CORS policy: Request header field customer-header is not allowed by Access-Control-Allow-Headers in preflight response. Solution # Add customers header as allowed headers\napp.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;], allowedHeaders: [\u0026#34;Content-Type\u0026#34;, \u0026#34;Customer-Header\u0026#34;, \u0026#34;X-Customer-Header\u0026#34;], })); Reference # YouTube video CORS Preflight Request Access-Control-Allow-Credentials "},{"id":54,"href":"/docs/programming/web/web-api/web-storage-api/","title":"Web Storage Api","section":"Web API","content":" Definition # The Web Storage API provides mechanisms by which browsers can store key/value pairs\nSource code # These mechanisms are available via the Window.sessionStorage and Window.localStorage properties (to be more precise, the Window object implements the WindowLocalStorage and WindowSessionStorage objects, which the localStorage and sessionStorage properties hang off) — invoking one of these will create an instance of the Storage object, through which data items can be set, retrieved and removed. A different Storage object is used for the sessionStorage and localStorage for each origin — they function and are controlled separately.\ninterface Window extends WindowLocalStorage, WindowSessionStorage { ... } interface WindowSessionStorage { readonly sessionStorage: Storage; } interface WindowLocalStorage { readonly localStorage: Storage; } interface Storage { readonly length: number; clear(): void; getItem(key: string): string | null; key(index: number): string | null; removeItem(key: string): void; setItem(key: string, value: string): void; [name: string]: any; } Property # The read-only sessionStorage property accesses a session Storage object for the current origin. sessionStorage is similar to localStorage; the difference is that while data in localStorage doesn\u0026rsquo;t expire, data in sessionStorage is cleared when the page session ends.\nData stored in sessionStorage and localStorage is specific to the protocol of the page. In particular, data stored by a script on a site accessed with HTTP (e.g., http://example.com) is put in a different sessionStorage object from the same site accessed with HTTPS (e.g., https://example.com).\nThe keys and the values are always in the UTF-16 string format, which uses two bytes per character. As with objects, integer keys are automatically converted to strings.\nReference # https://developer.mozilla.org/en-US/docs/Web/API/Web_Storage_API#web_storage_concepts_and_usage YouTube video with examples "},{"id":55,"href":"/docs/programming/web/http/cookie/","title":"Cookie","section":"HTTP","content":" What is cookie # An HTTP cookie (web cookie, browser cookie) is a small piece of data that a server sends to a user\u0026rsquo;s web browser. The browser may store the cookie and send it back to the same server with later requests. Typically, an HTTP cookie is used to tell if two requests come from the same browser—keeping a user logged in, for example. It remembers stateful information for the stateless HTTP protocol.\nCookies are created by server Cookies are stored in client\u0026rsquo;s browser Cookies are sent with every request to the same domain (Explain more below) Deprecation of cookie # Cookies were once used for general client-side storage. While this made sense when they were the only way to store data on the client, modern storage APIs are now recommended. Cookies are sent with every request, so they can worsen performance (especially for mobile data connections). Modern APIs for client storage are the Web Storage API (localStorage and sessionStorage) and IndexedDB.\nUse case of cookie # Session management Logins, shopping carts, game scores, or anything else the server should remember\nPersonalization User preferences, themes, and other settings\nYou visited a weather website and entered your address to get weather information. The server set an address cookie under the domain of the weather website. Next time when you revisit the website, the address cookie will be sent in the request\u0026rsquo;s header, and the website will show weather information of same address. Tracking Recording and analyzing user behavior (ADs)\nHow to set cookies # Cookies can be set and modified at the server level using\nthe Set-Cookie HTTP header, or with JavaScript using document.cookie. After receiving an HTTP request, a server can send one or more Set-Cookie headers with the response. The browser usually stores the cookie and sends it with requests made to the same server inside a Cookie HTTP header.\nSet-Cookie and Cookie headers # The Set-Cookie HTTP response header sends cookies from the server to the user agent. A simple cookie is set like this:\nSet-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt; Example:\nHTTP/2.0 200 OK Content-Type: text/html Set-Cookie: yummy_cookie=choco Set-Cookie: tasty_cookie=strawberry [page content] Define the lifecycle of a cookie # The lifetime of a cookie can be defined in two ways:\nSession cookies are deleted when the current session ends. The browser defines when the \u0026ldquo;current session\u0026rdquo; ends, and some browsers use session restoring when restarting. This can cause session cookies to last indefinitely. Permanent cookies are deleted at a date specified by the Expires attribute, or after a period of time specified by the Max-Age attribute. Example: Set-Cookie: id=a3fWa; Expires=Thu, 31 Oct 2021 07:28:00 GMT; Restrict access to cookies # Secure attribute A cookie with the Secure attribute is only sent to the server with an encrypted request over the HTTPS protocol. HttpOnly attribute A cookie with the HttpOnly attribute is inaccessible to the JavaScript Document.cookie API; it\u0026rsquo;s only sent to the server. For example, cookies that persist in server-side sessions don\u0026rsquo;t need to be available to JavaScript and should have the HttpOnly attribute. This precaution helps mitigate cross-site scripting (XSS) attacks. Example: Set-Cookie: id=a3fWa; Expires=Thu, 21 Oct 2021 07:28:00 GMT; Secure; HttpOnly Define where cookies are sent # The Domain and Path attributes define the scope of a cookie: what URLs the cookies should be sent to.\nDomain attribute # The Domain attribute specifies which hosts can receive a cookie. If the server does not specify a Domain, the browser defaults the domain to the same host that set the cookie, excluding subdomains. If Domain is specified, then subdomains are always included. Therefore, specifying Domain is less restrictive than omitting it. However, it can be helpful when subdomains need to share information about a user.\nFor example, if you set Domain=mozilla.org, cookies are available on domains like\nmozilla.orgdeveloper.mozilla.org If the the cookie was set by server with domain example.com, and no domain attribute was specified. Then the cookie will only be send to domain example.com.\nPath attribute # The Path attribute indicates a URL path that must exist in the requested URL in order to send the Cookie header. The (\u0026quot;/\u0026quot;) character is considered a directory separator, and subdirectories match as well. For example, if you set Path=/docs, these request paths match:\n/docs/docs//docs/Web//docs/Web/HTTP But these request paths don\u0026rsquo;t:\n//docsets/fr/docs TODO # There are still some other attributes and concepts regarding cookie, will dive deeper later is necessary Attach some picture about Set-Cookie and Cookie headers, and attributes of a cookie Dive deeper for HttpOnly attribute Reference # Cookie Definition Nice YouTube video "},{"id":56,"href":"/docs/programming/tools/hugo/set-up-hugo-in-git-hub-pages/","title":"Set Up Hugo in Git Hub Pages","section":"Hugo","content":" Create a repository to hold the source code of your blogs # Create a repository of Git Hub Pages # Create a new Hugo project in your local machine # cd ~/Projects hugo new site \u0026lt;site name\u0026gt; cd \u0026lt;site name\u0026gt; git init git remote add origin \u0026lt;repository URL of your blogs\u0026gt; git add . git commit -m \u0026#34;Initiate a new hugo project\u0026#34; git push origin main Choose theme for your blog site # Navigate to Hugo theme website Down your favorite theme to directory ~/Projects/\u0026lt;site name\u0026gt;/themes Modify your config file baseURL = \u0026#34;https://\u0026lt;URL of your Git Hub Pages\u0026gt;/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;\u0026lt;website name\u0026gt;\u0026#34; theme = \u0026#34;\u0026lt;theme name\u0026gt;\u0026#34; Run hugo server to check if everything in your local host is expected Add Git Hub Pages repository as submodule of your source repository # cd ~/Projects/\u0026lt;site name\u0026gt; git submodule add -b main \u0026lt;URL of your Git Hub Pages\u0026gt; public # Generate static resouce in public directory hugo cd public git add . git commit -m \u0026#34;\u0026#34;Initiate a new hugo project\u0026#34; git push origin main "}]