[{"id":0,"href":"/docs/project/online-chat/","title":"Online Chat","section":"Project","content":" Demo # GitHub\nHigh Level Architecture # This is an online chat system built with BIO (Blocking IO) Using Java. Each client has two socket connections with the server, one connection supports message push mode and another one supports message pull mode.\nThe system now supports below features:\nLogin Pull online users Online chat Logoff Because of supports for push and pull modes, features like\nUpload/Download files Group chat can be easily added in the system. However, this project is just for learning purpose, so I didn\u0026rsquo;t spend too much time on it. Key Components # Client # Server Listener It\u0026rsquo;s basically a class extending Thread containing a socket connected with the server. In its run() method, it listens to InputStream of the socket which takes messages from the server. The socket in this component connects to Publisher from the server to support message push mode\npublic class ServerListener extends Thread { private Socket socket; private boolean listening = true; public ServerListener(Socket socket, User user) { this.socket = socket; this.user = user; } public void run() { try { while(listening) { ObjectInputStream ois = new ObjectInputStream(socket.getInputStream()); Message response = (Message) ois.readObject(); MessageType type = response.getType(); switch(type) { ... // Operations } } } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } } Online Chat Client It\u0026rsquo;s a client class containing another socket connecting with the server. It supports message pull mode, which is that client sends a request to server and receives a response, and it\u0026rsquo;s a synchronous request. It has two private methods sendRequest and receiveResponse which are used to support communication with the server.\npublic class OnlineChatClient { private Socket client; private ObjectOutputStream oos; private ObjectInputStream ois; private void sendRequest(Message request) throws IOException { oos = new ObjectOutputStream(client.getOutputStream()); oos.writeObject(request); } private Message receiveResponse() throws IOException, ClassNotFoundException { ois = new ObjectInputStream(client.getInputStream()); return (Message) ois.readObject(); } } Server # Message Publisher The name of this component is descriptive. Its duty is to publish a message to a corresponding client and receive the response from the server. It works with Server Listener in clients to support message push mode. Each message publisher has a 1 to 1 relationship with the client. All message publishers are managed by Message Publisher Manager\npublic class MessagePublisher { private Socket socket; private ObjectOutputStream oos; public void publish(Object o) throws IOException { oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(o); } } Message Publisher Manager It maintains a Map to store all message publishers. The map\u0026rsquo;s key is user id and the value is the message publisher. It provides methods like add() get(String userId) and delete(String userId) to manage all message publishers.\npublic class MessagePublisherManager { private final static Map\u0026lt;String, MessagePublisher\u0026gt; publishers = new ConcurrentHashMap\u0026lt;\u0026gt;(); public static boolean contains(User user) { final String id = user.getId(); return publishers.containsKey(id); } public static MessagePublisher add(User user, Socket socket) { if (contains(user)) { return publishers.get(user); } final String id = user.getId(); MessagePublisher publisher = new MessagePublisher(user, socket); return MessagePublisherManager.publishers.put(id, publisher); } public static MessagePublisher get(String userId) { return publishers.get(userId); } public static MessagePublisher get(User user) { return get(user.getId()); } public static void remove(User user) { publishers.remove(user.getId()); } } Client Listener It is similar to Server Listener in clients. It\u0026rsquo;s a Thread subclass waiting for message from the server. The thread in the most time is blocked at line\nMessage response = (Message) ois.readObject(); Once messages sent to the socket\u0026rsquo;s receive queue (InputStream), it reads the message and performs corresponding operation and returns a response to the client.\npublic class ClientListener extends Thread { private Socket socket; private ObjectOutputStream oos; private boolean listening = true; public void run() { try { while(listening) { ObjectInputStream ois = new ObjectInputStream(socket.getInputStream()); Message response = (Message) ois.readObject(); MessageType type = response.getType(); switch(type) { ... // Operations } } } catch (IOException | ClassNotFoundException e) { e.printStackTrace(); } } private void sendResponse(Message message) throws IOException { oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(message); } } Client Listener Manager It manages all client listeners in a Map, supporting methods like add, get and remove.\npublic class ClientListenerManager { private final static Map\u0026lt;String, ClientListener\u0026gt; clientListeners = new ConcurrentHashMap\u0026lt;\u0026gt;(); private static ObjectOutputStream oos; public static void add(Socket socket, User user) throws IOException { Message response = Message.builder() .type(MessageType.CONNECT) .content(gson.toJson(BaseResponse.builder().successful(true).build())) .timeStamp(new Date().toString()).build(); oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(response); ClientListener listener = new ClientListener(socket, user); clientListeners.put(user.getId(), listener); listener.start(); } public static ClientListener get(User user) { final String id = user.getId(); return clientListeners.get(id); } public static ClientListener get(String id) { return clientListeners.get(id); } public static void remove(User user) { final String id = user.getId(); clientListeners.remove(id); } } Supported Features # Login # Client creates a new Socket connecting to port 9999 in local host (Server and clients are in local host) by sending below message. Message type is USER_LOGIN Before sending the message, user needs to enter user id and password Message request = Message.builder() .sender(user.getId()) .timeStamp(new Date().toString()) .type(MessageType.USER_LOGIN) .content(gson.toJson(user)) .build(); Server receives the login message from the client First server checks database if the user enters valid user id and password (Not implemented) After id and password validations, server creates a new MessagePublisher and add it into MessagePublisherManager In the end, server sends back a response to notify the client if login succeeds Client receives response from server. If login succeeded, client start a new ServerListener which is a subclass of Thread to listen to message from server. The thread is blocked when there is no messages from server. Connect: # This is not an API, but a process implicitly done in advance for features like GetOnlineUsers which utilizes OnlineChatClient\nClient creates a new Socket connecting to port 9999 in local host After server receives the request, it starts a new ClientListener, the subclass of Thread, which is listening message from the client. Then server add the ClientListener to ClientListenerManager Server send back response to the client Client receives response from server After above steps, a new socket connection between server and client is built, which supports message pull mode.\nGetOnlineUsers # Client sends a request to server Server checks MessagePublisherManager, get all online users\u0026rsquo; id Server sends a response back to client Client renders online users\u0026rsquo; id in terminal Chat # ClientA sends a request to server containing receiver\u0026rsquo;s id (ClientB) and the message Client listenerA in the server receives the request, it passes the message to message publisherB Message publisherB sends a request containing the message to clientB ClientB responds after receiving the message After message was successfully sent to clientB, client listenerA sends a response to clientA ClientA receives the response Logoff # Client sends a request to the server to logoff Server receives the request, responds to the client. Server close the socket in the client listener, and remove the listener from listener manager Server sends a request to client through message publisher Client receives the request, close socket in the server listen Client responds. Server receives the response from client. Then server closes the socket in message publisher and remove the message listener from listener manager After all aboves steps, all sockets are closed and each listener thread (ServerListener in the client and ClientListener in the server) is terminated.\nTo Be Improved # This system is built on BIO. In server and client sides, there is one thread blocked to listen to messages sent to the input stream of the socket. Thread is expensive because of the memory it occupies and performance influence when CPU switching among different threads Listener is a resource shared by two threads, main thread and the listener thread. Current code doesn\u0026rsquo;t ensure thread safety. The quick fix is to expose InputStream and OutputStream of the socket from the listener, and main thread can only access the two streams by calling the exposed methods. Also, the two methods should be synchronized. "},{"id":1,"href":"/docs/programming-core/java/thread/start-thread/","title":"Start Thread","section":"Thread","content":" Main Lesson # Java threads are crucial for executing multiple tasks concurrently in a program. Let\u0026rsquo;s dive into this topic:\nWhat is a Thread in Java? 🧵\nIn Java, a thread is the smallest unit of execution within a process. Think of it like a worker who performs a part of a larger task. Creating a Thread 💻\nThere are two ways to create a thread: By extending the Thread class. By implementing the Runnable interface. Example: Let\u0026rsquo;s create a simple thread that prints \u0026ldquo;Hello, Java Threads!\u0026rdquo;. Starting a Thread ✨\nUse the .start() method to begin the execution of a thread. It\u0026rsquo;s like telling our worker, \u0026ldquo;Go ahead and start your task!\u0026rdquo; Thread Lifecycle 📚\nUnderstand the states: New, Runnable, Running, Waiting/Blocked, and Terminated. It\u0026rsquo;s like a day in the life of our worker, from arrival to completion of the task. Synchronization 🤝\nWhen threads share resources, we need to ensure they don\u0026rsquo;t interfere with each other. It\u0026rsquo;s like coordinating workers to avoid conflicts and ensure smooth operation. Thread Safety 🔒\nEnsuring that our threads don\u0026rsquo;t cause data corruption or inconsistent results. Think of it as safety measures in a workplace. Several Ways to Start A Thread # Extends Thread class\npublic class ExtendsThread { public static void main(String[] args) { Thread_1 thread_ = new Thread_1(); thread_.start(); } } class Thread_1 extends Thread { public void run() { System.out.println(\u0026#34;Hello World!\u0026#34;); } } Thread_1 extends Thread class and overrides method run(). We create a Thread_1 object and invoked start() method inherited from parent class Thread to start a new thread. Besides extending Thread class, we can also create a thread by implementing Runnable interface\nImplements Runnable interface\npublic class ImplementsRunnable { public static void main(String[] args) { Thread thread = new Thread(new Thread_2()); thread.start(); } } class Thread_2 implements Runnable { public void run() { System.out.println(\u0026#34;Hello World!\u0026#34;); } } Create FutureTask object\nFutureTask has a Callable field. Callable and Runnable both stand for a task, the difference is that Callable has return value, but Runnable doesn\u0026rsquo;t In below code snippet, we pass the FutureTask to a thread, and start the thread task.get() method is a blocking thread. It stop current thread, which is main here until the task finishes (run() finish in FutureTask) // Create a new future task FutureTask\u0026lt;Integer\u0026gt; task = new FutureTask\u0026lt;\u0026gt;(() -\u0026gt; { log.debug(\u0026#34;running\u0026#34;); Thread.sleep(1000); return 100; }); // Create and start a new thread Thread t = new Thread(task, \u0026#34;Thread_Get_Int\u0026#34;); t.start(); log.debug(\u0026#34;In main thread\u0026#34;); // Blocking method Integer result = task.get(); log.debug(\u0026#34;{}\u0026#34;, result); Open Question: Why do we invoke start() to start a thread, what if I invoke run() directly? # That\u0026rsquo;s a great question about thread management in Java! Let\u0026rsquo;s delve into the difference between invoking start() and run() when dealing with threads.\nUsing start() Method:\nThe start() method is used to begin the execution of a new thread in the Java runtime environment. When start() is called, it performs necessary preparations and creates a new thread in which the run() method will execute. This means the run() method executes in a separate call stack. Key Point: start() enables concurrent execution, aligning with the whole purpose of using threads. Directly Calling run() Method:\nIf you directly call the run() method, it does not start a new thread. Instead, the run() method executes in the current thread\u0026rsquo;s call stack, just like a normal method call. This means no concurrent execution happens; it\u0026rsquo;s just a regular method call. Key Point: Calling run() directly defeats the purpose of threading, as it doesn\u0026rsquo;t utilize the multithreading capabilities of Java. Consequences of Invoking run() Instead of start():\nWhen you call run() directly, you\u0026rsquo;re not utilizing the threading capabilities of Java; your code is executed on the same thread that called run(). This can lead to performance issues if the purpose is to execute tasks concurrently. It essentially turns your thread into a regular object, not a thread of execution. Summary:\nUse start() to actually start a new thread, leading to concurrent execution. Directly calling run() is like any other method call and does not start a new thread. This distinction is crucial for effective multithreading in Java. It\u0026rsquo;s part of ensuring that your program benefits from concurrency and parallelism. 🦌💻🧵 public class InvokesRun { public static void main(String[] args) { System.out.println(\u0026#34;Thread for main method \u0026#34; + Thread.currentThread().getName()); RunThread runThread = new RunThread(); StartThread startThread = new StartThread(); runThread.run(); startThread.start(); } } class RunThread extends Thread { public void run() { System.out.println(\u0026#34;Thread for RunThread: \u0026#34; + Thread.currentThread().getName()); } } class StartThread extends Thread { public void run() { System.out.println(\u0026#34;Thread for StartThread: \u0026#34; + Thread.currentThread().getName()); } } "},{"id":2,"href":"/docs/web/web-securities/asymmetric-encryption/","title":"Asymmetric Encryption","section":"Web Securities","content":" What Is Asymmetric Encryption? # Asymmetric encryption, also known as public-key encryption, is a method of encrypting data that involves two separate keys: a public key and a private key. These keys are mathematically linked but not identical, hence the term \u0026ldquo;asymmetric.\u0026rdquo; This method provides a secure way of encrypting and decrypting information, and it\u0026rsquo;s widely used in various forms of digital communication and security protocols\nHow Does It Work? # Key Generation: # A pair of cryptographic keys is generated. The process involves complex algorithms like RSA or ECC, ensuring that these keys are mathematically linked. The public key is designed to be shared, while the private key is kept confidential by the owner.\nThe message encrypted with the public key can only be decrypted by the linked private key and vise verse.\nPublic Key Sharing: # The public key is distributed to anyone who might need to encrypt data intended for the owner. This can be done through public directories, digital certificates, or direct sharing.\nAfter Key Generation and Public Key Sharing,\nA has public key of B, and it own private key (A) B has public key of A, and it own private key (B) Encryption Process: # To send an encrypted message, the sender uses the recipient\u0026rsquo;s public key. This key encrypts the plaintext (original message), transforming it into ciphertext (encrypted message). The encryption is such that only the corresponding private key can efficiently decrypt the ciphertext.\nTransmission: # The ciphertext, now encrypted and secure, is transmitted over the network. Even if intercepted, the message remains secure because it can only be decrypted by the private key holder.\nDecryption: # The recipient uses their private key to decrypt the message. The private key reverses the encryption process, converting the ciphertext back into readable plaintext. Since only the intended recipient possesses the private key, the confidentiality and integrity of the message are maintained.\nReference # YouTube Video\n"},{"id":3,"href":"/docs/programming-core/java/nio/byte-buffer/","title":"Byte Buffer","section":"Nio","content":" Introduction # Java ByteBuffer is a class in Java\u0026rsquo;s java.nio package. It\u0026rsquo;s used for reading and writing data to and from buffers efficiently. Buffers are blocks of memory that can store data temporarily. ByteBuffer is particularly useful when dealing with I/O operations and for high-performance applications. 📘\nByteBuffer can be used in two modes:\nRead Mode: You can read data from the buffer. 💡 Write Mode: You can write data to the buffer. 💡 Practical Example # // Create a new ByteBuffer ByteBuffer buffer = ByteBuffer.allocate(10); // Write data into the buffer buffer.put((byte) 10); buffer.put((byte) 20); // Flip the buffer to read mode buffer.flip(); // Read data from the buffer byte first = buffer.get(); byte second = buffer.get(); In this example, we first write two bytes into the buffer and then read them back.\nImportant Concepts and Operations # Capacity, Limit, and Position: ByteBuffer has three important properties:\nCapacity: The maximum number of bytes it can hold. It\u0026rsquo;s set when the buffer is created and cannot be changed. Limit: The limit is the index of the first element that should not be read or written. It can change as you read/write data. Position: The next element to be read or written. Position will increase as you read or write data. Operations # Create a new ByteBuffer\nByteBuffer buffer = ByteBuffer.allocate(10); Write ByteBuffer\nPut single byte ByteBuffer buffer = ByteBuffer.allocate(10); buffer.put((byte) \u0026#39;a\u0026#39;); Put byte array ByteBuffer buffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;}); Read ByteBuffer\nflip(): switch to read mode\nbuffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); Here we create a new ByteBuffer, and write four bytes to the ByteBuffer. The ByteBuffer is still in write mode, and to switch to read mode, we can invoke flip() method.\nbuffer.flip(); flip() updates values of limit and position\nthis.limit = position; this.position = 0; get(): read one byte and increment position\nbuffer.flip(); char data = (char) buffer.get(); get(int i): read byte of index w/o position change\nbuffer.flip(); char data = (char) buffer.get(0); // \u0026#39;a\u0026#39; pointer position doesn\u0026rsquo;t increment clear() \u0026amp; compact(): switch from read mode to write mode\nclear(): clear() resets position to 0, and limit to the capacity of the ByteBuffer. It doesn\u0026rsquo;t erase the data, but prepares a new write operation for the buffer.\nbuffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); buffer.flip(); buffer.get(); buffer.get(); buffer.clear(); compact(): compact() moves unread data to the beginning of the buffer, and sets position after the last unread byte and limit to the capacity of the buffer.\nUse case: When you have partially processed the data in the buffer and want to keep remaining unprocessed data while sill making room for the new data to be added buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); buffer.flip(); buffer.get(); buffer.get(); buffer.compact(); Manipulate position pointer\nrewind(): reset position back to 0\nByteBuffer buffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); buffer.rewind(); rewind() only does one thing that is to reset position to 0. After rewind, you can write/read from the beginning of the ByteBuffer\nmark() and reset(): bookmark position and re-visit\nByteBuffer buffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); buffer.flip(); buffer.get(); buffer.get(); buffer.mark(); buffer.get(); buffer.get(); buffer.reset(); mark() and reset() introduce a new pointer mark in ByteBuffer, and the two methods are normally used together. Value of mark in most case is -1. When mark() is invoked, current position will be remembered by assigning position to mark\nIn above code snippet, after remembering position (mark()), we continue read two more bytes from the ByteBuffer where position is 4. reset() resets position to previous mark which is 2.\nComparing to rewind() which can only reset position to 0, mark() and reset() have enhanced feature which bookmarks position and resets position to previous mark\nWork with Channel\nChannel\nChannel is an important component in NIO, and it is like Stream in BIO. Different from Stream, Channel can be used bi-directional and can be used in conjunction with Buffer\nWhen Using Channel and Buffer, data can be read from a Channel into a Buffer or written from a Buffer into a Channel Read Channel\ntry (FileChannel channel = new FileInputStream(\u0026#34;~/example.txt\u0026#34;).getChannel()) { channel.read(buffer); } Here we get the Channel from FileInputStream, and read data from the channel into a ByteBuffer\nWrite Channel\ntry (FileChannel channel = new FileOutputStream(\u0026#34;~/example.txt\u0026#34;).getChannel()) { ByteBuffer buffer = ByteBuffer.allocate(10); buffer.put(new byte[]{\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;}); // Need to switch byteBuffer to read mode before writing to channel buffer.flip(); int len = channel.write(buffer); log.debug(String.format(\u0026#34;Write %s bytes to file\u0026#34;, len)); } Work with String\nfrom string to buffer String.getBytes():\nbuffer = ByteBuffer.allocate(16); buffer.put(\u0026#34;Hello World\u0026#34;.getBytes()); charset:\nByteBuffer buffer = StandardCharsets.UTF_8.encode(\u0026#34;Hello World\u0026#34;); ByteBuffer.wrap():\nByteBuffer buffer = ByteBuffer.wrap(\u0026#34;Hello World\u0026#34;.getBytes()); from buffer to string charset: ByteBuffer buffer = StandardCharsets.UTF_8.encode(\u0026#34;Hello World\u0026#34;); String str = StandardCharsets.UTF_8.decode(buffer).toString(); "},{"id":4,"href":"/docs/cloud/aws/cloudformation/","title":"CloudFormation","section":"Aws","content":" CloudFormation # Resources # Resource Type Reference\nResources Introduction\nSyntax # Resources: Logical ID: Type: Resource type Properties: Set of properties Logical ID: A unique logical ID for that resource, which can be referenced by other parts in the template\nResource Type: An identifier of the resource that you are declaring\nResource Type Reference Resource Properties: Additional options that you can specify for a resource\nParameters (Optional) # Guide Parameters allow you to input custom values for your template. You can think the template as a module or a function with arguments. Each time creating or updating a stack using the template is like invoking the function with parameters you defined in the template.\nSyntax # Parameters: ParameterLogicalID: Type: DataType ParameterProperty: value Example Parameters: InstanceTypeParameter: Type: String Default: t2.micro AllowedValues: - t2.micro - m1.small - m1.large Referencing a Parameter # User Ref intrinsic function to reference a parameter\nExample Ec2Instance: Type: AWS::EC2::Instance Properties: InstanceType: Ref: InstanceTypeParameter ImageId: ami-0ff8a91507f77f867 Pseudo Parameter # Guide Parameters are predefined by AWS CloudFormation. You can think them as environment variables\nMappings (Optional) # Guide Mappings are fixed values in your CloudFormation template. Different from Parameters which are unknown before creating the stack, Mappings are hard coded in the template and are known in advance\nYou can think Mappings as static configurations in your project\nSyntax # YAML # Mappings: MappingName: PrimaryKey01: SecondaryKey01: Value01 PrimaryKey02: SecondaryKey02: Value02 PrimaryKey03: SecondaryKey03: Value03 Example # Mappings: DependencyAwsAccountMapping: us-ease-1: dependency01: 123456789012 dependency02: 123456789013 us-west-2: dependency01: 987654321098 dependency02: 987654321987 Find a Value in a Mapping # You can use intrinsic function !FindInMap\nSyntax !FindInMap [MappingName, PrimaryKey, Secondary] Above function returns value01\nExample AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Mappings: RegionMap: us-east-1: HVM64: ami-0ff8a91507f77f867 HVMG2: ami-0a584ac55a7631c0c us-west-2: HVM64: ami-0bdb828fd58c52235 HVMG2: ami-066ee5fd4a9ef77f1 Resources: myEC2Instance: Type: \u0026#34;AWS::EC2::Instance\u0026#34; Properties: ImageId: !FindInMap [RegionMap, !Ref \u0026#34;AWS::Region\u0026#34;, HVM64] InstanceType: m1.small Outputs (Optional) # Guide\nCross-stack Reference: Outputs section declares output values that can be imported into other templates\nYou can\u0026rsquo;t delete the stack if its outputs are referenced by other stacks\nSyntax # Outputs: LogicalId: Description: Information about the value Value: Value to return Export: Name: Name of resource to export You can think above export statements as below to better understand\nexport LogicalId as Name; Import Outputs into Other Stacks # Output from StackA\nOutputs: WebServerSecurityGroup: Description: The security group ID to use for public web servers Value: \u0026#39;Fn::GetAtt\u0026#39;: - WebServerSecurityGroup - GroupId Export: Name: \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${AWS::StackName}-SecurityGroupID\u0026#39; Import into StackB\nResources: WebServerInstance: Type: \u0026#39;AWS::EC2::Instance\u0026#39; Properties: InstanceType: t2.micro ImageId: ami-a1b23456 NetworkInterfaces: - GroupSet: - Fn::ImportValue: \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${NetworkStackNameParameter}-SecurityGroupID\u0026#39; Conditions (Optional) # Guide The Conditions section contains statements that define the circumstances under which entities are created or configured\nDefine a Condition # AWSTemplateFormatVersion: 2010-09-09 Parameters: EnvType: Description: Environment type. Default: test Type: String AllowedValues: - prod - test ConstraintDescription: must specify prod or test. Conditions: IsProd: !Equals - !Ref EnvType - prod Define a Nested Condition # Conditions: IsProd: !Equals - !Ref EnvType - prod CreateBucket: !Not - !Equals - !Ref BucketName - \u0026#39;\u0026#39; CreateBucketPolicy: !And - !Condition IsProd - !Condition CreateBucket Use Conditions # Resources: EC2Instance: Type: \u0026#39;AWS::EC2::Instance\u0026#39; Condition: IsProd Rules (Optional) # Guide Rules validates parameters passed to a template during creation or updates of a stack\nWorking with Rules # Each template rule contains two properties:\nRule Condition (Optional) - determine when a rule takes effect Each rule can only have one condition If the condition is not defined, then the rule\u0026rsquo;s assertions always take effect Rule Assertion (Required) - describe what values users can use for a specific parameter One rule can have multiple assertions Example # Rules: testInstanceType: RuleCondition: !Equals - !Ref Environment - test Assertions: - Assert: \u0026#39;Fn::Contains\u0026#39;: - - a1.medium - !Ref InstanceType AssertDescription: \u0026#39;For a test environment, the instance type must be a1.medium\u0026#39; prodInstanceType: RuleCondition: !Equals - !Ref Environment - prod Assertions: - Assert: \u0026#39;Fn::Contains\u0026#39;: - - a1.large - !Ref InstanceType AssertDescription: \u0026#39;For a production environment, the instance type must be a1.large\u0026#39; Change Set # Guide Change Set allows you to preview how existing resources could be affected, like adding or deleting a resource, once you update the template or parameters for a stack\nNested Stack # Guide Nested stacks are stacks created as part of other stacks. You create a nested stack within another stack by using the AWS::CloudFormation::Stack resource.\nYou can think nested stack as a function invoked by other modules (stacks). Similar with functions which have arguments and return values, nested stacks have parameters and outputs\nExample # Nested Stack Template\nAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Parameters: BucketName: Type: String Description: The name of the S3 bucket Resources: MySampleBucket: Type: AWS::S3::Bucket Properties: BucketName: !Ref BucketName Outputs: CreatedBucketName: Description: Name of the created S3 bucket Value: !Ref MySampleBucket Export: Name: !Sub \u0026#34;${AWS::StackName}-BucketName\u0026#34; Parent Stack Template\nAWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Resources: MyNestedStack: Type: AWS::CloudFormation::Stack Properties: TemplateURL: \u0026#39;https://s3.amazonaws.com/mybucket/NestedStack.yaml\u0026#39; Parameters: BucketName: MyCustomBucketName Outputs: NestedStackBucketName: Description: The name of the bucket created in the nested stack Value: !GetAtt MyNestedStack.Outputs.CreatedBucketName Function Analogy # As I mentioned above, we can think nested stacks as functions invocations. We can transform above examples to a pseudo-code to illustrate the idea\nNested Function\ndef nested_function(bucket_name): created_bucket_name = create_bucket(bucket_name) return created_bucket_name Parent Function\ndef parent_function(): # Specify the bucket name to pass to the nested function (like Parameters in CloudFormation). bucket_name = \u0026#34;MyCustomBucketName\u0026#34; # Call the nested function, mimicking the creation of the nested stack. created_bucket_name = nested_function(bucket_name) # Output the result (mimicking the Outputs in CloudFormation). return created_bucket_name # Run the parent function to simulate the stack deployment. parent_function() CloudFormation Drift # A CloudFormation Drift is that the actual configuration of a stack is drifted or different from the template of the stack, in other words, the stack was modified manually\nStack Policy # A Stack Policy is a JSON file that defines the update actions that are allowed on specific resources during stack updates\n"},{"id":5,"href":"/docs/networking/dns/","title":"DNS","section":"Networking","content":" What Is DNS? # DNS, which stands for Domain Name System, is a fundamental component of the internet\u0026rsquo;s infrastructure. It functions like a phone book for the internet by translating human-friendly domain names (like www.example.com) into IP addresses (like 192.0.2.1) that computers use to identify each other on the network.\nHierarchy of Domain Names # Root Level Domain: The root level is the highest level in the DNS hierarchy and is represented by a dot (.) but is typically not visible in domain names. Root servers are the backbone of DNS, directing traffic to the correct Top-Level Domain (TLD) servers.\nTop-Level Domain (TLD): TLDs are the part of the domain name to the right of the dot. They are broadly categorized into two types: generic TLDs (gTLDs) like .com, .org, .net, and country code TLDs (ccTLDs) like .uk, .de, .ca. The TLD serves as a key part of the domain, indicating either the nature of the domain (like .com for commercial) or the geographical location (like .uk for the United Kingdom).\nSecond-Level Domain: The second-level domain is the segment of a domain name that directly precedes the TLD. For businesses or entities, this part of the domain name is often their brand or identity. For example, in google.com, \u0026ldquo;google\u0026rdquo; is the domain name chosen by the entity.\nSubdomain: Subdomains are additional parts of the domain that are added to the left of the second-level domain and separated by dots. They are used to organizing different sections or functions of a website. For instance, a company might use a subdomain for its blog (like blog.example.com) or a specific product (like product.example.com).\nDNS Resolution Process for www.google.com # User Requests the Domain:\nThe user enters www.google.com into their web browser. Browser Checks Cache:\nThe browser checks its own cache to see if it already has the IP address for www.google.com. Query to DNS Resolver:\nIf the IP address is not in the browser cache, the browser sends a query to a DNS resolver, typically provided by the ISP or a third-party service. DNS Resolver Checks Cache:\nThe DNS resolver checks its cache to see if it has a recent record of the IP address for www.google.com. DNS Resolver Queries Root Server:\nIf the IP address is not in the resolver\u0026rsquo;s cache, it queries one of the root name servers, which provides the IP address to the TLD name servers for .com. There are 13 logical root name servers around world. Each logical root name server has hundreds of physical hosts with same IP address. The query will be sent to the nearest physical root name server by router Query to .com TLD Name Server:\nThe resolver queries the .com TLD name server, which directs it to the name servers for the google.com domain. Query to google.com Name Server:\nThe resolver queries the google.com name servers to get the IP address for www.google.com. Resolver Receives IP Address:\nThe google.com name server responds with the IP address for www.google.com. DNS Resolver Caches the Response:\nThe DNS resolver caches the IP address for a specified time based on the TTL value. The TTL value is returned together with the IP address Resolver Returns IP Address to Browser:\nThe resolver sends the IP address back to the browser. Browser Caches the Response:\nThe browser caches the IP address for future use. Browser Connects to the IP Address:\nThe browser uses the IP address to establish a connection to the server hosting www.google.com, allowing the user to access the website. Reference # Amazon Route 53 Bilibili Video "},{"id":6,"href":"/docs/cloud/docker/containerize-full-stack-application/preamble/","title":"Docker Fundamentals Final Project","section":"Containerize Full Stack Application","content":" Final Project: Containerized Fullstack Web Application # 🚀 Preamble: Why This Project? # Throughout docker fundamental course, you\u0026rsquo;ve built a solid foundation in Docker — learning how to run containers, build images, manage networking, use volumes, optimize builds, and orchestrate multi-service setups with Docker Compose. Now it’s time to bring all of that knowledge together in a real-world fullstack project.\nThis final project mirrors how modern development teams design, deploy, and operate production-ready applications using Docker. It will reinforce your Docker expertise while giving you a hands-on, working example of containerized application architecture.\n📦 Project Summary # You’ll build a complete fullstack web application using:\nFastAPI for the backend API PostgreSQL for persistent data storage React for the frontend user interface Docker Compose to orchestrate all services seamlessly Each part of the project walks you through containerizing and wiring up these services into a cohesive development and runtime environment.\n🧩 Project Structure # Part Title Outcome Part 1 Set Up Backend API with FastAPI FastAPI runs inside Docker Part 2 Add PostgreSQL and Connect to FastAPI Backend connects to a database container Part 3 Create and Containerize the React Frontend React app runs in its own container Part 4 Wire Up Fullstack with Docker Compose One command runs frontend, backend, and DB Part 5 (opt) Add Reverse Proxy and Healthchecks Use NGINX and Docker healthchecks for production readiness ✅ Mini-Goals Throughout the Project # Orchestrate services using Docker Compose Persist database data using named volumes Connect services using Docker\u0026rsquo;s internal DNS and networks Use multi-stage builds for smaller, cleaner images Exclude unnecessary files using .dockerignore Monitor app readiness with HEALTHCHECK Use environment variables to inject secure configurations By the end, you’ll have a modular, production-style fullstack app you can run locally, deploy to the cloud, or use as a portfolio-quality reference.\nReady to dive in? Start with Project Part 1: Set Up Backend API with FastAPI.\n"},{"id":7,"href":"/docs/programming-core/java/netty/event-loop-group/","title":"EventLoop \u0026 EventLoopGroup","section":"Netty","content":" EventLoop \u0026amp; EventLoopGroup # EventLoop # What is an EventLoop?\nAn EventLoop in Netty is a fundamental component that handles all the events related to a single Channel.\nHow does EventLoop work?\nSingle Threaded: Each EventLoop is bound to a single thread, and each Channel is registered with one EventLoop. This means all I/O operations of a Channel are always executed by the same thread, ensuring thread safety and consistency.\nEvent Processing Loop: The EventLoop continuously checks for new events in its loop and processes them. Events might include connection acceptance, data read/write, or disconnection.\nTask Queue: Besides I/O operations, EventLoops have a task queue for tasks that are not directly related to I/O operations. This ensures that these tasks are also executed in the same thread, maintaining thread safety.\nEventLoopGroup # What is EventLoopGroup? An EventLoopGroup is a collection of EventLoop instances in Netty. It\u0026rsquo;s responsible for managing these EventLoops, which are the core components handling I/O operations and events for Channels (a connection or a socket).\nRoles of EventLoopGroup?\nBoss and Worker Groups: In server-side applications, there are typically two EventLoopGroups. The \u0026lsquo;boss\u0026rsquo; group accepts incoming connections and hands them over to the \u0026lsquo;worker\u0026rsquo; group, which then handles the actual I/O operations. See boss-and-work How does EventLoopGroup work?\nWhen a client attempts to connect to the server, this connection request first arrives at the boss EventLoopGroup. The boss EventLoopGroup accepts the connection and registers it with an EventLoop in the worker EventLoopGroup. Once a connection is assigned to an EventLoop in the worker group, that EventLoop is responsible for all the events and operations associated with that connection. The EventLoop in the worker EventLoopGroup processes incoming data, sends responses, and can execute tasks like message decoding, business logic processing, and message encoding. Examples # Execute a single task\nCode\nEventLoopGroup group = new NioEventLoopGroup(2); group.next().execute(() -\u0026gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { throw new RuntimeException(e); } log.debug(\u0026#34;Hello World!\u0026#34;); }); log.debug(\u0026#34;Hello main!\u0026#34;); Execution Result\n22:10:25.111 [main] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello main! 22:10:26.111 [nioEventLoopGroup-2-1] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello World! group.next() returns a EventLoop\nThe EventLoop can execute a Runnable task in its thread\nMain thread is not blocked\nExecute scheduled tasks\nCode EventLoopGroup group = new NioEventLoopGroup(2); AtomicInteger count = new AtomicInteger(0); group.next().scheduleAtFixedRate(() -\u0026gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { throw new RuntimeException(e); } log.debug(\u0026#34;Hello EventLoop-{}!\u0026#34;, count.getAndIncrement()); }, 0, 1, TimeUnit.SECONDS); Execution Result 22:10:26.112 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-0! 22:10:27.112 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-1! 22:10:28.113 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-2! 22:10:29.113 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-3! 22:10:30.114 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-4! 22:10:31.114 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-5! 22:10:32.114 [nioEventLoopGroup-2-2] DEBUG com.whatsbehind.netty_.component.EventLoopGroup_ - Hello EventLoop-6! ... "},{"id":8,"href":"/docs/distributed-system/load-balancing/haproxy/get-start/","title":"HAProxy: Get Started","section":"Haproxy","content":" HAProxy: Get Started # ✨ Goal of this Module # You will:\nSet up HAProxy using Docker Create a working HAProxy config for Layer 4 (TCP) load balancing Use nc (netcat) to simulate backend TCP servers Verify that HAProxy distributes connections across backends 🧱 Step 1: Project Setup # Create a folder for your HAProxy lab:\nmkdir haproxy-lab cd haproxy-lab touch haproxy.cfg docker-compose.yml Create two files inside it:\nhaproxy.cfg → the HAProxy configuration docker-compose.yml → to run HAProxy in Docker 🐳 Step 2: Docker Compose Setup # docker-compose.yml:\nservices: haproxy: image: haproxy:latest ports: - \u0026#34;9999:9999\u0026#34; # local_host_port:docker_container_port volumes: - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro # local_host_file:container_file:read-only This starts HAProxy in a container and loads your local haproxy.cfg.\n⚙️ Step 3: HAProxy Configuration # haproxy.cfg:\nglobal log stdout format raw daemon # Log to standard output; Format logs as raw; Run as deaemon defaults mode tcp timeout connect 5s # 5s timeout when connecting to backend timeout client 10s # Close connection if the client stops talking for 10s timeout server 10s # Close connection if backend stops talking for 10s frontend tcp_front # Declare frontend tcp_front (identifier) bind *:9999 # Take inbound traffic from all IPs on port 9999 default_backend tcp_backends backend tcp_backends # Define the pool of all backends balance roundrobin # User round robin algorithm to distribute request to backend servers server s1 host.docker.internal:9001 check # Connect to backend server s1 on localhost:9001; Do health check server s2 host.docker.internal:9002 check # Connect to backend server s2 on localhost:9002; Do health check host.docker.internal allows the HAProxy container to connect to services running on your host machine (your Mac or PC).\n💠 Step 4: Start HAProxy # Run this inside your haproxy-lab folder:\ndocker-compose up You should see logs like:\n[+] Running 1/0 ✔ Container haproxy-lab-haproxy-1 Created 0.0s Attaching to haproxy-1 haproxy-1 | [NOTICE] (1) : Initializing new worker (8) haproxy-1 | [NOTICE] (1) : Loading success. haproxy-1 | [WARNING] (8) : Server tcp_backends/s1 is DOWN, reason: Layer4 connection problem, info: \u0026#34;Connection refused\u0026#34;, check duration: 0ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue. haproxy-1 | [WARNING] (8) : Server tcp_backends/s2 is DOWN, reason: Layer4 connection problem, info: \u0026#34;Connection refused\u0026#34;, check duration: 1ms. 0 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue. haproxy-1 | [ALERT] (8) : backend \u0026#39;tcp_backends\u0026#39; has no server available! That means HAProxy is running but no backend servers are active\n🗅 Step 5: Start Persistent Netcat Servers # In two separate terminals, run these commands:\nServer A:\nwhile true; do echo \u0026#34;Hello from Server A\u0026#34; | nc -l 9001; done Server B:\nwhile true; do echo \u0026#34;Hello from Server B\u0026#34; | nc -l 9002; done This setup makes sure one message is sent per connection, and the server stays alive forever.\nYou should see logs in terminal of running HAProxy container\nhaproxy-1 | [WARNING] (8) : Server tcp_backends/s1 is UP, reason: Layer4 check passed, check duration: 2ms. 1 active and 0 backup servers online. 0 sessions requeued, 0 total in queue. haproxy-1 | [WARNING] (8) : Server tcp_backends/s2 is UP, reason: Layer4 check passed, check duration: 1ms. 2 active and 0 backup servers online. 0 sessions requeued, 0 total in queue. That means HAProxy is connecting to two backend servers\n🧲 Step 6: Test the Load Balancer # In another terminal:\nnc localhost 9999 You should see:\nHello from Server A Try it again:\nHello from Server B Keep running it and watch HAProxy alternate between servers — round-robin in action!\n"},{"id":9,"href":"/docs/programming-core/python/executing-a-python-file/","title":"How Is Python File Executed","section":"Python","content":" Concepts # __name__ # __name__ is a built-in variable (attribute) of a module. It is being used to indicate if a module is being run directly or being imported into other modules\nValue when module is running directly: When you run a Python module directly, __name__ of this module is set to __main__\nValue when module is imported: When the module is imported into others modules, __name__ is set to the module name (file name w/o .py extension). For example, a Python module my_module.py is imported, then its __name__ is set to my_module when it is imported\nExample # Assume you have two Python files main.py and my_module.py, and main.py is the running script and imports my_module.py\nDirectory structure\nsrc ├── main.py └── my_module.py my_module.py\n# my_module.py file print(f\u0026#34;__name__ in my_module file: {__name__}\u0026#34;) main.py\n# main.py file import my_module print(f\u0026#34;__name__ in main file: {__name__}\u0026#34;) Run python3 main.py in src directory, and you see logs in the terminal\n__name__ in my_module file: my_module __name__ in main file: __main__ Python Path (sys.path) # The sys.path in Python is a list of strings that specifies the search path for modules. When you import a module, the Python interpreter searches for the module in each directory listed in sys.path in order, from the first to the last. The search process stops as soon as the interpreter finds a module that matches the import statement, and then it attempts to load the module.\nHere\u0026rsquo;s what typically comprises sys.path:\nThe directory of the running script: The first entry in sys.path is the path to the directory containing the running script.\nPYTHONPATH Directories: If the PYTHONPATH environment variable is set, its contents are added to sys.path. PYTHONPATH is a list of directories separated by os.pathsep (which is ; on Windows and : on Unix/Linux) where Python looks for modules to import.\nStandard Library Directories: These are the directories that contain the Python standard library modules. The location of these directories depends on the Python installation.\nSite-packages Directory: This directory contains third-party modules installed using package managers like pip. There can be multiple site-packages directories if Python is configured with virtual environments or if there are user-specific installations (using pip install --user).\nBuilt-in Modules # Built-in modules are an integral part of Python and are automatically loaded with the Python interpreter, providing foundational functionality that Python code can utilize. They don\u0026rsquo;t have specific files like standard Python modules (.py files) on the disk, so they are not under directories in sys.path\nTo see all built-in modules of your Python interpreters:\nimport sys print(sys.builtin_module_names) You will see a list of built-in modules printed in the terminal. Some common built-in modules:\n(\u0026#39;sys\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;math\u0026#39;) To see origin of a module:\nimport importlib.util module_name = \u0026#34;sys\u0026#34; module_spec = importlib.util.find_spec(module_name) print(f\u0026#34;Origin of module [{module_name}]: [{module_spec.origin}]\u0026#34;) stdout Origin of module [sys]: [built-in] Origin of non-built-in modules, like site-package modules, is the directory containing the module. For example\nOrigin of module [langchain]: [/home/\u0026lt;user_name\u0026gt;/.local/lib/python3.10/site-packages/langchain/__init__.py] sys.modules # sys.modules is a dictionary in Python. It acts as a cache of all modules that have already been imported during the runtime of a program. This dictionary maps module names (as strings) to module objects, ensuring that each module is imported only once, even if it is referenced in multiple import statements across the program.\nCaching Mechanism # When you import a module, Python first checks sys.modules to see if the module is already loaded. If it is, Python uses the existing module object from sys.modules instead of reloading the module from file.\nAssume you have two Python files. When running python3 module_cache.py, you should only see \u0026ldquo;I am imported\u0026rdquo; get printed once.\nsrc ├── module_cache.py └── my_module.py my_module.py\n# my_module.py print(\u0026#34;I am imported\u0026#34;) module_cache.py\n# module_cache.py import my_module import my_module Module Compilation # Source Code Compilation\nWhen a Python module is imported or a Python script is executed, the Python interpreter first compiles the source code (.py files) into bytecode (.pyc files). Bytecode is a low-level, platform-independent representation of the source code that can be executed by the Python Virtual Machine (PVM).\nBytecode Execution\nAfter compilation, the bytecode is directly executed by the Python interpreter.\nIn-Memory Compilation\nThe bytecode for the running script is compiled in memory and is not automatically saved to disk.\nOn-Disk Cache\nThe bytecode of imported modules are cached on disk (__pycache__ directory) for further use. The __pycache__ directory is typically located in the same directory as the .py source file it corresponds to\nNamespace # A namespace in Python is essentially a mapping from names to objects (dict[str, Any]). It\u0026rsquo;s a container of all variables and their values (primitive type) or references (objects) in current scope\nType of Namespaces # Built-in namespace: Contains names provided by Python itself, such as built-in functions (print, len) and exception names.\nGlobal Namespace: Specific to a module, this contains names from various imported modules and variables defined at the level of the current module. Each module has its own global namespace.\nLocal Namespace: Specific to a function or method invocation, this contains names defined in the function. These names only exist while the function is executing.\nExamples # Below example explains the main purpose of namespace: Prevent naming conflicts by isolating names in separate scopes\nstring = \u0026#34;global namespace\u0026#34; def my_func(): string = \u0026#34;local namespace of my_func\u0026#34; print(string) def nested_func(): string = \u0026#34;local namespace of nested_func\u0026#34; print(string) nested_func() my_func() print(string) Initially, string is defined in global namespace of current module\nWhen function my_func() is invoked, a local namespace is created for it during its execution. A new string variable is defined in the local namespace\nWithin my_func(), nested_func() is defined and invoked. Another local namespace is created and nested within the local namespace of my_func(). Within the nested local namespace, a new string is defined\nOutput\nlocal namespace of my_func local namespace of nested_func global namespace Execution Process # File and File Structure # File structure\nsrc ├── main.py └── my_module.py Files\n# my_module.py print(\u0026#34;I am my_module.\u0026#34;) def greeting(): print(\u0026#34;Hello World!\u0026#34;) # main.py import my_module my_module.greeting() Executing python3 main.py # Python interpreter (python3) initializes\nPython constructs sys.path, including:\nThe directory of the script being run (src in this case)\nStandard library directories\nDirectories in PYTHONPATH\nSite-packages directories\nPython compiles and executes main.py\nPython loads my_module.py\nFirst Python searches the module in sys.modules cache\nBecause it is first time importing my_module, so the module is not in sys.modules cache. Then Python starts to search directories in sys.path list\nPython found my_module.py in the same directory containing the running script main.py and loads the file\nIf there is no cache file in __pycache__ under src or the source code of my_module.py has been modified since last compilation, Python compiles the source code into bytecode and stores it in __pycache__\nAfter compiling my_module, Python starts to execute it and print I am my_module in the terminal\nPython adds my_module to global namespace of main.py\nPython continues executing main.py\nInvokes greeting() function from my_module, and prints Hello World! in the terminal "},{"id":10,"href":"/docs/cloud/aws/security/kms/","title":"KMS","section":"Security","content":" What is KMS? # AWS Key Management Service is a full management service to generate and manage encryption keys\nKey Concepts # Customer Master Key (CMK) # CMKs are primary resources created and managed by KMS. It\u0026rsquo;s a logical representation of a master key. It includes metadata like key ID, description, alias and key state. More importantly, it contains key materials which are used to encrypt and decrypt your data.\nCMKs can be:\nCustomer-managed: These are created and managed by AWS users AWS-managed: There are created and managed by AWS services, like aws/s3, aws/ddb etc\u0026hellip; Key policies # Key policies are resource policies that are attached to every CMK to determine who has what permissions on the CMK under which conditions. Every CMK must have one and only one key policy\nData Keys # Data keys are SYMMETRIC keys generated by AWS KMS. They are normally used to encrypt and decrypt large size data on client side. You can use CMK to generate, encrypt and decrypt data keys\nWhy Need Data Keys? # AWS KMS APIs kms:Encrypt and kms:Decrypt only support data size less than 4 KB. The workaround to encrypt large size data is that KMS generates data keys and clients use the generated data keys to encrypt and decrypt large size data on their sides and no need to upload the data to KMS.\nHow to Use Data Keys? # Creating a CMK # First calling create-key API to create a CMK\naws kms create-key You will get response like:\n{ \u0026#34;KeyMetadata\u0026#34;: { ... \u0026#34;KeyId\u0026#34;: \u0026#34;bca1a73c-b9d2-43e2-97e7-XXXXXXXXXXXX\u0026#34;, ... } } Creating an alias for the CMK\naws kms create-alias --target-key-id \u0026#34;bca1a73c-b9d2-43e2-97e7-XXXXXXXXXXXX\u0026#34; --alias-name alias/demo-data-key We won\u0026rsquo;t use the generated CMK to encrypt data, but use the data key. That is called Envelop Encryption\nGenerating a Data Key # Generating a symmetric data key\naws kms generate-data-key --key-id alias/demo-data-key --key-spec AES_256 You will get response:\n{ \u0026#34;CiphertextBlob\u0026#34;: \u0026#34;AQIDAHiNuXXnu8SiDat5B2+53PrUzQvrztxy2goKhceTVwNoDAFboLcM9DYLJRprvSF16VWhAAAAfjB8BgkqhkiG9w0BBwagbzBtAgEAMGgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMaAKf1DtSn99SoSYsAgEQgDsPACB1itRaJX6yTjEQuWidGXpMYwL9x47oxbepRkjQjIta4jdisEZU3VbqJhGRKLJzwFepzU+ofZWKBA==\u0026#34;, \u0026#34;Plaintext\u0026#34;: \u0026#34;tJ6HM0yWt2SwkEGDhG7IgVilYlU8j2IaX+wrr24Af7w=\u0026#34;, \u0026#34;KeyId\u0026#34;: \u0026#34;arn\\:aws\\:kms\\:eu-central-1\\:XXXXXXXXXXXX\\:key/0265a04d-b823-4a16-9fbe-772a5cb6baea\u0026#34; } Components\nPlaintext: The data key, unencrypted. It is used to encrypt or decrypt data by clients. Clients should remove it from memory every time after use it CiphertextBlob: The data key, encrypted by the CMK. Clients should keep it persistently. Relationship\nThe plaintext and encrypted data key are generated by running an encryption algorithm on the CMK. One CMK can generate multiple data keys\nStoring encrypted data key\nexport DATA_KEY_CIPHERTEXT=\u0026#34;AQIDAHiNuXXnu8SiDat5B2+53PrUzQvrztxy2goKhceTVwNoDAFboLcM9DYLJRprvSF16VWhAAAAfjB8BgkqhkiG9w0BBwagbzBtAgEAMGgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMaAKf1DtSn99SoSYsAgEQgDsPACB1itRaJX6yTjEQuWidGXpMYwL9x47oxbepRkjQjIta4jdisEZU3VbqJhGRKLJzwFepzU+ofZWKBA==\u0026#34; echo $DATA_KEY_CIPHERTEXT | base64 --decode \u0026gt; data_key_ciphertext Decrypting Encrypted Data Key # After generating a data key, client keeps the encrypted data key and should never keep the plaintext data key in their disk. To encrypt data using data key, client needs to decrypt the encrypted data key to get the plaintext data key\nexport DATA_KEY_PLAINTEXT=`aws kms decrypt --ciphertext-blob fileb://./data_key_ciphertext --query Plaintext --output text` Encrypting with Plaintext Data Key # Now you have the plaintext data key. You can use your preferred encryption algorithm to encrypt your data with the plaintext data key. Remember to remove the plaintext data key from memory after using\nEnvelop Encryption # Envelop Encryption is to encrypt your plaintext data with a plaintext data key and then encrypt the plaintext data key with a master key (CMK)\nHow AWS Services Utilize KMS # Data Protection # Data protection in AWS refers to protecting data in transit (data transmits over wire) and protecting data at rest (storing data at disk). You can protect your data in transit using TLS/SSL protocol (HTTPS), or encrypt your data at client side. To protect data at rest, you have two options:\nServer-Side encryption Client-Side encryption Use S3 as example to introduce how KMS is involved in server-side encryption\nS3 Server-Side Encryption # SSE-S3 # SSE-S3 stands for Server-Side Encryption with Amazon S3-managed Keys.\nDefault option No KMS is involved, so no extra costs Only S3 related permissions are required to access objects in S3 buckets It\u0026rsquo;s the most basic encryption method provided by S3. After you upload an object to S3 buckets, before it is saved to disk, the object will be encrypted by a S3-managed key. When you download objects from S3 buckets, the encrypted objects will be decrypted first and then downloaded to client side.\nSSE-KMS # SSE-KMS stands for Server-Side Encryption with CMK stored in KMS\nLoss: KMS is involved, so you need to pay extra costs Gain: Extra layer of security. You need to have permissions of S3 and KMS to access objects in S3 buckets To upload an object encrypted with an AWS KMS key to Amazon S3, you need kms:GenerateDataKey permissions on the key. To download an object encrypted with an AWS KMS key, you need kms:Decrypt permissions. Encryption on each object provides high security, but that results in too many calls from S3 to KMS. To balance security and HTTP traffic (also costs), instead of generating data key for each object, you can use bucket key to encrypt objects.\nS3 Bucket Keys for SSE-KMS After you configure your bucket to use S3 Bucket Key for SSE-KMS, AWS generates a short-live key from AWS KMS and then keep it in S3 for a period of time. The bucket key will create data keys for new objects during its lifecycle. That eliminates a lot of requests to KMS to generate data key for each object. SSE-C # AWS S3 allows you to use custom encryption key for server-side encryption. Clients manage the encryption key and provide the key as part of their requests and S3 manages the encryption process.\nReference # AWS KMS Workshop\n"},{"id":11,"href":"/docs/distributed-system/load-balancing/load-balancer/","title":"Load Balancer","section":"Load Balancing","content":" Why Load Balancer? # Assume you are holding a service on a server and the service supports multiple connections. The traffic that it serves is not high, so the server can handle those traffic concurrently. Everything looks good, but unfortunately the server some day goes down, and it can\u0026rsquo;t serve any traffic. To improve the availability of your service, you bought another server and replicate your service code in the new server which can serves traffic also. Now you have two servers to receive clients\u0026rsquo; requests, and the question is how to determine the target server for each request? That\u0026rsquo;s where Load Balancer comes in\nWhat is Load Balancer? # Load Balancer is a network component sitting in front of a set of servers (hosts) to distribute clients\u0026rsquo; requests evenly to those servers behind it\nWhat are Benefits of Load Balancers? # Improve availability of your service Load balancer monitors servers behind it and forwards requests to healthy servers. If one server goes down, the load balancer removes it from the target list and forwards requests to other servers. Client can still send request and get response from the server\nEvenly distribute requests If you choose load balancer algorithm properly, requests should be sent to each server evenly and that protects servers from being overwhelmed\nType of Load Balancer # Hardware Load Balancer # Software Load Balancer # Application Load Balancer # This type of load balancer operates on application layer of OSI model, and it typically supports HTTPS/HTTP protocol. It routes requests based on request contents, so the SSL/TLS session needs termination, in other words, decrypting the request contents. The request will be forwarded to a server w/ or w/o TLS encryption.\nNetwork Load Balancer # This type of load balancer operates on transport layer, and it supports TCP protocol. It examines IP address and other network information to redirect traffic. Typically, no SSL/TLS termination is needed, because it has no idea what HTTP and TLS are.\nHow Does Load Balancer work? # The client builds a TCP connection with the load balancer and has a TLS handshake\nClient sends an HTTPS request to the load balancer\nLoad balancer decrypts the request\nLoad balancer reads and parses request contents to determine the target of the request\nLoad balancer builds a TCP connection with the selected target or use the active connections that was built\nLoad balancer forwards the request to the server and receives the response\nLoad balancer forwards the response to the client\nLoad Balancing Algorithm # A load balancer has multiple servers behind it, and load balancing algorithm is to choose the target for client requests.\nRound Robin method The requests are sent to server in a predefined order\nleast connection method The load balancer checks the connections with each server and select the server with the least connections\nWeighted least connection method Some servers can handle more active connections than others. You can assign different weights to servers and the load balancer will choose the server with least \u0026ldquo;active connections / weights\u0026rdquo;\nHealth Check # Load balancer monitors servers by pinging the service (IP address + port). That helps load balancer to manage health status of servers and forward requests to healthy servers. Once a server is unhealthy, the load balancer removes it from the target list until the server is healthy again.\nThrottling # When there is a peak of requests, and load balancer or the servers can\u0026rsquo;t handle so many requests, there are two main solutions\nSpillover/Throttling: The load balancer fails the request and return a response with error status code like 503 Request Queue: Instead of failing the request immediately, the load balancer queues the requests and forwards the requests once servers have capacity to handle them "},{"id":12,"href":"/docs/distributed-system/monitoring/metric-log/","title":"Metric \u0026 Log","section":"Monitoring","content":" Introduction # Metrics and logs are important for a service. They help to monitor the health of a service and can also be used to debug when service goes down or crashes. This post won\u0026rsquo;t discuss importance of monitoring system but focuses on how to design a monitoring system in your host\nRequirements # Upload Data to A Web Monitoring System # Logs are records of code execution that are stored somewhere in your host. Log files rotate with given interval, like 1 hr. You log files may look like below:\nservice.log-2024-01-01-1 There are some limitations to store logs in you host:\nSize limitation Depends on how long you want to retain your logs. However, regardless the log retention, 5 years or 10 years, it is a waste of disk storage to store logs in you host Limited query function When debugging service issues, you only care about specific log patterns within a range of time. What you can do is to use regular expression and grep to query the logs which is not handy. Apart from that, it is hard to virtualize the metrics for your system Considering above limitation, a good solution is to upload your logs and metrics to a monitoring system (like AWS CloudWatch) which\nhas unlimited storage provides out-of-box query features is able to virtualize your metrics monitors metrics and triggers actions if needed and more\u0026hellip; Decouple Data Collection and Publication # To upload data to a monitoring system, we can invoke provided APIs. However, it is not a good practice to call the APIs for each single log or metric in your service code considering a lot of network traffic and low efficiency\nThe better way is to decouple the process of emitting logs and metrics with publishing them. One solution is to buffer your logs and metrics in you hosts and have an out-of-process tool that publishes the records to remote machines\nservice.log service.metrics SDKs to Emit Logs and Metrics # Comparing with metrics, logs are easier to format. The most basic log should contain two parts, timestamp and message. Timestamp is the time when the log is emitted and message contains the execution details, including class and code line that emit the log, log level (INFO, DEBUG, WARNING, ERROR), and custom message.\nMetrics are more complex which contains more information. The SKD should have a model class for the metric. Also, it should provide APIs to emit the metrics which serializes the metric instance to a string with predefined format to the buffer files\nSample codes\nMetric metric = new Metric(); metric.setTime(...); metric.setName(...); metric.setValue(...); ... metric.emit(); Sample metric string\nThe contents within delimiter ----- is a metric serialization containing all parameters of a metric\n----- time: ... name: ... value: ... ... ----- Agent to publish metrics and logs # We should have a tool to publish metrics and logs to remote monitoring system. It could be out-of-process, so it doesn\u0026rsquo;t impact your service. It periodically reads log and metric files and uploads them. To balance the traffic and size of one request, selecting a proper time interval is important\nArchitecture # Components # Service # Your service that is running in the host and generating logs and metrics. It utilizes APIs from log/metric SKD to write logs and metrics to local files. The SDK should provide below features:\nMetric model APIs to write log and emit metric Serialize metric to a specific schema that is understandable by the log/metric agent Rotate log/metric files Metric/Log Files # Files that buffer logs and metrics generated by the service\nMetric/Log Agent # An out-of-process software that\npolls data from metric/log files transform metric schema to specific format that is predefined by the cloud monitoring system periodically push metrics and logs to the remote system Cloud Monitoring System # A third party tool that stores your metrics and logs. It provides some feature like data virtualization, log query, metric action triggering etc\u0026hellip;\nConcepts # Metric # A metric is a behavior of your service or system. It is easier to explain it using examples. Common metrics for a service including count, latency, errors of API calls.\nA metric is a collection of multiple data point which is one measurement. For example, when you receive an API request, the count metric should have one data point with value 1, which means your API gets called once. You record that time when receiving the request and when you finish the process before respond to the client, you record current time. The subtraction between endTime and startTime is the value of one latency data point. If the API process fails, then data point of error with value 1 is recorded.\nSchema # Metric: { nameSpace: string, unit: string, dimensions: { key: string, value: string } dataPoints: [ { timestamp: date, value: number/string } ] } Above is a simple schema of AWS CloudWatch metric.\nnameSpace: Group name of your metrics. You manage your metrics under different groups. You can create groups based on service name, resource type etc\u0026hellip; unit: The unit for the value of metric\u0026rsquo;s data point, i.e, COUNT, MILLI_SECONDS dimensions: Map of key value pairs, which are attributes of your metric dataPoints: Collection of metric measurements timestamp: The time when the data point is recorded value: The value for one data point, like ONE API call, 50 ms latency "},{"id":13,"href":"/docs/web/restful-api/restful-get/","title":"RESTful GET","section":"Restful API","content":" Understanding GET in RESTful APIs: Best Practices \u0026amp; Use Cases # Introduction # The GET method is one of the most fundamental HTTP methods used in RESTful APIs. It is designed to retrieve data from a server without making modifications. In this guide, we\u0026rsquo;ll explore how GET works, when to use it, best practices, and common patterns for designing RESTful GET APIs.\n1. What is the GET Method? # The GET method is used for reading and retrieving data. Unlike POST, PUT, or DELETE, it does not modify any server-side resources.\nKey Characteristics of GET: # ✔️ Read-only: It fetches data but does not change anything. ✔️ Idempotent: Multiple identical GET requests return the same result. ✔️ Safe: No risk of unintended changes to the server. ✔️ Cacheable: Can be cached to improve performance. ✔️ Does not require a request body: Data is sent through the URL using path parameters or query parameters.\n2. Common GET API Patterns in REST # a) Fetching a Specific Resource (Path Parameter :) # When retrieving a single resource, a path parameter (:) is commonly used.\n🔹 Example:\nGET /users/:userId ➡ GET /users/12345 retrieves user 12345.\n🔹 Response:\n{ \u0026#34;id\u0026#34;: 12345, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34; } ✅ When to use: When requesting a single entity.\nb) Fetching Multiple Resources (Collection Pattern) # When retrieving a list of resources, a collection endpoint is used.\n🔹 Example:\nGET /products ➡ Retrieves all products.\n🔹 Response:\n[ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Laptop\u0026#34;, \u0026#34;price\u0026#34;: 1200 }, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Phone\u0026#34;, \u0026#34;price\u0026#34;: 800 } ] ✅ When to use: When requesting a collection of items.\nc) Searching and Filtering (Query Parameters ?) # Use query parameters to filter, search, or refine results dynamically.\n🔹 Example:\nGET /products?category=electronics\u0026amp;price_lt=1000 ➡ Retrieves electronics under $1000.\n🔹 Response:\n[ { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Phone\u0026#34;, \u0026#34;price\u0026#34;: 800 } ] ✅ When to use: When applying search filters to a dataset.\nd) Pagination (Handling Large Datasets Efficiently) # Use page and pageSize to limit the number of results.\n🔹 Example:\nGET /users?page=2\u0026amp;pageSize=20 ➡ Retrieves page 2 with 20 users per page.\n🔹 Response:\n{ \u0026#34;page\u0026#34;: 2, \u0026#34;pageSize\u0026#34;: 20, \u0026#34;totalPages\u0026#34;: 5, \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: 21, \u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34; }, { \u0026#34;id\u0026#34;: 22, \u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34; } ] } ✅ When to use: When dealing with large datasets.\ne) Sorting Results # Use sort and order query parameters to arrange results.\n🔹 Example:\nGET /products?sort=price\u0026amp;order=desc ➡ Retrieves products sorted by price in descending order.\n🔹 Response:\n[ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Laptop\u0026#34;, \u0026#34;price\u0026#34;: 1200 }, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Phone\u0026#34;, \u0026#34;price\u0026#34;: 800 } ] ✅ When to use: When users need sorted lists.\n3. GET API Best Practices # ✅ 1. Use Nouns in URLs (Avoid Verbs) # ❌ Bad: GET /getUsers\n✅ Good: GET /users\n✅ 2. Use Query Parameters for Filters \u0026amp; Pagination # ❌ Bad: GET /users/filter?name=John\n✅ Good: GET /users?name=John\n✅ 3. Use Proper Status Codes # 200 OK → Request was successful. 204 No Content → No data available. 404 Not Found → Resource does not exist. 400 Bad Request → Invalid query parameters. ✅ 4. Support Caching # Use ETag and Cache-Control headers for faster responses.\n✅ 5. Use Pagination for Large Datasets # Do not return thousands of records in one response—use limit \u0026amp; offset.\n4. When NOT to Use GET # ❌ 1. When modifying data\nUse POST, PUT, or DELETE instead. Example: GET /deleteUser?userId=123 ❌ (wrong, use DELETE /users/123) ❌ 2. When sending sensitive data\nAvoid sending passwords or tokens in the URL (GET /login?password=abc123). ❌ 3. When handling complex search logic\nUse POST with a request body if the query is too large. 5. Summary # Feature GET Method Purpose Retrieve data (read-only) Modifies Data? ❌ No Idempotent? ✅ Yes Safe? ✅ Yes Caching Support? ✅ Yes Request Body? ❌ No (parameters in URL) Best Use Cases Fetching single or multiple resources "},{"id":14,"href":"/docs/web/http/start-http/","title":"Start HTTP","section":"HTTP","content":" 🌐 Introduction to HTTP: The Foundation of the Web # HTTP (Hypertext Transfer Protocol) is the cornerstone of communication on the web. It defines the rules by which clients (like browsers or mobile apps) and servers exchange information. Every time you open a website, submit a form, or load an image, you\u0026rsquo;re using HTTP.\n🧠 What is HTTP? # HTTP is a request-response protocol that allows a client to request data from a server and receive a response. It is:\nApplication-layer protocol: Operates on the top layer of the OSI model. Stateless: Each request is independent, meaning the server does not retain any information about previous requests. Text-based: Both request and response messages are human-readable and structured. 🔄 Components of HTTP Communication # Component Description Client Initiates the request. Examples include web browsers, Postman, or curl. Server Receives the request, processes it, and sends back a response. Request The message sent from client to server. Includes method, URL, headers, and (optionally) a body. Response The message sent from server to client. Includes status code, headers, and body. 🧾 Common HTTP Methods # GET – Retrieve data POST – Submit data PUT – Replace data PATCH – Modify data DELETE – Remove data 📤 Example HTTP Request # GET /hello HTTP/1.1 Host: example.com User-Agent: Mozilla/5.0 Accept: text/html 📥 Example HTTP Response # HTTP/1.1 200 OK Content-Type: application/json {\u0026#34;message\u0026#34;: \u0026#34;Hello, HTTP World!\u0026#34;} 🛠️ Hands-On Practice: Your First HTTP Server with FastAPI # ✅ Step 1: Set Up the Environment # Create a Python project and a virtual environment:\nmkdir master-http cd master-http python3 -m venv venv This creates an isolated environment named venv to manage dependencies independently of your system Python installation.\nActivate the environment:\nsource venv/bin/activate Install FastAPI and Uvicorn:\npip install fastapi uvicorn ✍️ Step 2: Create Your Server File # Create a subdirectory 01-start-http and a file named main.py:\nmkdir 01-start-http cd 01-start-http touch main.py Paste in the following Python code:\nfrom fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/hello\u0026#34;) def say_hello(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello, HTTP World!\u0026#34;} 🚀 Step 3: Run the Server # Use Uvicorn to start your FastAPI app:\nuvicorn main:app --reload Visit http://localhost:8000/hello in your browser, and you should see:\n{\u0026#34;message\u0026#34;: \u0026#34;Hello, HTTP World!\u0026#34;} 🧪 Step 4: Make a Request with Curl # Try calling your endpoint from the terminal:\ncurl http://localhost:8000/hello "},{"id":15,"href":"/docs/operating-system/mac-os/terminal-command/text-processing-and-searching/","title":"Text Processing and Searching","section":"Terminal Command","content":" grep # Introduction to grep # grep stands for \u0026ldquo;Global Regular Expression Print.\u0026rdquo; It is a powerful command-line utility used for searching plain-text data for lines that match a regular expression. grep is widely used for searching specific patterns within files and outputting the matching lines.\nBasic Syntax # grep [options] pattern [file...] pattern: The string or regular expression to search for. [file...]: The file(s) to search. If no files are specified, grep searches the input provided from the standard input. Common Options # -i: Ignore case distinctions in the pattern. -v: Invert the match, showing lines that do not match the pattern. -r or -R: Recursively search directories. -n: Prefix each line of output with the line number within its input file. -l: Print only the names of files containing matches. -c: Print only a count of matching lines per file. -E: Use extended regular expressions. -o: Only show the matched words Practices # Let\u0026rsquo;s have some hands-on practices for grep.\nSet up Practicing Env # Navigate to a directory that you want to practice grep\ncd ~/practics/mac-os/terminal-commands/grep Write contents to file practice.txt\ncat \u0026lt;\u0026lt;EOL \u0026gt; practice.txt practice practices Practices Learn with practices I am practicing grep command EOL Create a new directory and copy practice.txt to the nested directory\nmkdir nested_dir cp practice.txt nested_dir Verify that you have the practice env set up\ntree . The file structure should look like below\n. ├── nested_dir │ └── practice.txt └── practice.txt 2 directories, 2 files Make Your Hands Dirty # Basic Search\nSearches for \u0026ldquo;practice\u0026rdquo; in file practice.txt, and prints the matching lines\ngrep practice practice.txt The matching words are case-sensitive\npractice practices Learn with practices Case-Insensitive Search\nSearches for \u0026ldquo;practice\u0026rdquo; in practice.txt, ignoring case.\ngrep -i practice practice.txt Word \u0026ldquo;Practice\u0026rdquo; with capital \u0026ldquo;P\u0026rdquo; is matched\npractice practices Practices Learn with practices Recursive Search\nSearch for practice in all files under current directory, including nested directory\ngrep -r practice . Output\n./nested_dir/practice.txt:practice ./nested_dir/practice.txt:practices ./nested_dir/practice.txt:Learn with practices ./practice.txt:practice ./practice.txt:practices ./practice.txt:Learn with practices Count Matches\nPrints the number of lines containing \u0026lsquo;pattern\u0026rsquo; in practice.txt\ngrep -c practice practice.txt Output\n3 Display Line Numbers\nSearches for \u0026ldquo;practice\u0026rdquo; in practice.txt and displays the line numbers of matching lines\ngrep -n practice practice.txt Output\n1:practice 2:practices 4:Learn with practices Search for Exact Word\nSearches for the exact word \u0026ldquo;practice\u0026rdquo; in practice.txt\ngrep -w practice practice.txt Only word practice is exactly matched\npractice Invert Match\nPrints lines that do not match \u0026ldquo;practice\u0026rdquo; in practice.txt\ngrep -v practice practice.txt Output\nPractices I am practicing grep command Display Only Filenames\nPrints the names of files containing \u0026lsquo;pattern\u0026rsquo; within the current directory\ngrep -lr practice . Output\n./nested_dir/practice.txt ./practice.txt Using Extended RegEx\nSearches for \u0026ldquo;practice\u0026rdquo; and \u0026ldquo;practicing\u0026rdquo; in file practice.txt using extended regular expressions\ngrep -E \u0026#34;practice|practicing\u0026#34; practice.txt Output\npractice practices Learn with practices I am practicing grep command Only Show Matched Parts\nOnly shows the matched word \u0026ldquo;practice\u0026rdquo; and prints line number\ngrep -on practice practice.txt Output\n1:practice 2:practice 4:practice sed # "},{"id":16,"href":"/docs/distributed-system/technology/redis/why-is-redis-so-fast/","title":"Why Is Redis So Fast","section":"Redis","content":" Why is Redis So Fast? # Redis is a blazing-fast, in-memory data structure store that serves as a database, cache, and message broker. Its name stands for \u0026ldquo;Remote Dictionary Server,\u0026rdquo; and it’s an open-source project known for its speed, simplicity, and versatility.\nRedis achieves its incredible speed through several key design principles and optimizations:\n1. In-Memory Operations # Unlike traditional databases that rely on disk storage, Redis stores all data in memory. Modern DRAM (Dynamic Random-Access Memory) is a type of volatile memory widely used in computers for its high speed and low latency.\nUnderstanding MT/s and Bandwidth # For example, DDR5 DRAM—the latest generation of DRAM—can achieve memory access speeds of around 4800 MT/s (megatransfers per second). MT/s reflects the number of data transfers per second over the memory bus. DRAM employs double data rate (DDR) technology, which allows it to send data on both the rising and falling edges of the clock cycle, effectively doubling the transfer rate. Each transfer moves a fixed amount of data, and this is used to calculate the bandwidth (GB/s):\nBandwidth (GB/s) = MT/s × Bus Width (Bytes) ÷ 1000\nFor instance, with DDR5-4800, having 4800 MT/s and a bus width of 64 bits (8 bytes): Bandwidth = 4800 × 8 ÷ 1000 = 38.4 GB/s per channel.\nCalculating Redis Operations # Assume each Redis operation requires 100 bytes of data (both request and response). Using the theoretical bandwidth of 38.4 GB/s for a single channel, the rough queries per second (QPS) Redis could achieve is:\nQPS = Bandwidth (Bytes/s) ÷ Operation Size (Bytes) QPS = 38.4 × 10^9 ÷ 100 = 384 million operations per second.\nThis theoretical calculation showcases the massive potential throughput of Redis when operating entirely in memory with modern DRAM, far exceeding the capabilities of traditional disk-based systems and enabling sub-millisecond response times. However, this is just a rough calculation. In real-world scenarios, the QPS of Redis is bottlenecked by other factors such as CPU, network bandwidth, and I/O limitations, with memory being just one of the constraints.\n2. Efficient Data Structures # Redis uses highly optimized data structures specifically designed to maximize performance. For example, dictionaries in Redis allow O(1) average-time complexity for lookups and inserts, making key-value storage extremely efficient. Skip lists are used for sorted sets, enabling fast range queries and ordered operations with O(log n) complexity. Additionally, Redis employs lightweight compression techniques to reduce memory usage while ensuring minimal impact on data access speed. These tailored data structures ensure that Redis can handle millions of operations per second while maintaining consistent low latency.\n3. Single-Threaded Event Loop # The benefit of using a single-threaded event loop in Redis is that it eliminates the need for context switching and complex locking mechanisms, enabling more predictable and efficient operation. This simplicity also avoids issues like race conditions and deadlocks, making Redis highly reliable for lightweight commands and in-memory operations.\nHowever, this raises a critical question: can a single thread handle high volumes of operations in a modern computing environment? To address this, let’s calculate the theoretical performance of a single-threaded Redis instance, assuming one operation takes 100 CPU cycles on average.\nCalculating Redis Operations Per Second (CPU-bound) # CPU Clock Speed: 4.5 GHz (4.5 × 10^9 cycles/second) Cycles Per Operation: 100 CPU cycles per operation (assumption for lightweight Redis commands). Thus, the number of operations Redis can handle is:\nQPS = Clock Speed / Cycles Per Operation QPS = 4.5 × 10^9 / 100 = 45 million operations per second.\n4. Minimal Protocol Overhead # RESP (Redis Serialization Protocol) is a lightweight, custom protocol dedicated to Redis that offers several key advantages:\nCompact Format: RESP eliminates the overhead of verbose headers and negotiation mechanisms, which eases CPU usage by requiring significantly fewer cycles to parse and execute compared to general-purpose protocols like HTTP. Reduced Network Bandwidth: By containing fewer bytes in both requests and responses, RESP minimizes the network bandwidth required for communication, improving overall throughput. Integrated Pipelining: RESP seamlessly supports pipelining, allowing multiple commands to be sent in a single request, which further reduces latency and boosts performance. 5. Pipelining # Redis allows clients to send multiple commands in a single request through its pipelining feature, which reduces round-trip latency and significantly improves throughput. For example, instead of waiting for each command\u0026rsquo;s response before sending the next, a client can send a batch of commands like SET key1 value1, SET key2 value2, and GET key1 all at once. The server processes them sequentially and sends back a batch of responses, minimizing network delays. 6. Optimized Network IO # Redis handles network requests and responses efficiently using non-blocking I/O and event-driven mechanisms. Here’s a step-by-step explanation of how Redis handles client requests and responses:\nStep-by-Step Handling of a Request and Response in Non-Blocking I/O # Client Sends a TCP Request:\nThe client sends a command (e.g., GET key1) over an established TCP connection to Redis. The data is broken into packets and sent to the server. OS Receives TCP Packets:\nThe operating system (OS) kernel on the server receives the incoming packets, reassembles them, and stores the complete request in the socket buffer, which is managed by the OS. In non-blocking mode, Redis’s main thread does not wait for the complete message to be ready in the buffer. Instead, Redis continues processing other ready events in its event loop until notified by the OS that the request is ready for reading. Redis Reads the Request:\nWhen the complete request is ready in the socket buffer, the OS notifies Redis that it is ready to be read and processed. Redis’s main thread processes the \u0026ldquo;ready-to-read\u0026rdquo; event in its event loop. It executes a non-blocking read() system call to fetch the request data from the socket buffer into Redis’s user-space memory. If the full request is not yet available, Redis skips this socket and continues processing other events, avoiding blocking. Redis Processes the Request:\nRedis parses and executes the command (e.g., GET key1) by retrieving the key’s value from its in-memory data structure. The response (e.g., value1) is prepared in memory, waiting to be written to the socket buffer. Redis Writes the Response to the Socket Buffer:\nRedis executes a non-blocking write() system call to send the response to the socket buffer. If the socket buffer has enough space, the data is copied immediately, and the write() call returns. If the buffer is full (e.g., due to a slow client), the write() call defers, and Redis marks the socket as \u0026ldquo;waiting for readiness.\u0026rdquo; After marking, Redis does not block; instead, it immediately moves on to process other ready requests or tasks in its event loop. Redis retries the write when the OS notifies that the socket is writable again, ensuring efficient use of the main thread while maintaining responsiveness. OS Transmits the Data to the Client:\nOnce the response is in the socket buffer, the OS and the network interface card (NIC) handle the actual transmission of the data over the network. Redis itself is not involved in this process; the OS takes full responsibility for reliably sending the data to the client, including handling retransmissions and acknowledgments. Key Points of Non-Blocking I/O # Redis never blocks its main thread while waiting for I/O operations (e.g., receiving a request, writing a response). The OS handles low-level tasks like managing socket buffers and transmitting data over the network. Redis’s event loop ensures it efficiently processes ready events (e.g., requests from other clients) while waiting for I/O readiness. This non-blocking I/O model, combined with Redis’s single-threaded design, allows it to scale efficiently and handle thousands of concurrent connections without bottlenecks.\nNetwork Bandwidth Calculation # Let’s calculate the theoretical maximum number of requests Redis can handle based solely on network bandwidth. Assume both the request and response are 100 bytes each.\nAssumptions: # Request size = 200 bytes, considering network overhead like TCP/IP headers Response size = 200 bytes Total data per operation = 200 (request) + 200 (response) = 400 bytes Network bandwidth = 10 Gbps (10 gigabits per second) Calculation: # Convert bandwidth to bytes per second:\n10 Gbps = 10 × 10^9 bits/second\n1 byte = 8 bits, so:\nBandwidth (bytes/second) = 10 × 10^9 ÷ 8 = 1250 × 10^6 bytes/second\nCalculate the maximum requests per second:\nRequests per second (RPS) = Bandwidth (bytes/second) ÷ Data per operation (bytes)\nRPS = 1250 × 10^6 ÷ 400 = 3.125 million requests per second\nConclusion: # With a 10 Gbps network connection, Redis can theoretically handle up to 3.125 million requests per second network bandwidth-wise, assuming no other bottlenecks (e.g., CPU or memory).\n7. Partitioning # With Redis Cluster, data can be partitioned across multiple nodes, enabling horizontal scaling while maintaining high performance. If higher QPS is required and a single thread cannot handle the load, the system can be scaled by partitioning incoming requests and deploying more Redis instances to distribute the workload efficiently.\n"},{"id":17,"href":"/docs/cloud/k8s/why-k8s/","title":"Why K8S?","section":"K8s","content":" 🚀 Kubernetes for Beginners: Why Software Engineers Should Care About It # 📅 Preamble # Software today isn’t just about writing applications — it’s about running them reliably.\nAs systems grow larger and more complex, managing applications becomes just as important as building them.\nContainers like Docker changed how we package software, but when you have dozens or hundreds of containers, managing them manually becomes overwhelming.\nThat’s where Kubernetes comes in — a powerful system to automate and scale container management.\nThis post introduces Kubernetes in simple language for software engineers who have no prior experience but are eager to get started.\n🔢 What Problem Does Kubernetes Solve? # Imagine your team builds several services: an API, a frontend, and a database.\nYou containerize each service with Docker — great!\nBut soon, problems arise:\nHow do you start and stop hundreds of containers across many servers? How do you restart services automatically if they crash? How do you scale up when user traffic spikes? How do you update applications without downtime? How do services find each other securely? Managing containers manually just doesn’t scale.\nKubernetes solves all of these problems.\nIt automates the deployment, scaling, networking, and health management of containers across clusters of servers.\n💡 What Is Kubernetes (k8s)? # Kubernetes (often abbreviated as k8s) is an open-source system for automating the deployment, scaling, and operation of containerized applications.\nOriginally developed at Google, Kubernetes has become the standard platform for running modern applications across clouds and datacenters.\nAt its core, Kubernetes:\nDeploys your applications Monitors and heals them when things go wrong Scales them based on load Manages their networking and security It’s like having an army of smart assistants constantly ensuring your app stays healthy, available, and fast.\n🔍 Key Concepts in Kubernetes (A Light Preview) # Before diving deeper in later lessons, here’s a sneak peek at Kubernetes building blocks:\nPod: The smallest unit in Kubernetes — usually wraps one container. Node: A machine (virtual or physical) where Pods run. Cluster: A collection of Nodes managed by Kubernetes. Service: A stable address to access one or more Pods. Deployment: A description of how many Pods you want and how they should behave. You don’t need to memorize these yet — just recognize that Kubernetes organizes, watches, and controls your app infrastructure.\n🌐 How Kubernetes Relates to Docker and Containers # Many engineers wonder:\n\u0026ldquo;I know Docker — isn’t that enough?\u0026rdquo;\nHere’s the relationship:\nConcept Role Container A small, lightweight, isolated environment for running apps. Docker A popular tool to build and run containers easily. Kubernetes A system to manage many containers across many machines reliably. Simple Analogy:\nDocker helps you build a car (container). Kubernetes manages a fleet of cars (containers) — parking them, fixing them, scaling them when needed. Both are essential but solve different parts of the application delivery puzzle.\n📚 Real-World Use Cases for Kubernetes # You might wonder where Kubernetes is actually used.\nHere are some real-world examples:\nWeb Applications: Companies like Airbnb and Shopify use Kubernetes to manage hundreds of services. Auto-Scaling: E-commerce apps automatically add servers during holiday sales. High Availability: Banks use Kubernetes to ensure their critical services stay online. Blue-Green Deployments: Tech companies update apps with zero downtime using Kubernetes. Multi-Cloud Strategy: Enterprises run Kubernetes across AWS, Azure, and on-premises servers seamlessly. In short, Kubernetes runs mission-critical applications all around you.\n💼 Why Software Engineers Should Learn Kubernetes # Whether you\u0026rsquo;re a backend engineer, frontend engineer, or full-stack developer, learning Kubernetes will:\nDeepen your system understanding — know how apps live beyond your laptop. Boost your career — Kubernetes skills are in huge demand. Empower your personal projects — deploy your side-projects reliably at scale. Make you production-ready — think like a true software architect, not just a coder. Even if you don’t operate infrastructure daily, understanding Kubernetes helps you design better, more resilient, and scalable systems.\n🚀 What’s Next? # In the next post, we\u0026rsquo;ll get hands-on:\nInstall Kubernetes on your laptop with Minikube. Deploy your very first application to Kubernetes. Understand the basic building blocks by doing, not just reading. Stay tuned! Your journey to Kubernetes mastery starts now.\n"},{"id":18,"href":"/docs/programming-core/java/nio/blocking-mode/","title":"Blocking Mode","section":"Nio","content":" Blocking Mode # This post will introduce the blocking mode of network connection and communication in Java code. We will first wirte both server and client codes. Then demo the blocking mode and its problems.\nCode Example # Server\n@Slf4j public class Server { public static void main(String[] args) throws IOException { ByteBuffer buffer = ByteBuffer.allocate(32); // Create server ServerSocketChannel ssc = ServerSocketChannel.open(); // Server listens to port 9999 at local host ssc.bind(new InetSocketAddress(9999)); log.debug(\u0026#34;Create server listening to port 9999\u0026#34;); List\u0026lt;SocketChannel\u0026gt; socketChannels = new ArrayList\u0026lt;\u0026gt;(); while (true) { // Accept: build connection with client log.debug(\u0026#34;Waiting for client connection...\u0026#34;); SocketChannel socketChannel = ssc.accept(); log.debug(\u0026#34;Build connection with client \u0026#34; + socketChannel.getRemoteAddress()); // Add socketChannel to a list socketChannels.add(socketChannel); // Iterate all SocketChannels and read data from channel and write it to the ByteBuffer for (SocketChannel channel : socketChannels) { log.debug(\u0026#34;Start to read channel from client \u0026#34; + channel.getRemoteAddress()); channel.read(buffer); buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); log.debug(\u0026#34;Complete reading data\u0026#34;); } } } } This is code for the server. First we create a ServerSocketChannel listens to port 9999 of local host We have an infinite while loop where the server accepts connection from clients. Once connection is built, the server creates a new SocketChannel and add it into a list After connection is built, we iterate collection of all SocketChannel, and read data from the channel and write it to a ByteBuffer In the server code above, we add multiple logs before and after client connection and reading data from the channel to better demonstrate blocking mode.\nClient\n@Slf4j public class Client { public static void main(String[] args) throws IOException { SocketChannel sc = SocketChannel.open(); log.debug(\u0026#34;Connecting to server\u0026#34;); sc.connect(new InetSocketAddress(InetAddress.getLocalHost(), 9999)); log.debug(\u0026#34;Connected with server\u0026#34;); while (true) { String input = Scanner_.scanLine(\u0026#34;Input: \u0026#34;); log.debug(\u0026#34;Sending data [{}] to server\u0026#34;, input); sc.write(StandardCharsets.UTF_8.encode(input)); log.debug(\u0026#34;Sent data [{}] to server\u0026#34;, input); } } } In client\u0026rsquo;s code, we connect to the server Then in a while loop, we scan input from terminal and send the data to the server Same as server\u0026rsquo;s code, we add some logs\nDemo # Start the server # Server log:\n20:29:52.904 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Create server listening to port 9999 20:29:52.906 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Waiting for client connection... We start a server, and server is blocked until a client connects to the server. This is the first blocking place in the server code. SocketChannel socketChannel = ssc.accept(); Start clientA # Server log:\n20:33:01.791 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Build connection with client /127.0.0.1:50632 20:33:01.791 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Start to read channel from client /127.0.0.1:50632 Client of port(50632) connects to the server\nServer code is blocked again until recevies any data from clientA\nchannel.read(buffer); ClientA log:\n20:33:01.777 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Connecting to server 20:33:01.780 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Connected with server Input: ClientA conects to the server and waiting for user to type in the terminal Send data from clientA to the server # ClientA log:\nInput: Hello Server from ClientA! 20:38:02.356 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Sending data [Hello Server from ClientA!] to server 20:38:02.358 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Sent data [Hello Server from ClientA!] to server Input: In clientA, we typed Hello Server from ClientA!. After hitting Enter key, the string we just typed was sent to the server. Server log:\n20:38:02.358 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server from ClientA! 20:38:02.359 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Complete reading data 20:38:02.359 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Waiting for client connection... Server receives the string Hello Server from ClientA!, and prints the string in the terminal Server is not blocked at reading data from channel, and code is executed again Server is blocked at waiting for connection from another client Send data again from clientA to the server # ClientA log:\nInput: Hello Server from ClientA Again! 20:42:28.083 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Sending data [Hello Server from ClientA Again!] to server 20:42:28.084 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Sent data [Hello Server from ClientA Again!] to server Input: ClientA sends a new string Hello Server from ClientA Again! to the server Server\nServer doesn\u0026rsquo;t have any new logs printed. Because it was blocked at waiting for client connection, no matter how much data sent from clientA, it won\u0026rsquo;t be handled by the server ClientB connects to the server # Client B log:\n20:45:17.107 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Connecting to server 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Client - Connected with server Input: A new clientB connects to the server Server log:\n20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Build connection with client /127.0.0.1:50698 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Start to read channel from client /127.0.0.1:50632 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server from ClientA Again! 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Complete reading data 20:45:17.111 [main] DEBUG com.whatsbehind.netty_.nio.blocking.Server - Start to read channel from client /127.0.0.1:50698 ClientB from port(50698) connects to the srever. After new client connection, code is executed again. Server starts to read channels from clientA (port 50632) and prints the data sent some time ago Hello Server from ClientA Again Server is blocked again waiting for clientB sending data\u0026hellip; Summary # As we can see in the demo, blocking mode is low efficient and unable to hanle multiple client connections using one thread. To handle connection and receiving data from multiple clients properly, we have to create a new thread for each connection, which is unrealistic. In next post, we will discuss unblocking mode and its underlying problems.\n"},{"id":19,"href":"/docs/programming-core/java/netty/channel/","title":"Channel \u0026 ChannelFuture","section":"Netty","content":" Concept # What is Channel? # Definition\nA Channel in Netty represents an open network connection, such as a socket. It\u0026rsquo;s a key abstraction that encapsulates the underlying network transport, such as TCP or UDP.\nRole\nData Communication: A Channel is used for reading data from and writing data to the network. State Management: It keeps track of the state of a network connection (e.g., whether it\u0026rsquo;s open, connected, etc.). What is ChannelFuture? # Definition\nA ChannelFuture represents the result of an asynchronous Channel I/O operation. It\u0026rsquo;s a promise-like mechanism that provides a way to be notified when an asynchronous operation completes.\nUsage # Create a ChannelFuture # ChannelFuture channelFuture = new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer\u0026lt;NioSocketChannel\u0026gt;() { @Override protected void initChannel(NioSocketChannel nioSocketChannel) throws Exception { nioSocketChannel.pipeline().addLast(new StringEncoder()); } }) .connect(new InetSocketAddress(9999)); Channel Connect # Let\u0026rsquo;s add a LoggingHandler to better track the lifecycle of the Channel\nChannelFuture channelFuture = new Bootstrap() .group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler(new ChannelInitializer\u0026lt;NioSocketChannel\u0026gt;() { @Override protected void initChannel(NioSocketChannel nioSocketChannel) throws Exception { + nioSocketChannel.pipeline().addLast(new LoggingHandler()); nioSocketChannel.pipeline().addLast(new StringEncoder()); } }) .connect(new InetSocketAddress(9999)); Synchronous Way # Code\nlog.debug(\u0026#34;Start to connect...\u0026#34;); channelFuture.sync(); log.debug(\u0026#34;Connected\u0026#34;); Channel channel = channelFuture.channel(); channel.writeAndFlush(\u0026#34;Hello World\u0026#34;); Execution Result\n2023-12-31 11:16:55 [main] DEBUG c.w.netty_.component.channel.Client - Start to connect... 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2] REGISTERED 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2] CONNECT: 0.0.0.0/0.0.0.0:9999 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2, L:/127.0.0.1:49310 - R:0.0.0.0/0.0.0.0:9999] ACTIVE 2023-12-31 11:16:55 [main] DEBUG c.w.netty_.component.channel.Client - Connected 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2, L:/127.0.0.1:49310 - R:0.0.0.0/0.0.0.0:9999] WRITE: 11B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 57 6f 72 6c 64 |Hello World | +--------+-------------------------------------------------+----------------+ 2023-12-31 11:16:55 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x77a252c2, L:/127.0.0.1:49310 - R:0.0.0.0/0.0.0.0:9999] FLUSH Explain\nThe lifecycle of Channel and custom logs are printed the in the terminal Two thread: \u0026ldquo;main\u0026rdquo; and \u0026ldquo;worker\u0026rdquo; (nioEventLoopGroup-2-1) are involved Netty build the connection from the client to the server in a non-blocking asynchronous way. Main thread initiates the connection The connection then is handed over to the worker thread The Channel is REGISTERED -\u0026gt; CONNECT -\u0026gt; ACTIVE sync() stops the main thread until the Channel becomes CONNECT Then \u0026ldquo;Hello World\u0026rdquo; is written and flushed Although Netty builds the connection in a non-blocking and asynchronous way, because of using sync() method, the logs are printed in linear order like building the connection synchronously Asynchronous Way / Callback # Code\nchannelFuture.addListener(future -\u0026gt; ((ChannelFuture) future).channel().writeAndFlush(\u0026#34;Hello World\u0026#34;)); Execution Result:\n2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807] REGISTERED 2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807] CONNECT: 0.0.0.0/0.0.0.0:9999 2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807, L:/127.0.0.1:49450 - R:0.0.0.0/0.0.0.0:9999] WRITE: 11B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 57 6f 72 6c 64 |Hello World | +--------+-------------------------------------------------+----------------+ 2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807, L:/127.0.0.1:49450 - R:0.0.0.0/0.0.0.0:9999] FLUSH 2023-12-31 11:31:08 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x46b4d807, L:/127.0.0.1:49450 - R:0.0.0.0/0.0.0.0:9999] ACTIVE How does it work?\nIn Netty, operations like reading, writing or connecting over a network channel are asynchronous. When you invoke such an operation, it doesn\u0026rsquo;t complete immediately.\nPerform the Asynchronous Operation: You initiate an operation, like channel connection, which returns a ChannelFuture Attach Listener to ChannelFuture: You use addListener to attach a ChannelFutureListener to this ChannelFuture Completion of the Operation: When the operation you listen to finishes, the callback method is invoked Channel Close # Synchronous Way # Code\nChannel channel = channelFuture.sync().channel(); channel.close(); ChannelFuture closeFuture = channel.closeFuture(); closeFuture.sync(); group.shutdownGracefully(); Execution result\n2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311] REGISTERED 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311] CONNECT: localhost/127.0.0.1:9999 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311, L:/127.0.0.1:45504 - R:localhost/127.0.0.1:9999] ACTIVE 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311, L:/127.0.0.1:45504 - R:localhost/127.0.0.1:9999] CLOSE 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311, L:/127.0.0.1:45504 ! R:localhost/127.0.0.1:9999] INACTIVE 2023-12-31 21:26:27 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x6675c311, L:/127.0.0.1:45504 ! R:localhost/127.0.0.1:9999] UNREGISTERED Asynchronous Way # Code Channel channel = channelFuture.sync().channel(); channel.close(); channel.closeFuture().addListener(future -\u0026gt; { group.shutdownGracefully(); }); Channel Lifecycle # In the execution history of closing the Channel, we can see the lifecycle or state of a Channel\nREGISTERED: The Channel is registered with an EventLoop. This means it\u0026rsquo;s now bound to a particular thread for its I/O operations CONNECT: The Channel is trying to connect to a server ACTIVE: A Channel is considered active when it\u0026rsquo;s connected to a remote peer and ready for I/O operations CLOSE: The Channel closes the connection and releases all underlying resources. A closed Channel can\u0026rsquo;t be reopened INACTIVE: A Channel in the INACTIVE state has been disconnected from the remote endpoint but is not necessarily fully closed. It is potential for reuse UNREGISTERED: The Channel is unregistered from its EventLoop "},{"id":20,"href":"/docs/programming-core/java/thread/common-methods/","title":"Common Methods","section":"Thread","content":" Common Methods of Java Thread # Sleep(long millis) # Static method that causes the currently executing thread to sleep (temporarily cease execution) for the specified number of milliseconds. If a thread is sleeping, its state is changed to TIMED_WAITING Sleep method could be interrupted by calling interrupt(). After interruption, sleep throws InterruptedException, and thread state is changed to RUNNABLE Example # Code Thread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { log.debug(\u0026#34;Interrupted\u0026#34;); log.debug(\u0026#34;t1 state: {}\u0026#34;, Thread.currentThread().getState()); } }); t1.setName(\u0026#34;t1\u0026#34;); t1.start(); Thread.sleep(50); log.debug(\u0026#34;t1 state: {}\u0026#34;, t1.getState()); log.debug(\u0026#34;Interrupting thread t1\u0026#34;); t1.interrupt(); Execution results 2024-01-06 12:55:33 [main] DEBUG c.w.thread_.commonmethods.Sleep_ - t1 state: TIMED_WAITING 2024-01-06 12:55:33 [main] DEBUG c.w.thread_.commonmethods.Sleep_ - Interrupting thread t1 2024-01-06 12:55:33 [t1] DEBUG c.w.thread_.commonmethods.Sleep_ - Interrupted 2024-01-06 12:55:33 [t1] DEBUG c.w.thread_.commonmethods.Sleep_ - t1 state: RUNNABLE join() # Waits for the thread to die or terminate. Use case # Code\n@Slf4j public class Join_ { static int result = 0; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(50); result++; } catch (InterruptedException e) { throw new RuntimeException(e); } }); t1.start(); log.debug(\u0026#34;result value: {}\u0026#34;, result); } } Execution Result\n2024-01-06 13:18:42 [main] DEBUG c.w.thread_.commonmethods.Join_ - result value: 0 result was printed in the main thread, but its value is not incremented in thread t1 yet, because the thread first sleeps 50ms Solution # Code\n@Slf4j public class Join_ { static int result = 0; public static void main(String[] args) throws InterruptedException { Thread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(50); result++; } catch (InterruptedException e) { throw new RuntimeException(e); } }); t1.start(); + t1.join(); log.debug(\u0026#34;result value: {}\u0026#34;, result); } } Flowchart\nExecution Result\n2024-01-06 13:18:42 [main] DEBUG c.w.thread_.commonmethods.Join_ - result value: 1 In above code snippet, we invoke t1.join() method, which is a blocking method. main thread will wait here until t1 thread terminates State # The thread that is waiting for another thread to terminate is in State WAITING\nCode\nThread t2 invokes t1.join() and waits for t1 to terminate Check t2 state while it is blocked at t1.join() Thread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(50); log.debug(\u0026#34;t1 terminates\u0026#34;); } catch (InterruptedException e) { throw new RuntimeException(e); } }); t1.setName(\u0026#34;t1\u0026#34;); Thread t2 = new Thread(() -\u0026gt; { try { log.debug(\u0026#34;t2 is waiting for t1 terminating\u0026#34;); t1.join(); log.debug(\u0026#34;t2 terminates\u0026#34;); } catch (InterruptedException e) { throw new RuntimeException(e); } }); t2.setName(\u0026#34;t2\u0026#34;); t1.start(); t2.start(); Thread.sleep(20); log.debug(\u0026#34;t2 state: {}\u0026#34;, t2.getState()); Execution result\n2024-01-06 15:36:16 [t2] DEBUG c.w.thread_.commonmethods.Join_ - t2 is waiting for t1 terminating 2024-01-06 15:36:16 [main] DEBUG c.w.thread_.commonmethods.Join_ - t2 state: WAITING 2024-01-06 15:36:16 [t1] DEBUG c.w.thread_.commonmethods.Join_ - t1 terminates 2024-01-06 15:36:16 [t2] DEBUG c.w.thread_.commonmethods.Join_ - t2 terminates interrupt() # The interrupt() method is used to signal a thread that it should interrupt its current operation. This method is part of the Thread class. When interrupt() is called on a thread:\nIf the thread is executing a blocking operation (like sleep(), wait(), or join()), it immediately throws an InterruptedException, which can be caught and handled. This exception CLEARS the interrupted status of the thread. If the thread is NOT in a blocking operation, it doesn\u0026rsquo;t immediately cause an effect other than setting the thread\u0026rsquo;s interrupted status. Example # Interrupt a blocking operation\nThread t1 = new Thread(() -\u0026gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, Thread.currentThread().isInterrupted()); } }, \u0026#34;t1\u0026#34;); t1.start(); Thread.sleep(50); log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, t1.isInterrupted()); t1.interrupt(); log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, t1.isInterrupted()); 2024-01-06 21:58:27 [main] DEBUG c.w.t.c.InterruptBlockedThread - t1 is interrupted false 2024-01-06 21:58:27 [main] DEBUG c.w.t.c.InterruptBlockedThread - t1 is interrupted true 2024-01-06 21:58:27 [t1] DEBUG c.w.t.c.InterruptBlockedThread - t1 is interrupted false Before interrupting t1 thread, its interrupted status is false After invoking interrupt() method, the interrupted status is set to true immediately Thread t1 is interrupted while it is sleeping, and the interruption throws InterruptedException. The exception clears the interrupted status Interrupt a running operation\nThread t1 = new Thread(() -\u0026gt; { while(true) { if (Thread.currentThread().isInterrupted()) { break; } } }, \u0026#34;t1\u0026#34;); t1.start(); Thread.sleep(50); log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, t1.isInterrupted()); t1.interrupt(); log.debug(\u0026#34;t1 is interrupted {}\u0026#34;, t1.isInterrupted()); 2024-01-06 22:02:18 [main] DEBUG c.w.t.c.InterruptRunningThread - t1 is interrupted false 2024-01-06 22:02:18 [main] DEBUG c.w.t.c.InterruptRunningThread - t1 is interrupted true If a thread is interrupted while it is running an operation. Only interrupted status is set to true. Hence interrupt() method is a polite way to signal the thread to stop, and still give the thread to handle some logic and clear some resources before terminating the thread Gracefully Terminate Threads # Code\n@Slf4j public class GracefullyTerminateThread { Thread thread; public GracefullyTerminateThread() {} public void start() { thread = new Thread(() -\u0026gt; { while(!Thread.currentThread().isInterrupted()) { try { log.debug(\u0026#34;Hello World\u0026#34;); Thread.sleep(1000); } catch (InterruptedException e) { log.debug(\u0026#34;Interrupt sleep\u0026#34;); Thread.currentThread().interrupt(); } } log.debug(\u0026#34;Thread gracefully terminates\u0026#34;); }); thread.start(); } public void gracefullyStop() { if (thread != null) { thread.interrupt(); } } } Flowchart\nThe thread continuously performs its task (logging and sleeping) in a loop, constantly checking for an interrupt signal. When gracefullyStop() is called from outside (another thread or method), it signals the thread to stop by setting its interrupted status. Upon receiving the interrupt signal, the thread exits its normal operation, performs any necessary clean-up, and then terminates. Execution\nTest code\npublic class GracefullyTerminate { public static void main(String[] args) throws InterruptedException { GracefullyTerminateThread thread = new GracefullyTerminateThread(); thread.start(); Thread.sleep(3000); thread.gracefullyStop(); } } Result\n2024-01-07 14:18:39 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Hello World 2024-01-07 14:18:40 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Hello World 2024-01-07 14:18:41 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Hello World 2024-01-07 14:18:42 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Interrupt sleep 2024-01-07 14:18:42 [Thread-0] DEBUG c.w.t.c.g.GracefullyTerminateThread - Thread gracefully terminates "},{"id":21,"href":"/docs/distributed-system/technology/redis/data-presistence/","title":"Data Persistence","section":"Redis","content":" Redis Data Persistence # Redis Data Persistence: How Does It Work? # Redis is widely known for its blazing-fast in-memory operations, but it also provides robust mechanisms for ensuring data persistence, making it suitable for a wide range of use cases. Let’s dive into how Redis achieves data persistence and ensures durability.\n1. Snapshotting (RDB - Redis Database File) # Snapshotting is a point-in-time persistence mechanism where Redis saves the entire dataset to disk at regular intervals.\nHow it Works # Redis creates a binary dump of the dataset in the form of an RDB file by forking a child process. The child process handles the snapshot creation entirely, ensuring minimal impact on runtime performance as the main thread remains free to serve client requests. Configuration # Snapshotting can be configured using the save directives in the Redis configuration file (redis.conf). For example:\nsave 60 1000 # Save a snapshot every 60 seconds if at least 1000 changes occurred Advantages # Quick Recovery: Snapshots provide a complete and consistent dataset, enabling fast recovery if the server crashes. Minimal Performance Impact: The child process handles I/O, leaving the main thread responsive. Disadvantages # Potential Data Loss: Changes made after the last snapshot will be lost if the server crashes. Resource Intensive: Copying the entire dataset can be I/O heavy, especially for large datasets. 2. Append-Only File (AOF) # The Append-Only File is a more granular persistence mechanism that logs every write operation to a file in a write-ahead manner, similar to a write-ahead log (WAL) in traditional databases. This ensures that data changes are saved incrementally and can be reconstructed reliably during a restart.\nHow it Works # Redis logs each write operation (e.g., SET, DEL) in an append-only format, which can be configured to log either every single write or at a specified interval. During recovery, Redis replays these logged operations to reconstruct the dataset. Configuration # Enable AOF persistence with the following in redis.conf:\nappendonly yes # Redis provides three options for AOF synchronization: - appendfsync always: Every write operation is immediately flushed to disk. This ensures maximum durability but may impact performance. - appendfsync everysec: Write operations are flushed to disk every second, balancing durability and performance. - appendfsync no: The OS decides when to flush data to disk, prioritizing performance over durability. Advantages # Higher Durability: Logs every operation, reducing potential data loss. Disadvantages # Performance Overhead: Logging every operation (e.g., appendfsync always) can introduce significant disk I/O and latency. Large File Sizes: AOF files are generally larger than RDB snapshots. AOF Rewriting: To address growing file sizes, Redis compacts the AOF file by rewriting it with the minimal set of commands needed to rebuild the current dataset. This process, while efficient, introduces additional disk I/O. Slow Recovery: Incremental recovery from AOF files can take longer compared to RDB snapshots, especially for large datasets, as Redis replays every logged operation to reconstruct the dataset. "},{"id":22,"href":"/docs/networking/dns-hands-on/","title":"DNS Hands On","section":"Networking","content":" Introduction # In dns post, I introduced hierarchy of domain names and how browser queries IP address for a domain name from domain name servers. This post we will query domain name servers step by step to get the IP address of domain google.com. Also, I will introduce a new concept record in domain name server\nEnvironment and Tools # I will use CLI dig to query domain name server in Linux system\nRecords # Records are resourced managed by domain name servers. They are a mapping of domain name to its various type of information. You can think records as entries in the DB managed by domain name servers. Records have below important fields\nName # The domain name\nType # Type of the record. Here are some common record types\nA (Address Record): Maps a domain name directly to its IPv4 address AAAA (Quad Record): Maps a domain name directly to its IPv6 address CNAME (Canonical Name Record): Maps a domain name to another domain name. It is used to alias one domain name to another NS (Name Server Record): Map a domain name to a domain name server TTL (Time To Live) # TTL specifies the duration in seconds that the record may be cached by the resolver. After this time, the resolver must query the name server again to ensure it has the current record\nValue # Value varies depending on the record type\nA/AAAA record: it contains the IP address of the domain name points to CNAME record: the target domain to which it alias to Hands On # Query Root Domain . # Command\ndig NS . dig: CLI command to look up records in domain name server NS: Query NS records .: The domain name to query. Here is root domain The query is sent to the root name server, and the router will select the nearest physical name server and route the query to it.\nResponse\n;; ANSWER SECTION: . 0 IN NS f.root-servers.net. . 0 IN NS g.root-servers.net. . 0 IN NS h.root-servers.net. . 0 IN NS i.root-servers.net. . 0 IN NS j.root-servers.net. . 0 IN NS k.root-servers.net. . 0 IN NS l.root-servers.net. . 0 IN NS m.root-servers.net. . 0 IN NS a.root-servers.net. . 0 IN NS b.root-servers.net. . 0 IN NS c.root-servers.net. . 0 IN NS d.root-servers.net. . 0 IN NS e.root-servers.net. a.root-servers.net. 0 IN A 198.41.0.4 a.root-servers.net. 0 IN AAAA 2001:503:ba3e::2:30 b.root-servers.net. 0 IN A 170.247.170.2 b.root-servers.net. 0 IN AAAA 2801:1b8:10::b c.root-servers.net. 0 IN A 192.33.4.12 c.root-servers.net. 0 IN AAAA 2001:500:2::c d.root-servers.net. 0 IN A 199.7.91.13 d.root-servers.net. 0 IN AAAA 2001:500:2d::d e.root-servers.net. 0 IN A 192.203.230.10 e.root-servers.net. 0 IN AAAA 2001:500:a8::e f.root-servers.net. 0 IN A 192.5.5.241 f.root-servers.net. 0 IN AAAA 2001:500:2f::f g.root-servers.net. 0 IN A 192.112.36.4 The command returns 13 NS records of root domain servers and A/AAAA records of each root name server.\nRoot Name Server # There are 13 logical root name servers around the world, naming from a to m and with .root-servers.net. as suffix. However, it doesn\u0026rsquo;t mean there are only 13 physical servers. Actually each logical root name server has hundreds of physical servers with same IP address using anycast addressing. When the router receives client\u0026rsquo;s query to root servers, it will select the nearest physical server with the least latency or other network conditions\nQuery Name Server of com. # Command\ndig @a.root-servers.net. NS com. Here I pick one root name server a.root-servers.net. and query the NS record for domain name com.\nResponse\n;; AUTHORITY SECTION: com. 172800 IN NS e.gtld-servers.net. com. 172800 IN NS b.gtld-servers.net. com. 172800 IN NS j.gtld-servers.net. com. 172800 IN NS m.gtld-servers.net. com. 172800 IN NS i.gtld-servers.net. com. 172800 IN NS f.gtld-servers.net. com. 172800 IN NS a.gtld-servers.net. com. 172800 IN NS g.gtld-servers.net. com. 172800 IN NS h.gtld-servers.net. com. 172800 IN NS l.gtld-servers.net. com. 172800 IN NS k.gtld-servers.net. com. 172800 IN NS c.gtld-servers.net. com. 172800 IN NS d.gtld-servers.net. ;; ADDITIONAL SECTION: e.gtld-servers.net. 172800 IN A 192.12.94.30 e.gtld-servers.net. 172800 IN AAAA 2001:502:1ca1::30 b.gtld-servers.net. 172800 IN A 192.33.14.30 b.gtld-servers.net. 172800 IN AAAA 2001:503:231d::2:30 j.gtld-servers.net. 172800 IN A 192.48.79.30 j.gtld-servers.net. 172800 IN AAAA 2001:502:7094::30 m.gtld-servers.net. 172800 IN A 192.55.83.30 m.gtld-servers.net. 172800 IN AAAA 2001:501:b1f9::30 i.gtld-servers.net. 172800 IN A 192.43.172.30 i.gtld-servers.net. 172800 IN AAAA 2001:503:39c1::30 f.gtld-servers.net. 172800 IN A 192.35.51.30 f.gtld-servers.net. 172800 IN AAAA 2001:503:d414::30 a.gtld-servers.net. 172800 IN A 192.5.6.30 a.gtld-servers.net. 172800 IN AAAA 2001:503:a83e::2:30 g.gtld-servers.net. 172800 IN A 192.42.93.30 g.gtld-servers.net. 172800 IN AAAA 2001:503:eea3::30 h.gtld-servers.net. 172800 IN A 192.54.112.30 h.gtld-servers.net. 172800 IN AAAA 2001:502:8cc::30 l.gtld-servers.net. 172800 IN A 192.41.162.30 l.gtld-servers.net. 172800 IN AAAA 2001:500:d937::30 k.gtld-servers.net. 172800 IN A 192.52.178.30 k.gtld-servers.net. 172800 IN AAAA 2001:503:d2d::30 c.gtld-servers.net. 172800 IN A 192.26.92.30 The response contains two sections of answers:\nAuthority Section: This section lists the authoritative name servers for the .com TLD Additional Section: This section lists the IPv4 and IPv6 addresses for the authoritative TLD name servers Query Name Server of google.com # Command\ndig @e.gtld-servers.net. NS google.com Send the query to the TLD name server e.gtld-servers.net.\nResponse\n;; AUTHORITY SECTION: google.com. 172800 IN NS ns2.google.com. google.com. 172800 IN NS ns1.google.com. google.com. 172800 IN NS ns3.google.com. google.com. 172800 IN NS ns4.google.com. ;; ADDITIONAL SECTION: ns2.google.com. 172800 IN AAAA 2001:4860:4802:34::a ns2.google.com. 172800 IN A 216.239.34.10 ns1.google.com. 172800 IN AAAA 2001:4860:4802:32::a ns1.google.com. 172800 IN A 216.239.32.10 ns3.google.com. 172800 IN AAAA 2001:4860:4802:36::a ns3.google.com. 172800 IN A 216.239.36.10 ns4.google.com. 172800 IN AAAA 2001:4860:4802:38::a ns4.google.com. 172800 IN A 216.239.38.10 The response also contains two sections: the authoritative name servers google.com and their IP addresses. Those records are NS records for google.com name servers instead of web servers. To get the IP address of the web server, we need to make an extra query to the Google\u0026rsquo;s name server\nQuery IP Address of google.com Web Server # Command\ndig @ns2.google.com. google.com Response\n;; ANSWER SECTION: google.com. 300 IN A 142.250.217.78 The response only contains one A record which is the IPv4 address of google.com web server. You can type the IP address in your browser, and you will be routed to website www.google.com\n"},{"id":23,"href":"/docs/distributed-system/load-balancing/haproxy/health-check/","title":"HAProxy: Health Check","section":"Haproxy","content":" HAProxy: Health Check # In a typical load-balanced setup, if one of your backend services crashes or becomes unreachable, HAProxy can detect this and stop sending traffic to it. Health checks help HAProxy:\nDetect failures early Prevent routing to broken services Recover gracefully when the backend comes back 🔧 Basic Health Check Configuration # Here is a simple example:\nbackend tcp_backends balance roundrobin option tcp-check server s1 host.docker.internal:9001 check inter 2s fall 3 rise 2 server s2 host.docker.internal:9002 check inter 2s fall 3 rise 2 🔍 Explanation:\noption tcp-check: Enables HAProxy to perform active TCP checks. check: Enables health checks on the server. inter 2s: Run a check every 2 seconds. fall 3: Mark the server DOWN after 3 failed checks. rise 2: Mark the server UP after 2 successful checks. 🔹 Optional Advanced Check Options # port\nUse a different port for health checks (if it\u0026rsquo;s not the same as the service port):\nserver s1 host.docker.internal:9001 check port 9101 addr\nUse a different IP for the health check:\nserver s1 192.168.1.10:9001 check addr 10.0.0.5 observe layer4\nForce HAProxy to only use TCP-level checks (default in mode tcp):\nserver s1 host.docker.internal:9001 check observe layer4 on-marked-down shutdown-sessions\nImmediately terminate active connections between HAProxy and backend servers when a server is marked down:\nserver s1 host.docker.internal:9001 check fall 3 rise 2 on-marked-down shutdown-sessions 📊 Observing Health Transitions # When a server is marked DOWN or comes back UP, HAProxy logs entries like:\nhaproxy-1 | [WARNING] (8) : Server tcp_backends/s1 is DOWN, reason: Layer4 connection problem, info: \u0026#34;Connection refused at initial connection step of tcp-check\u0026#34;, check duration: 0ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue. haproxy-1 | [WARNING] (8) : Server tcp_backends/s1 is UP, reason: Layer4 check passed, check duration: 1ms. 2 active and 0 backup servers online. 0 sessions requeued, 0 total in queue. To test this manually:\nStart two backend servers (e.g., using nc) Kill one of them Watch HAProxy log the failure after 3 checks Restart the server See HAProxy restore it after 2 successful checks "},{"id":24,"href":"/docs/web/http/http-methods-and-status-codes/","title":"HTTP Methods \u0026 Status Codes","section":"HTTP","content":" 📘 HTTP Methods \u0026amp; Status Codes # Understanding HTTP methods and status codes is essential for designing and debugging web APIs. This lesson explores the most commonly used HTTP methods and how servers communicate the results of requests using standardized status codes.\n🔧 Common HTTP Methods # HTTP methods (also called verbs) define the action the client wants the server to perform. Here are the most commonly used methods:\nMethod Purpose Idempotent Safe GET Retrieve data ✅ ✅ POST Submit data to create resource ❌ ❌ PUT Replace a resource ✅ ❌ PATCH Modify part of a resource ❌ ❌ DELETE Remove a resource ✅ ❌ ✅ Idempotent means the request can be made multiple times with the same result.\n✅ Safe means the request does not change server state.\n🚦 HTTP Status Codes # HTTP responses include status codes to indicate the outcome of a request. These codes are grouped into categories:\nCode Range Meaning Examples 1xx Informational 100 Continue 2xx Success 200 OK, 201 Created, 204 No Content 3xx Redirection 301 Moved Permanently, 304 Not Modified 4xx Client Error 400 Bad Request, 404 Not Found 5xx Server Error 500 Internal Server Error 🔍 Notable 2xx Status Codes # 200 OK: A generic success response. Typically, includes a response body. 201 Created: Indicates that a new resource has been created, commonly used after a successful POST request. May include a Location header with the URL of the newly created resource. 204 No Content: Indicates success but no content is returned in the response body. Often used for DELETE or silent PUT requests. 🛠️ Hands-On Practice with FastAPI # In this exercise, you\u0026rsquo;ll implement endpoints for each HTTP method and return the appropriate status code.\n📄 Create or Update main.py # Create a new subdirectory 02-http-methods-status-codes and a new Python file main.py:\nmkdir 02-http-methods-status-codes cd 02-http-methods-status-codes touch main.py Paste in the following Python code:\nfrom fastapi import FastAPI from fastapi.responses import JSONResponse from pydantic import BaseModel app = FastAPI() # In-memory store items = {} class Item(BaseModel): name: str description: str @app.get(\u0026#34;/items/{item_id}\u0026#34;, status_code=200) def read_item(item_id: str): if item_id not in items: return JSONResponse(status_code=404, content={\u0026#34;error\u0026#34;: \u0026#34;Item not found\u0026#34;}) return items[item_id] @app.post(\u0026#34;/items/{item_id}\u0026#34;, status_code=201) def create_item(item_id: str, item: Item): items[item_id] = item.dict() return items[item_id] @app.put(\u0026#34;/items/{item_id}\u0026#34;, status_code=200) def replace_item(item_id: str, item: Item): items[item_id] = item.dict() return items[item_id] @app.patch(\u0026#34;/items/{item_id}\u0026#34;, status_code=200) def update_description(item_id: str, item: Item): if item_id in items: items[item_id][\u0026#34;description\u0026#34;] = item.description return items.get(item_id, {\u0026#34;error\u0026#34;: \u0026#34;Item not found\u0026#34;}) @app.delete(\u0026#34;/items/{item_id}\u0026#34;, status_code=204) def delete_item(item_id: str): if item_id in items: del items[item_id] return 🧪 Try These Commands in Your Terminal # Use Uvicorn to start your FastAPI app:\nuvicorn main:app --reload Invoke APIs in another terminal (simulate clients):\n# Create an item curl -X POST http://localhost:8000/items/123 -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Book\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;FastAPI Guide\u0026#34;}\u0026#39; # Read the item curl http://localhost:8000/items/123 # Replace the item curl -X PUT http://localhost:8000/items/123 -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Notebook\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Updated\u0026#34;}\u0026#39; # Patch the description curl -X PATCH http://localhost:8000/items/123 -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Notebook\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Final revision\u0026#34;}\u0026#39; # Delete the item curl -X DELETE http://localhost:8000/items/123 # Read the item after deletion curl http://localhost:8000/items/123 You should see logs in server side:\nINFO: 127.0.0.1:62315 - \u0026#34;POST /items/123 HTTP/1.1\u0026#34; 201 Created INFO: 127.0.0.1:62317 - \u0026#34;GET /items/123 HTTP/1.1\u0026#34; 200 OK INFO: 127.0.0.1:62319 - \u0026#34;PUT /items/123 HTTP/1.1\u0026#34; 200 OK INFO: 127.0.0.1:62321 - \u0026#34;PATCH /items/123 HTTP/1.1\u0026#34; 200 OK INFO: 127.0.0.1:62323 - \u0026#34;DELETE /items/123 HTTP/1.1\u0026#34; 204 No Content INFO: 127.0.0.1:62325 - \u0026#34;GET /items/123 HTTP/1.1\u0026#34; 404 Not Found Logs in client side:\n{\u0026#34;name\u0026#34;:\u0026#34;Book\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;FastAPI Guide\u0026#34;} {\u0026#34;name\u0026#34;:\u0026#34;Book\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;FastAPI Guide\u0026#34;} {\u0026#34;name\u0026#34;:\u0026#34;Notebook\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Updated\u0026#34;} {\u0026#34;name\u0026#34;:\u0026#34;Notebook\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Final revision\u0026#34;} {\u0026#34;error\u0026#34;:\u0026#34;Item not found\u0026#34;}% "},{"id":25,"href":"/docs/cloud/docker/fundamentals/lesson-1-start-docker/","title":"Lesson 1: Start Docker","section":"Fundamentals","content":" 🐳 Docker Mastery Course – Lesson 1 # 💡 Introduction: What Is Docker and Why Use It? # Welcome to Lesson 1 of the Docker Mastery Course! In this lesson, you\u0026rsquo;ll discover what Docker is, why it\u0026rsquo;s valuable, and how to verify your installation by running your very first container. This is the first step in your journey to mastering Docker through a project-driven, hands-on approach.\n🎯 Learning Objective # By the end of this lesson, you will:\nUnderstand the purpose of Docker Learn how Docker solves common deployment problems Install Docker Run and inspect your first Docker container 📚 Core Concepts # 1. The Problem Docker Solves # Have you ever had an app that runs perfectly on your local machine but crashes in production? This happens because the environments differ:\nDifferent operating systems Different library versions Different configurations Docker solves this issue by packaging everything your application needs into a container, ensuring consistency across development, testing, and production.\n2. What Is Docker? # Docker is a containerization platform. It allows developers to bundle an application and all its dependencies into a standardized unit called a container. These containers:\nAre lightweight and fast Run consistently across different environments Simplify testing and deployment 3. Key Terminology # Term Description Image A blueprint for creating containers; read-only. Container A running instance of an image. Docker Engine The core software that runs and manages containers. 🛠️ Hands-On Practice # ✅ Step 1: Install Docker # For macOS: # Visit Docker Desktop for Mac Download and install the application Grant necessary permissions during setup Launch Docker Desktop (look for the whale icon in the top menu bar) Verify the installation:\ndocker --version Example output:\nDocker version 24.0.2, build cb74dfc ✅ Step 2: Run Your First Container # Run a simple test container:\ndocker run hello-world What this does:\nPulls the hello-world image from Docker Hub Creates a container from that image Executes it and prints a success message Expected output:\nHello from Docker! This message shows that your installation appears to be working correctly. ... ✅ Step 3: List Your Containers # To list all containers (even those that have exited):\ndocker ps -a You should see a container entry for hello-world.\n🏁 Checkpoint # You’ve successfully:\nInstalled Docker Learned what Docker is and why it\u0026rsquo;s useful Run your first Docker container 🎉 Congratulations! You’re officially a Docker user.\n📅 What’s Next? # Lesson 2: Basic Docker Commands\nLearn how to start, stop, and manage real-world containers (like Nginx) using essential Docker CLI commands.\nStay tuned and get ready to deepen your Docker skills!\n"},{"id":26,"href":"/docs/system-design/common-interview-questions/lesson-1-start-docker/","title":"Lesson 1: Start Docker","section":"System Design","content":" 🐳 Docker Mastery Course – Lesson 1 # 💡 Introduction: What Is Docker and Why Use It? # Welcome to Lesson 1 of the Docker Mastery Course! In this lesson, you\u0026rsquo;ll discover what Docker is, why it\u0026rsquo;s valuable, and how to verify your installation by running your very first container. This is the first step in your journey to mastering Docker through a project-driven, hands-on approach.\n🎯 Learning Objective # By the end of this lesson, you will:\nUnderstand the purpose of Docker Learn how Docker solves common deployment problems Install Docker Run and inspect your first Docker container 📚 Core Concepts # 1. The Problem Docker Solves # Have you ever had an app that runs perfectly on your local machine but crashes in production? This happens because the environments differ:\nDifferent operating systems Different library versions Different configurations Docker solves this issue by packaging everything your application needs into a container, ensuring consistency across development, testing, and production.\n2. What Is Docker? # Docker is a containerization platform. It allows developers to bundle an application and all its dependencies into a standardized unit called a container. These containers:\nAre lightweight and fast Run consistently across different environments Simplify testing and deployment 3. Key Terminology # Term Description Image A blueprint for creating containers; read-only. Container A running instance of an image. Docker Engine The core software that runs and manages containers. 🛠️ Hands-On Practice # ✅ Step 1: Install Docker # For macOS: # Visit Docker Desktop for Mac Download and install the application Grant necessary permissions during setup Launch Docker Desktop (look for the whale icon in the top menu bar) Verify the installation:\ndocker --version Example output:\nDocker version 24.0.2, build cb74dfc ✅ Step 2: Run Your First Container # Run a simple test container:\ndocker run hello-world What this does:\nPulls the hello-world image from Docker Hub Creates a container from that image Executes it and prints a success message Expected output:\nHello from Docker! This message shows that your installation appears to be working correctly. ... ✅ Step 3: List Your Containers # To list all containers (even those that have exited):\ndocker ps -a You should see a container entry for hello-world.\n🏁 Checkpoint # You’ve successfully:\nInstalled Docker Learned what Docker is and why it\u0026rsquo;s useful Run your first Docker container 🎉 Congratulations! You’re officially a Docker user.\n📅 What’s Next? # Lesson 2: Basic Docker Commands\nLearn how to start, stop, and manage real-world containers (like Nginx) using essential Docker CLI commands.\nStay tuned and get ready to deepen your Docker skills!\n"},{"id":27,"href":"/docs/project/node-js-auth/","title":"Node Js Auth","section":"Project","content":" Introduction # This project is for learning purpose. It is a practice of\nhow to use JWT (Json Web Token) to authenticate user login with Google using OAuth2 Tech Stack # Node.js Express: Quickly start a local host MongoDB/Mongoose: Database to store users @hapi/joi: Package to validate parameters of objects bcryptjs: Hash confidential information including passwords in this project jsonwebtoken: JWT package to sign and verify a auth token axios: Send HTTP requests querystring: Package to parse and assembly query string in HTTP request Reference # Implement JWT using node.js and express: Very nice video which guides me step by step to build this project Google OAuth2 with node.js: Video that explains and implements OAuth2 flow from end to end OAuth 2.0 and OpenID Connect: Plain English explains the evolution of OAuth2 and OpenID Connect What Is JWT? # A JWT (JSON Web Token) is like a compact digital note or a small piece of data that web servers and clients (like your browser or a mobile app) use to communicate secure information. It\u0026rsquo;s like a tiny, encoded message.\nStructure of JWT # A JWT is made up of three parts, each encoded in base64 and separated by dots (.):\nHeader: The header typically consists of two parts: the type of the token, which is JWT, and the signing algorithm being used, such as SHA256 or RSA. Payload: The payload contains information of the identity, like id, isa(issuedAt), and etc\u0026hellip; Signature: This is the secure part of JWT. It is created by taking encoded header, encoded payload, a secret only known by the server, then running it through the signing algorithm mentioned in the header. If the algorithm is Symmetric algorithm (like HS256): The same secret is used for signing and verification Asymmetric algorithm (like RS256): A private key will be used for signing, and a corresponding public key will be used for verification Risk of JWT # Data is Not Encrypted The first two parts of a JWT token are only base64 encoded, but not encrypted, so everyone can decode them and read data directly. Hence, confidential information should not be stored in the payload Susceptible to Theft If a JWT is stolen, it could by used by unauthorized parties to gain access to the system No Revocation Mechanism Once issued, a JWT can\u0026rsquo;t be revoked before it expires Key Management Challenges The security of JWT depends on how secret key is managed by the server. If the secret is not well managed or securely stored, it could lead security vulnerabilities Best Practice # Use HTTPS to prevent interception of tokens. Keep expiration times as short as practical. Avoid storing sensitive data in the token. Implement token refresh mechanisms. Securely manage signing keys. API # Register # Url: /api/user/register Method: POST Request Body { name: string, email: string, password: string } Response Body { userId: string } Business logic Validate parameters in the request body Validate email is not registered in DB Insert a new user in DB Login # Url: /api/user/login Method: POST Request Body { email: string, password: string } Response Header { auth-token: string } Body { token: string } Business logic Verify if email and password match Sign a new JWT and return it Posts # Url: /api/posts Method: GET Request Header { auth-token: string } Response Body { posts: { title: string, description: string } } Business logic Verify the auth-token from request headers Return a hard-coded post object After users login, server signs a JWT to users. By include the JWT in the request headers, server could verify it to determine if user is authenticated (login)\nOAuth2 # What is OAuth2? # OAuth2 is an open standard for access delegation, which is commonly used as the way for internet users to grant a client or application access to their resources under another server without sharing their passwords\nWhy OAuth2? # Secure Delegation of Access: Users often need to grant a third party website or application access to their data on another service (like accessing your google account from a social media app). Sharing credentials for this purpose is highly insecure\nFine Grained Authorization: User may not want to give full access of their all data\nStandardization and Interoperability: With requirements of authentication and authorization from many internet service, a standard way is needed\nReducing Password Fatigue: Users are overwhelmed by the need to create username and password for each single service\nKey Components of OAuth2 # Resource Owner: The user who authorizes an application to access their account Client: The application that wants to access user\u0026rsquo;s account Resource Server: The server hosting user\u0026rsquo;s data Authorization Server: The server that authenticates user and issues access token to the application How OAuth2 works # Authorization Request\nThe client requests authorization to access user\u0026rsquo;s resources. This is usually done through a redirection, where client passes along its identity (client id) and the scope of the access it\u0026rsquo;s requesting User Authenticate and Consent\nThe user is asked to login to the authorization server and to approve the requested access by the client Authorization Grant\nUpon successful authentication and consent, authorization server issues an authorization grant to the client. The authorization grant can be of different type, like an authorization code or an implicit grant, depending on the OAuth flow being used Access Token Request (In case of authorization code grant)\nIf an authorization code is granted, then client exchanges the code for an access token. This is done by sent a request to authorization server\u0026rsquo;s token endpoint where client authenticates itself and presents the authorization code Issuance of Access Token\nThe authorization server authenticates the client validates the authorization grant and if valid issues an access token (and possibly a refresh token) Accessing the Resource\nThe client uses access to token to make a request to the resource server for the protected resources Resource Server Validates Token\nThe resource server validates the token and if valid serves the request Resource Deliver\nThe client receives the protected resources Why OpenID Connect # OAuth2 was originally designed for authorization, but not for authentication. Different companies have their own standards to authenticate users using OAuth2, in other words, OAuth2 is overused for authentication.\nOpenID Connect is an authentication layer built on top of OAuth2. It was developed to address the need for a standardized authentication process.\nHow OpendID Connect works # Authentication Request The client includes openid in the scope of the authorization request. That indicates that the client is requesting an ID token in addition to access token Token Response Access Token: Just like OAuth2, access token is issued by the authorization server. This token grants access to user\u0026rsquo;s resources ID Token: This is unique to OpenID Connect. The ID token is a JWT which contains claims about user\u0026rsquo;s identity and profile Token Validation Upon receiving the tokens, the client must validate the ID tokens to ensure its integrity and authenticity. This involves verifying JWT signature and the claims it contains "},{"id":28,"href":"/docs/cloud/docker/containerize-full-stack-application/par1-backend-service/","title":"Part1: Backend Service","section":"Containerize Full Stack Application","content":" 🧱 Project Part 1: Set Up Backend API with FastAPI # 🎯 Goal # In this part of the project, you\u0026rsquo;ll create a FastAPI application, containerize it with a Dockerfile, and orchestrate it using Docker Compose. This backend service will serve as the foundation of your fullstack application.\n🗂️ Project Structure Overview # Start with the following folder structure:\ndocker-fullstack-app/ │ ├── backend/ │ ├── app.py │ ├── requirements.txt │ └── Dockerfile │ ├── docker-compose.yml ✅ Step 1: Create the FastAPI App # Inside the backend/ folder:\napp.py # from fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/\u0026#34;) def read_root(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello from FastAPI\u0026#34;} requirements.txt # fastapi uvicorn ✅ Step 2: Write the Dockerfile # In the backend/ folder:\nDockerfile # FROM python:3.11-slim WORKDIR /app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY . . CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] This Dockerfile installs the dependencies and runs your FastAPI app with Uvicorn.\n✅ Step 3: Add .dockerignore # Still inside the backend/ folder:\n.dockerignore # __pycache__/ *.pyc *.pyo *.log This helps reduce build context size and keeps the image clean.\n✅ Step 4: Define the Docker Compose File # In the root directory docker-fullstack-app/, create the following:\ndocker-compose.yml # version: \u0026#34;3.9\u0026#34; services: backend: build: context: ./backend ports: - \u0026#34;8000:8000\u0026#34; This file defines a backend service built from the Dockerfile and exposes it on port 8000.\n✅ Step 5: Build and Run the Service # From the root project directory:\ndocker-compose up --build Open your browser and go to http://localhost:8000. You should see:\n{\u0026#34;message\u0026#34;:\u0026#34;Hello from FastAPI\u0026#34;} 🔍 Step 6: Inspect the Running Container # To open a shell session inside the running container:\ndocker exec -it docker-fullstack-app-backend-1 sh Check the contents of the app directory:\nls /app You should see your source files (app.py, etc.) inside /app.\n✅ Congratulations! Your backend API is now running inside a container. You\u0026rsquo;re ready to move on to Project Part 2: Add PostgreSQL and Connect to FastAPI.\n"},{"id":29,"href":"/docs/programming-core/python/re/","title":"RegEx","section":"Python","content":" re in Python # re.search() # The re.search() method is used to search a string for a match to a regular expression pattern. It scans through the string from left to right and returns the FIRST match it finds. If a match is found, it returns a match object; otherwise, it returns None.\nMethod Signature # search(pattern: str, string: str, flags: int=0) -\u0026gt; re.Match pattern: The regular expression pattern to search for. string: The string to search within. flags: Optional flags to modify the behavior of the pattern (default is 0). Match Object Methods # If a match is found, re.search() returns a match object. This object provides several useful methods and attributes:\n.group() -\u0026gt; str: Returns the string matched by the regular expression. .start() -\u0026gt; int: Returns the starting position of the match. .end() -\u0026gt; int: Returns the ending position of the match. .span() -\u0026gt; turple(int): Returns a tuple containing the (start, end) positions of the match. Example # import re pattern = r\u0026#34;\\d+\u0026#34; input_string = \u0026#34;There are 123 numbers in 456 text\u0026#34; match = re.search(pattern, input_string) print(f\u0026#34;`re.search()` type -\u0026gt; {type(match)}\u0026#34;) if match: print(f\u0026#34;`match.group()` type -\u0026gt; {type(match.group())}\u0026#34;) print(f\u0026#34;`match.group()` value -\u0026gt; {match.group()}\u0026#34;) print(f\u0026#34;`match.start()` type -\u0026gt; {type(match.start())}\u0026#34;) print(f\u0026#34;`match.start()` value -\u0026gt; {match.start()}\u0026#34;) print(f\u0026#34;`match.span()` type -\u0026gt; {type(match.span())}\u0026#34;) print(f\u0026#34;`match.span()` value -\u0026gt; {match.span()}\u0026#34;) else: pass Output\n`re.search()` type -\u0026gt; \u0026lt;class \u0026#39;re.Match\u0026#39;\u0026gt; `match.group()` type -\u0026gt; \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; `match.group()` value -\u0026gt; 123 `match.start()` type -\u0026gt; \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; `match.start()` value -\u0026gt; 10 `match.span()` type -\u0026gt; \u0026lt;class \u0026#39;tuple\u0026#39;\u0026gt; `match.span()` value -\u0026gt; (10, 13) re.match() # The re.match() method is used to determine if the regular expression pattern matches AT THE BEGINNING of the string. If a match is found at the beginning of the string, it returns a match object. Otherwise, it returns None.\nMethod Signature # match(pattern: str, string: str, flags: int=0) -\u0026gt; re.Match Example # import re pattern = r\u0026#34;word\u0026#34; word_at_beginning = \u0026#34;word is at the beginning of the sentence\u0026#34; word_not_at_beginning = \u0026#34;The word is not at the beginning of the sentence\u0026#34; match = re.match(pattern, word_at_beginning) non_match = re.match(pattern, word_not_at_beginning) if match: print(f\u0026#34;Matching string -\u0026gt; {match.group()}\u0026#34;) else: pass if non_match: pass else: print(f\u0026#34;word is not found at the beginning\u0026#34;) Output\nMatching string -\u0026gt; word word is not found at the beginning re.fullmatch() # The re.fullmatch() method is used to check if the entire string matches a regular expression pattern. It differs from re.match() in that re.fullmatch() requires the whole string to match the pattern, not just a prefix. If the entire string matches the pattern, it returns a match object; otherwise, it returns None\nMethod Signature # fullmatch(pattern: str, string: str, flags: int=0) -\u0026gt; re.Match Example # import re pattern = \u0026#34;word\u0026#34; full_match_string = \u0026#34;word\u0026#34; non_full_match_string = \u0026#34;words\u0026#34; match = re.fullmatch(pattern, full_match_string) non_match = re.fullmatch(pattern, non_full_match_string) if match: print(\u0026#34;Found full match\u0026#34;) else: pass if non_match: pass else: print(\u0026#34;No full match\u0026#34;) Output\nFound full match No full match re.findall() # Method Signature # The re.findall() method is used to find all occurrences of a pattern in a string. It returns a list of all matches found. If no matches are found, it returns an empty list. Unlike re.search() and re.match(), which return match objects, re.findall() directly returns the matched strings.\nfindall(pattern: str, string: str, flags: int=0) -\u0026gt; list(str) Example # import re pattern = r\u0026#34;\\bword\\b\u0026#34; text = \u0026#34;word in a wordy word sentence with words\u0026#34; matches = re.findall(pattern, text) print(f\u0026#34;Type of `re.findall()` -\u0026gt; {type(matches)}\u0026#34;) print(f\u0026#34;Type of item in matches -\u0026gt; {type(matches[0])}\u0026#34;) print(\u0026#34;All matches found:\u0026#34;, matches) Output\nType of `re.findall()` -\u0026gt; \u0026lt;class \u0026#39;list\u0026#39;\u0026gt; Type of item in matches -\u0026gt; \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; All matches found: [\u0026#39;word\u0026#39;, \u0026#39;word\u0026#39;] "},{"id":30,"href":"/docs/distributed-system/design-data-intensive-applications/replication/","title":"Replication","section":"Design Data Intensive Applications","content":" Definition # Replication means keeping a copy of the same data on multiple machines that are connected via a network.\nAdvantages of Replication # Reduce Latency: To keep data geographically close to your users Increase Availability: To allow the system to continue working even if some of its parts have failed Increase Read Throughput: To scale out the number of machines that can serve read queries Principles of Replication # Data Consistency: Each node that stores a copy of the database is called a replica. Every write to the database needs to be processed by every replica; otherwise, the replicas would no longer contain the same data (Inconsistency). Single Leader Mode # The most common solution for Data Consistency is called leader-based replication (also known as active/passive or master–slave replication)\nHow Does It Work? # One of the replicas is designated the leader which takes all write requests\nThe other replicas are known as followers. All data written to the leader should be replicated to all followers in same order\nBoth leader and followers can take read requests\nSynchronous Versus Asynchronous Replication # Synchronous Replication # The Replication to follower 1 is synchronous: the leader waits until follower 1 has confirmed that it received the write before reporting success to the user, and before making the write visible to other clients.\nAdvantage: The follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. When the leader node fails suddenly, we are sure that the latest data is available on the follower Disadvantage: If the synchronous follower doesn\u0026rsquo;t respond for whatever reason, the leader has to block all writes until the follower is available again Semi-Synchronous Replication # It\u0026rsquo;s not realistic to make all followers synchronous because any one node outage would cause the whole system to halt. Sometimes you make one follower synchronous and the others asynchronous. If the synchronous node fails, one of the asynchronous node is made synchronous. That ensures at least two nodes (the leader and the synchronous nodes) have update-to-date data. The configuration is sometimes called semi-synchronous\nAsynchronous Replication # The Replication to follower 2 is asynchronous: the leader sends the message, but doesn’t wait for a response from the follower\nAdvantage: The system can continue to work (allow writes) even though all followers have failed and fallen behind\nDisadvantage: If the leader node fails and is not recoverable, any writes that have not yet replicated to followers are lost. This means that a write is not guaranteed to be durable\nSetting up New Followers # Steps # Take a snapshot of the leader\u0026rsquo;s database at some point in time\nCopy the snapshot to the new follower node\nStep 1 and 2 are not sufficient, because while copying the snapshot, clients are still writing new data to the leader database. When the copy process is complete, the new follower database actually doesn\u0026rsquo;t catch up the leader node. Hence, further steps are needed\nThe follower connects to the leader and requests all the data changes that have happened since the snapshot was taken. This requires that the snapshot is associated with an exact position in the leader’s replication log.\nThe new follower node catches up when the new node has process all backlog of data changes since the snapshot\nHandling Node Outages # High availability system should be running as a whole despite individual node failures or maintenance.\nFollower Failure # On its disk, each follower should keep a log of data changes it has received and processed from the leader. If a follower crashes and is restarted, from its log it knows the last transaction it has processed before the fault happened. Thus, it can connect to the leader and request all data changes that occurred during the time when the follower was disconnected\nLeader Failure: Failover # When the leader node fails,\nOne of the followers need to be promoted to be the new leader Clients should be reconfigured to send writes to the new leader Followers need to consumer data from the new leader Steps # Determine that the leader has failed\nCommon method is to use a timeout: nodes bounces messages back and forth between each other. If a node doesn\u0026rsquo;t respond for some period of time \u0026ndash; for example, 30 seconds \u0026ndash; it is assumed to be dead\nChoosing a new leader\nThe best candidate for the leadership is the replica with most up-to-date data changes from the old leader to minimize any data loss\nReconfiguring the system to use the new leader\nClients now need to send writes to the new leader The system needs to ensure that the old leader to be a follower and recognize the new leader when it is recovered Things can go wrong # If asynchronous replication is used, new leader may not have all writes from the old leader before it failed. If the old leader rejoins, what should happen to those writes? It is hard to merge them into new leader because it may have received and processed conflicting writes. For example, new entries whose key was auto-incremented were inserted to the old leader database, and insertions were not received by the new leader, so when new entries are inserted to the new leader database, same keys would be used. The most common solution is to discard the conflicting and not synchronized data changes which compromises system durability, and it is dangerous.\nSplit brain: two nodes both believe that they are the leader. If both nodes accept writes and there is no process to resolve conflict, data is likely to be lost or corrupted\nTimeout trade-offs: A longer timeout means a longer time to recover in case the leader fails. Too short timeout may cause unnecessary failover. A temporary load spike or a network glitch could cause a node\u0026rsquo;s slow response\nImplementation of Replication Logs # Statement-Based Replication # The leader logs every write request (statement) that it executes and sends that statement log to its followers. INSERT, UPDATE and DELETE statements for a relational database\nProblem: Nondeterministic functions, like NOW() or RAND() are likely to generate different values in each replica Write-Ahead Log (WAL) shipping # Storage engines usually append every write to a log. For example in B-Trees, every modification is written to a write-ahead log (WAL). The database then applies the operation from WAL to the B-Tree structure database. That ensures durability: changes are recoverable in case of a crash\nThe log is an append-only sequence of bytes containing all writes to the database. Besides writing the log to disk, the leader can also send the log to followers. When followers apply the log, it builds a copy of exact data structure as found on the leader\nShortage: Backward compatible\nFor WAL, the storage engine needs an internal conversion from the log to the actual modification of the database. The conversion process may not be backward compatible, in other words, different versions of storage engine may convert same log to different data changes on the database. If leader and followers are using different versions of software after maintenance, that may cause problem\nLogical (row-based) log replication # Logical log represents the data change of a write operation. Use relational database as example:\nFor an inserted row: the log contains new values of all columns For a deleted row: the log contains all necessary information \u0026ndash; for example, primary key \u0026ndash; to identify the deleted row For an updated row: the log contains enough information to identify the updated row, and new values of all columns or columns that have been modified A transaction that modifies several rows generates several such log records, followed by a record indicating that the transaction was committed.\nLogical log contains information of data that has been changed, so this technique is called change data capture. The advantage of logical log is that it is decoupled from the storage engine, so it\u0026rsquo;s more easy to keep it backward compatible. Also, it is easy for external applications to parse\nReplication Lag # Replication lag is the delay between a write happening on the leader and being reflected on a follower\nProblem of Replication Lag # Leader based replication allows all writes go through a single node and clients can read from any nodes in the system. If workloads in your application consist of mostly reads and only small percentage of write, then this is an attractive option. You can simply add more followers to server more reads (Scalability) w/o increasing latency.\nIn that case, it is not realistic to use synchronous replication because any node of failure can make the entire system unavailable for writing. Hence, the majority of followers should be asynchronous. That brings another problem, some followers that have fallen behind the leader (large replication lag) would respond inconsistent values to clients. If you stop writes, after some time, those followers would catch up leader. That is called eventually consistent.\nSolution 1: Read Your Writes # This is a guarantee that user would always see any updates they submitted, but it doesn\u0026rsquo;t make any promise about others\nHow to achieve?\nRead from leader for data that can only be modified by the user, like user\u0026rsquo;s profile, or user\u0026rsquo;s posts. If most things in the application are potentially modified by the user, that approach won\u0026rsquo;t work\nThe client can remember the timestamp for its most recent write. If the replica is not up-to-date, the query can be handled by another replica or wait until the replica has caught up. The timestamp could be logical, like log sequence number or version of the queried entry\nProblem of multiple devices read-your-write\nApproaches that require remembering the timestamp are more difficulty because one device doesn\u0026rsquo;t know the updates made on other devices. Some possible solutions:\nServer pushes timestamps of updates from any devices to all other connected devices Solution 2: Monotonic Reads # Monotonic reads is a guarantee that below kind of anomaly does not happen. It’s a lesser guarantee than strong consistency, but a stronger guarantee than eventual consistency.\nAnomaly\nUser can read things moving backward in time. That can happen when user first read from replica with little lag and then read from replica with greater lag\nThe first query returns a comment that was recently added by user 1234, but the second query doesn’t return anything because the lagging follower has not yet picked up that write\nAchieving monotonic reads\nOne possible way to achieve monotonic reads is to make sure all reads from one user are made to same replica (consistent hashing)\nMultiple Leader Replication # Multiple-leader replication allows more than one node to accept writes. A node that processes a write must forward the data change to ALL other nodes (Each leader in this model is simultaneously follower of other leaders)\nUse Cases for Multi-Leader Replication # Multi-datacenter operation # Within each datacenter, regular leader–follower replication is used; between datacenters, each datacenter’s leader replicates its changes to the leaders in other datacenters.\nComparison between single-leader and multiple-leader configuration in a multiple-datacenter deployment\nPerformance\nIn a single-leader configuration, every write must go over the internet to the datacenter with the leader. That can add significant latency to writes from other datacenters\nIn a multiple-leader configuration, every write can be processed in the local datacenter and asynchronously replicated to other data centers. The inner-datacenter delay is hidden from users\nTolerance of datacenter outage\nIn a single-leader configuration, if the datacenter with the leader fails, failover can promote follower from other datacenters to be the new leader\nIn a multiple-leader configuration, every datacenter works independently. If one datacenter fails, traffic can be handled by other datacenters\nTolerance of network outage\nWrites from other datacenters need to go through the public network and to be handled by the leader synchronously. Thus, A single-leader configuration is very sensitive to the problem of inter-datacenter network, which may be less reliable than local datacenter network\nA multiple-leader configuration with asynchronous replication tolerates network problem better. A temporary network issue won\u0026rsquo;t affect writes to be processed\nClients with Offline Operations # Assume you have a calendar app on you mobile phone, you are allowed to add meetings even though the app is offline. Not only on the mobile phone, you can also make changes in the calendar app on your laptop. When the app is online, the changes that you made should be synced to other devices\nAt an architectural point of view. The setup is same as multiple-leader configuration. The device is a \u0026ldquo;datacenter\u0026rdquo;, and the network between datacenters is not reliable (devices could be offline). Clients can still write to the datacenter when network outage happens\nCollaborative Editing # Real-time collaborative editing applications, like Google Docs, allow users to update same document simultaneously. It\u0026rsquo;s not a real multiple-leader datacenter problem, but they have a lot of commons\nOne way to achieve collaborative editing is to lock the document when any user is editing the file. Other users can only edit the file when previous user commits the change and the lock is released. This setup is like one-leader configuration, and only one client can write the database simultaneously\nFor multiple-leader configuration like setup, the application allows multiple users to edit simultaneously\nHandling Write Conflicts # The biggest problem with multi-leader replication is that write conflicts. For example, a document is edited by two users simultaneously on Google Drive. User one changes the title from A to B, and user two changes the title from A to C. Each user\u0026rsquo;s changes is processed by its local leader. However, when the changes are asynchronously replicated, a conflict is detected\nApproach 1: Synchronous Versus Asynchronous Conflict Detection # You can configure replication between datacenters synchronous \u0026ndash; i.e., wait for the write to be replicated to all replicas before telling the user the write was successful. However, by doing so, you will lose the main advantage of multiple-leader replication: allow each replica to accept writes independently\nApproach 2: Conflict Avoidance # You can force all writes to a record are processed by one leader, so the write conflict won\u0026rsquo;t occur.\nFor example, in applications that user can edit their own data, like user profiles, you can ensure that requests from a specific user are always routed to the leader in same datacenter for reads and writes. From user\u0026rsquo;s perspective, the configuration is single-leader\nProblems\nThis approach won\u0026rsquo;t work if the datacenter accepting writes fails and all requests are re-routed to other datacenters, or user has moved to a location which is closer to another datacenter\nApproach 3: Converging to A Consistent State # We don\u0026rsquo;t worry about wirte conflicts in single-leader database because all writes are in sequential order because of write lock. When multiple writes are replicated to followers, the last write should have the final value. This doesn\u0026rsquo;t apply to multiple-leader database because writes are made concurrently and independently in different datacenters. Thus, we need an approach to determine the convergent value when write conflicts happen\nGive each write a version ID, and write with the highest version wins\nPrioritize replica, write from replica with higher priority wins\nProblems\nBoth approaches may cause data loss\nMultiple-Leader Topologies # A replication topology describes the communication paths along which writes are propagated from one node to another\nCircular topology: Each node receives writes from one node and forwards those writes (plus any writes of its own) to one other node.\nPreventing infinite loop: Every write should be tagged with the identifier of all nodes that it has passed through Problem: A single node failure can interrupt the flow of replication. For example, the write flow is A -\u0026gt; B -\u0026gt; C -\u0026gt; D -\u0026gt; A, if node C fails, then writes from node A and B can\u0026rsquo;t be propagated to node D Star topology: A designated root node accepts write replication from other leaders and forwards the writes to all other leaders\nSingle point of failure: Root node failure would interrupt the replication to all other leaders All-to-all topology: Every leader sends it writes to all other leaders\nReverse causal order: Some network links are faster than others, write replication 2 that depends on write replication 1 may arrive at one node earlier Client A inserts a row into a table on leader 1, and client B updates that row on leader 3. However, leader 2 may receive the update first, which doesn\u0026rsquo;t make sense because it\u0026rsquo;s an update on a row that doesn\u0026rsquo;t exist in the database\nLeaderless Replication # Storage system that abandons the concept of a leader and allows any replica to directly accept writes from clients\nWriting to The Database When One Node Is Down # Imagine you have a database with three replicas, and one node is down. There is no failover in leaderless database because of no leader node.\nWrite\nLet\u0026rsquo;s see what will happen when one node is down. Client sends the write to all replicas in parallel, and two nodes accept the write and one node that is down misses it. It\u0026rsquo;s sufficient for two out of three replicas to acknowledge the write. After client receives two ok responses, we consider the write is successful.\nRead\nNow imagine that the unavailable node comes back online, and clients start reading from it. Any writes that happened while the node was down are missing from that node. To solve this problem, when client reads, it sends the request to several replicas. Client will receive the up-to-date data from one node and stale data from another. Version number will determine which value is newer.\nSolutions of Replication Lag # Read repair\nWhen a client makes a read request to several nodes in parallel, the client detects the stale data and write new value to the replica with stale data. This approach works well for data that is frequently read\nAnti-entropy process\nIn addition, some datastores have a background process that constantly looks for differences in the data between replicas and copies any missing data from one replica to another.\nQuorums for reading and writing # In the example (writing to the database when one node is down), if we know that every successful write is guaranteed to be present on at least two out of three replicas, that means at most one replica can be stale. Thus, if we read from at least two replicas, we can be sure that at least one of the two is up-to-date. If the third replica is down or slow to respond, reads can nevertheless continue returning an up-to-date value.\nGeneral condition\nIf there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. (In our example, n = 3, w = 2, r = 2.) As long as w + r \u0026gt; n, we expect to get an up-to-date value when reading, because at least one of the r nodes we’re reading from must be up-to-date. Reads and writes that obey these r and w values are called quorum reads and writes\nNormally, reads and writes are always sent to all n replicas in parallel. The parameters w and r determine how many nodes we wait for—i.e., how many of the n nodes need to report success before we consider the read or write to be successful.\nLimitations\nWhen the quorum condition is fulfilled, you always expect to read the most up-to-date data. That is the case because the set of nodes to which you have written and the set of node from which you have read must overlap. However, even with w + r \u0026gt; n, there are likely to be edge cases where stale values are returned.\nSloppy quorum\nConcurrent read and write, the new value haven\u0026rsquo;t been reflected on some replicas\nConsider a system with 5 replicas, with a write quorum of 3 and a read quorum of 3. Imagine a scenario where a write operation updates replicas 1, 2 and 3, and a read operation queries replica 3, 4, and 5. If replica 3 has not yet finished updating while it is queried, it might still return the stale value\nIf a node carrying a new value fails, and its data is restored from a replica carrying an old value, the number of replicas carrying the new value may fall below w\nA write succeeded on fewer than w nodes, and the write is not rolled back on those succeeded replicas. That means the write is reported as failed, but the consequent read may return the new value (Old value is supposed to be returned because the new value is from a failed write)\nSloppy Quorum \u0026amp; Hinted Handoff # For some quorum configurations, there are some designated nodes to accept writes for a specific key. If some designated node fail, sloppy quorum allows other nodes to accept those writes and respond success to clients. When the failed designated nodes are available, values updated will be transferred from sloppy replicas to the designated replicas. This is call *hinted handoff\nAlthough condition w + r \u0026gt; n is still fulfilled in sloppy quorum, in some cases, stale values will be returned to clients. For example, a read operation happens before hinted handoff completes, and it contacts a set of nodes that don\u0026rsquo;t have the new value. Thus, sloppy quorum achieves high availability, which allows client to write when some designated nodes fail, by compromising consistency (eventual consistency)\nConcurrent Writes # Two clients update the same data with different values concurrently (write #1 and write #2). One node receives two writes in order write #1 and write #2, but another node receives same writes in reverse order. If each node just overwrites the data with latest write, then values of same data in different nodes are inconsistent. In order to become eventually consistent, the replicas should converge to the same value.\nLast write wins (discarding concurrent write)\nOne approach is to declare that each replica only store the most \u0026ldquo;recent\u0026rdquo; value and allow the \u0026ldquo;older\u0026rdquo; values to be overwritten and discarded. Then we need a way of unambiguously determining which write is more \u0026ldquo;recent\u0026rdquo;. We can use timestamp, as long as the two writes don\u0026rsquo;t happen at exactly same time, we can determine which one is more \u0026ldquo;recent\u0026rdquo; even with \u0026ldquo;0.000001s\u0026rdquo; difference in time. However, order in concurrently writes makes no sense, because the two writes don\u0026rsquo;t know and depend on each other.\nFor example, imagine one item is in the cart of someone\u0026rsquo;s Amazon account. One person increments the number of the item by 1, and at same time another person removes the item from the cart in another device. Two write operations update the item concurrently, and both don\u0026rsquo;t know each other. Then which one should win, 0 or 2? Use timestamp can achieve eventual consistency, but may confuse clients because both writes responded success, but one write is implicitly discarded\n"},{"id":31,"href":"/docs/web/web-securities/session-vs-token/","title":"Session vs Token","section":"Web Securities","content":" Introduction # Both sessions and tokens are used for user authentication and maintaining user state across multiple HTTP requests in a web application.\nProcess # Session-Based Authentication Process # 1-2: User Login Attempt\nThe user submits their login credentials (usually username and password) through the client (e.g., a web browser). 3-4: Credentials Verification\nThe server receives the credentials and verifies them against its user database or authentication source. 5-6: Session Creation\nUpon successful verification, the server creates a new session. This session is stored in the server\u0026rsquo;s memory or a session store (like a database). 7: Sending Session ID to Client\nThe server sends this session ID back to the client, typically as a cookie. This cookie is stored in the user\u0026rsquo;s browser. Client Stores Session ID\nThe browser stores this session ID and sends it along with every subsequent request to the server. Server Session Validation\nFor each new request from the client, the server reads the session ID from the cookie, looks up the corresponding session in its session store, and validates it. Token-Based Authentication Process (Using JWT) # 1-2: User Login Attempt\nThe user submits their login credentials (usually username and password) through the client (e.g., a web browser). 3-4: Credentials Verification\nThe server receives the credentials and verifies them against its user database or authentication source. Token Generation\nUpon successful verification, the server generates a token (like a JWT). This token includes encoded user information and is digitally signed by the server. 5: Server sends Token to Client\n6: Client Stores Token\nToken Sent with Requests\nFor every subsequent request, the client attaches this token, typically in the HTTP Authorization header. Server Token Validation\nThe server validates the token on each request. This involves verifying the token’s integrity and possibly checking against a list of revoked tokens. Comparison # Session-Based Authentication # User Information Storage:\nIn session-based authentication, user information is stored on the server. This can include user identity, authentication status, user preferences, and other session-related data. Server Memory and Resources:\nSince the data is stored server-side, it consumes server memory and resources. This can be significant, especially with a large number of concurrent users. Client-Server Interaction:\nDuring subsequent requests, the client sends this session ID back to the server. The server uses this ID to retrieve the user\u0026rsquo;s session information and authenticate the request. Security:\nThe actual user data is not exposed to the client, which can be a security advantage. However, the session ID needs to be protected to prevent session hijacking. Scalability Concerns:\nStoring session data for each user can impact the scalability of the application, especially in distributed systems where session data might need to be shared across servers. Token-Based Authentication (e.g., JWT) # User Information Storage:\nIn token-based systems like JWT, user authentication information is encoded and optionally encrypted within the token itself. This can include user ID, roles, permissions, and other claims. Client-Side Storage:\nThe token is stored on the client side, typically in the browser\u0026rsquo;s local storage, session storage, or as an HTTP-only cookie. Stateless Operation:\nEach request from the client includes the token, allowing the server to authenticate the request statelessly, without needing to store user session data. Security:\nWhile the user information in the token can be read by the client, sensitive data should never be stored in a token payload unless encrypted. The token is also susceptible to different types of attacks (like XSS), and thus must be handled securely. Scalability and Performance:\nSince the server does not store session state, token-based authentication can be more scalable, reducing the load and memory requirements on the server. "},{"id":32,"href":"/docs/cloud/aws/message/sns/","title":"SNS","section":"Message","content":" Architecture # Pub-Sub Model # Publisher # Service or application that sends message to SNS Topic. SNS Topic allows multiple message publisher\nSNS Topic # SNS uses topics to logically separate messages into channels\nFanout The Fanout scenario is when a message published to an SNS topic is replicated and pushed to multiple endpoints, such as Kinesis Data Firehose delivery streams, Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing.\nFilter Policy After subscribing to an SNS topic, subscribers will receive all messages published by the topic. To receive a subset of messages, a subscriber must assign a filter policy to the topic subscription.\nRetry Policy Retry Policy is a JSON Object which allows SNS Topic to re-publish message to subscribers when message delivery fails\nDLQ You can set up an SQS DLQ (Dead Letter Queue) to receive messages failed to be delivered after all retries\nResource Policy It\u0026rsquo;s a policy attached to SNS Topic which defines WHO (Primary) can do WHAT (Actions) on WHICH resource\nSubscriber # To receive messages from an SNS topic, applications or receives should subscribe to the SNS topic. After subscription, every message sent to the SNS topic will be replicated and pushed to all subscribers\nSNS V.S. SQS # Push V.S. Pull # SNS topics push message to subscribers. SQS queues retain messages, and consumers need to pull messages from the queue\nMessage Retention # An SQS message is stored on the queue for up to 14 days until it is successfully processed by a consumer. SNS does not retain messages so if there are no subscribers for a topic, the message is discarded.\nMessage Duplication # SNS topics duplicate messages and fan out them to all subscribers. Although SQS queues can have multiple consumers, but one message is processed by only one consumer. The message goes back to the queue if consumers doesn\u0026rsquo;t delete the message before invisible timeout runs out\nMessage Processing # SNS only concerns with message delivery. Apart from message delivery, SQS also cares if the message is processed properly by the consumer. The messages are deleted by the consumer after message processing is finished\n"},{"id":33,"href":"/docs/cloud/aws/message/sqs/","title":"SQS","section":"Message","content":" Architecture of SQS System # Three Main Components # Producers\nProducers in above system are the identities who send message to SQS queue\nSQS Queue\nSQS Queue is a buffer that stores messages and decouples producers and consumers in the system\nConsumers\nConsumers are identities in the system poll message from SQS queue\nHow to Use SQS Queue # Producer sends message\nProducer sens message to SQS Queue, and the message will be visible to all consumers\nMessage state - Visible: The message is able to be polled by any consumers. The message is in visible state when Message is sent to the queue and not pulled by any consumers Message is not deleted before the visibility timeout runs out Visibility timeout: The time that a message stays in the queue but is hidden to all consumers. Message is supposed to be polled and processed by only one consumer at a time. Hence, once a consumer polls the message, it should not be visible to all consumers Consumer polls message\nConsumer polls messages (up to 10 in one batch) from the queue and processes the messages. The message will be invisible until 1. Consumer who polls the message completes message processing and deletes the message 2. Or consumer doesn\u0026rsquo;t delete the message before visibility timeout runs out, and the message becomes visible again\nMessage state - Invisible: The message can\u0026rsquo;t be polled by any consumers. Consumer deletes message\nConsumer completes message processing and deletes the message from the queue\nSQS Encryption # SQS Access Policy # Policy that is attached to a SQS queue, defines WHO (the identity) can perform WHAT actions (APIs that are allowed) on this SQS queue\nCross Account Access Policy # { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;Account-B-ID\u0026gt;:root\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;SQS:SendMessage\u0026#34;, \u0026#34;SQS:ReceiveMessage\u0026#34;, \u0026#34;SQS:DeleteMessage\u0026#34;, \u0026#34;SQS:GetQueueAttributes\u0026#34;, \u0026#34;SQS:GetQueueUrl\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:\u0026lt;region\u0026gt;:\u0026lt;Account-A-ID\u0026gt;:\u0026lt;queue-name\u0026gt;\u0026#34; } ] } The access policy allows Account-B to access SQS queue in Account-A SQS Standard Queue # Attributes # Unlimited throughput and number of messages in queue Default retention of message: 4 days. Max: 14 days Low latency Limitation of 256 KB per message Can have duplicate messages Could be out of order DLQ (Dead Letter Queue) # If consumer doesn\u0026rsquo;t delete the message from the queue before visibility timeout runs out, the message will be visible again and receive count for that message increments.\nWe can set up a threshold of how many times a message could go back to the queue. If MaximumReceives threshold is exceeded, the message goes into a dead letter queue\nDLQ is mainly for debugging purpose. It also supports to re-drive failing messages which sends messages from DLQ to SQS queue\n"},{"id":34,"href":"/docs/cloud/k8s/start-first-pod/","title":"Start First Pod","section":"K8s","content":" 🧭 Lesson 1: Set Up Kubernetes Locally and Run Your First Pod (macOS Edition) # 🔢 Goal # By the end of this lesson, you\u0026rsquo;ll be able to:\nInstall a local Kubernetes cluster using Minikube on macOS Understand the basic components behind Kubernetes operations Deploy and explore your very first Pod using the command line 🛠️ Step 1: Install Required Tools (macOS) # We’ll install three essential tools:\nDocker: Runs containers locally on your machine. Minikube: Sets up a simple Kubernetes cluster on your laptop. kubectl: A command-line tool to control Kubernetes resources. 1. Install Docker Desktop # Docker provides the container runtime that Kubernetes will use under the hood.\nbrew install --cask docker After installation, open Docker Desktop from your Applications folder and ensure it is running. You should see the Docker whale icon in the macOS menu bar.\n2. Install Minikube # Minikube creates a single-node Kubernetes cluster that runs locally.\nbrew install minikube Verify installation:\nminikube version 3. Install kubectl # kubectl is your main interface to interact with Kubernetes clusters.\nbrew install kubectl Verify installation:\nkubectl version --client 🚀 Step 2: Start Your Kubernetes Cluster # Minikube uses Docker to simulate a cluster with one node. Before continuing, make sure Docker Desktop is running.\nOnce Docker is ready, start your Kubernetes cluster:\nminikube start This command will:\nSpin up a virtual Kubernetes node Launch the control plane (the core of Kubernetes that manages apps) Configure networking so Kubernetes can route traffic to your apps To confirm everything is working:\nkubectl get nodes Expected output:\nNAME STATUS ROLES AGE VERSION minikube Ready control-plane 1m v1.xx.x ✅ Congratulations — you now have a running Kubernetes cluster!\n🏢 Step 3: Run Your First Pod # In Kubernetes, a Pod is the smallest deployable unit. It wraps one or more containers. Let’s create a Pod that runs an nginx web server:\nkubectl run my-first-pod --image=nginx This command tells Kubernetes:\n\u0026ldquo;Create a Pod named my-first-pod\u0026rdquo; \u0026ldquo;Use the official nginx container image\u0026rdquo; To verify that your Pod is running:\nkubectl get pods Example output:\nNAME READY STATUS RESTARTS AGE my-first-pod 1/1 Running 0 20s 🔎 Step 4: Explore the Pod # Let’s dig into the Pod to see what’s happening inside.\nView Pod Details # This command provides detailed info about the Pod:\nkubectl describe pod my-first-pod It includes:\nWhat node it\u0026rsquo;s running on Which container image is used Event logs like when the image was pulled and started Open a Shell Inside the Pod # To really understand what’s running inside your Pod, you can open a shell session into its container. Think of it like opening a terminal on a remote machine — you can inspect files, check logs, or run commands.\nkubectl exec -it my-first-pod -- /bin/bash Once inside, try a few basic commands:\nls # Lists files in the container curl localhost # Tests if the nginx server is running exit # Exits the shell and returns to your local terminal ❌ Step 5: Clean Up (Optional) # When you\u0026rsquo;re finished, it\u0026rsquo;s a good idea to clean up your environment:\nDelete the Pod:\nkubectl delete pod my-first-pod Stop the Minikube cluster:\nminikube stop Keeping your system clean will make future exercises easier to manage.\n🔄 Recap # In this lesson, you:\nInstalled Docker, Minikube, and kubectl on macOS Started a local Kubernetes cluster Created and explored your first Kubernetes Pod Opened a shell inside the container to see it in action These are foundational steps that will support everything you do in Kubernetes going forward.\n🎉 Next Up # In Lesson 2, you\u0026rsquo;ll:\nWrite your first Kubernetes YAML configuration Use a Deployment to manage Pods declaratively Expose your application to the outside world with a Service You\u0026rsquo;re off to a great start — let\u0026rsquo;s keep going!\n"},{"id":35,"href":"/docs/programming-core/java/thread/thread-lifecycle/","title":"Thread Lifecycle","section":"Thread","content":" Six Java Thread States # NEW State:\nWhen you create an instance of a Thread class (or a class that extends Thread), the thread is in the New state. At this point, the thread is not yet running. Example: Thread t = new Thread(); RUNNABLE State:\nWhen you invoke the start() method, the thread moves to the RUNNABLE state In a typical implementation, a Java thread in the RUNNABLE state corresponds to an OS thread that is eligible for running A RUNNABLE state means the thread is either running on the CPU or waiting for Scheduler to allocate CPU resource BLOCKED State:\nA thread enters the BLOCKED state when it tries to acquire a monitor lock (intrinsic lock) of an object but another thread already holds that lock. This commonly occurs in synchronized blocks or methods. WAITING State:\nA thread is in the WAITING state when it is waiting for another thread to perform a specific action. Unlike the BLOCKED state, where a thread is waiting to acquire a lock, in the WAITING state, a thread is waiting for another thread\u0026rsquo;s action without any lock competition How WAITING state occurs Object.wait() WITHOUT timeout Thread.join() WITHOUT timeout TIMED_WAITING State:\nA thread is in this state when it is waiting for another thread to perform an action for up to a specified waiting time. After the time expires, the thread will automatically return to the RUNNABLE state if it\u0026rsquo;s not blocked by other means How TIMED_WAITING state occurs Thread.sleep(long millis) Object.wait WITH timeout Thread.join WITH timeout TERMINATED State:\nA thread enters the TERMINATED state when it completes the execution of its run() method or if an exception occurs. Once terminated, a thread cannot be restarted. State Transition # Demo # Code\n@Slf4j public class SixStates { public static void main(String[] args) { Thread newThread = new Thread(() -\u0026gt; { log.debug(\u0026#34;Start a new thread\u0026#34;); }, \u0026#34;NEW-thread\u0026#34;); Thread runnableThread = new Thread(() -\u0026gt; { while(true) { } }, \u0026#34;RUNNABLE-thread\u0026#34;); runnableThread.start(); Thread timedWaitingThread = new Thread(() -\u0026gt; { try { Thread.sleep(1_000_000); } catch (InterruptedException e) { throw new RuntimeException(e); } }, \u0026#34;TIMED_WAITING-thread\u0026#34;); timedWaitingThread.start(); Thread waitingThread = new Thread(() -\u0026gt; { try { synchronized (SixStates.class) { runnableThread.join(); } } catch (InterruptedException e) { throw new RuntimeException(e); } }, \u0026#34;WAITING-thread\u0026#34;); waitingThread.start(); Thread blockedThread = new Thread(() -\u0026gt; { synchronized (SixStates.class) { log.debug(\u0026#34;BLOCKED thread\u0026#34;); } }, \u0026#34;BLOCKED-thread\u0026#34;); blockedThread.start(); Thread terminatedThread = new Thread(() -\u0026gt; { }, \u0026#34;TERMINATED-thread\u0026#34;); terminatedThread.start(); log.debug(\u0026#34;{}\u0026#34;, newThread.getState()); log.debug(\u0026#34;{}\u0026#34;, runnableThread.getState()); log.debug(\u0026#34;{}\u0026#34;, timedWaitingThread.getState()); log.debug(\u0026#34;{}\u0026#34;, waitingThread.getState()); log.debug(\u0026#34;{}\u0026#34;, blockedThread.getState()); log.debug(\u0026#34;{}\u0026#34;, terminatedThread.getState()); } } Execution 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - NEW 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - RUNNABLE 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - TIMED_WAITING 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - WAITING 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - BLOCKED 2024-01-07 14:57:44 [main] DEBUG c.w.thread_.lifecycle.SixStates - TERMINATED "},{"id":36,"href":"/docs/cloud/aws/message/eventbridge/","title":"EventBridge","section":"Message","content":" What is AWS EventBridge? # AWS EventBridge is a serverless event bus service that is used to build event driven applications. EventBridge allows you to ingest, filter, transform and deliver events from sources to targets.\nReference # AWS EventBridge Workshop EventBridge Global Endpoint Message Services for Serverless Applications Architecture # Components and Concepts # Data Sources # Data sources or event publishers are applications that publish data to the EventBridge. They could be your own applications, Software-as-a-Service (SaaS) applications and AWS services. Data sources call PutEvent API to publish events to an event bus in the EventBridge.\nAWS services: EventBridge catches all AWS service events, but to utilize them, you need to create a rule to route event of interests to target Event Buses # Event buses receive events. They are data channels in EventBridge, and you can think them as topics in SNS. When applications publish events to EventBridge, it must specify the event bus name.\nRules # A Rule matches incoming events and routes them to targets for processing. A single rule can route matching events to multiple targets. Rules should be attached to an event bus.\nExample # EC2 Event\n{ \u0026#34;version\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;12345678-1234-1234-1234-123456789012\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;EC2 Instance State-change Notification\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;aws.ec2\u0026#34;, \u0026#34;account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2024-02-10T21:22:00Z\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-west-2\u0026#34;, \u0026#34;resources\u0026#34;: [ \u0026#34;arn:aws:ec2:us-west-2:123456789012:instance/i-1234567890abcdef0\u0026#34; ], \u0026#34;detail\u0026#34;: { \u0026#34;instance-id\u0026#34;: \u0026#34;i-1234567890abcdef0\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;running\u0026#34; } } Rule to SQS Queue\nEC2StateChangeRule: Type: AWS::Events::Rule Properties: Description: \u0026#34;EventBridge rule to route EC2 state change events to SQS with transformation\u0026#34; EventBusName: default EventPattern: source: - \u0026#34;aws.ec2\u0026#34; detail-type: - \u0026#34;EC2 Instance State-change Notification\u0026#34; detail: state: - \u0026#34;pending\u0026#34; - \u0026#34;running\u0026#34; - \u0026#34;stopped\u0026#34; - \u0026#34;terminated\u0026#34; Targets: - Id: \u0026#34;TargetSQS\u0026#34; Arn: !GetAtt MySQSQueue.Arn InputTransformer: InputPathsMap: instanceId: \u0026#34;$.detail.instance-id\u0026#34; state: \u0026#34;$.detail.state\u0026#34; InputTemplate: \u0026#39;{\u0026#34;instanceId\u0026#34;: \u0026lt;instanceId\u0026gt;, \u0026#34;state\u0026#34;: \u0026lt;state\u0026gt;}\u0026#39; Important Features # Event Pattern # Only events matching the event pattern will be delivered to targets\nInput Transformer # Input transformer is a feature that can transform source event to the format you desire and pass the transformed event to target\nArchive and Replay # Archive events. Replay allows you to resend events from archive to event bus\nTo create an archive for an event bus, here are some key attributes you need to specify\nEvent Source: Event bus that you want to archive events Retention Period: Time that events will be retained in archive Event Pattern: Only events matching the event pattern will be archived Schema Registry # EventBridge can infer schemas based on events routed through an event bus by using schema discovery. This can be used to generate code bindings directly to your IDE for type-safe languages like Python, Java, and TypeScript. This can help accelerate development by automating the generation of classes and code directly from events.\nGlobal Endpoint # Global Endpoint allows you to publish events to event bus in secondary region when the event bus in primary region is down. It improves availability and reliability of your event-driven applications.\nServices Involved # Route53 Health check event bus in primary region and emit health metrics to CloudWatch Route PutEvent requests to event bus in primary or secondary region CloudWatch Alarm Once health check alarm is over threshold, the alarm notifies Route53 that primary event bus is down Event buses When primary event bus is healthy, event is routed to primary event bus and replicated to secondary event bus and vise verse when primary event bus is down How does it work # Publisher calls PutEvent to send events to EventBridge global endpoint hosted in Route53 The Event is routed to primary event bus and optionally replicated to secondary event bus in real time depending on your setup Once health check metrics are over threshold in CloudWatch alarm, route53 is notified, and it starts failover which routes all events to secondary event bus until the alarm is inactive. You have multiple options when events are routed to secondary event bus Implement same process solution to process the event Archive all events routed to secondary event bus and replay them to primary event bus once it is healthy again Loss of Global Endpoint # More costs Event duplication All fields are same for original and replicated events except event id. Consumers should be designed with idempotency (de-duplication). SNS v.s. EventBridge # Commons # They are important components in event driver application to decouple data sources and data consumers Push data to consumers Unlike SQS queues, they don\u0026rsquo;t retain data They can have multiple data sources and consumers Support retry and DLQ Similar components SNS Topic - Event Bus Subscription with filter policy - Rule Differences # EventBridge is integrated with SaaS which allows them to put events directly to event buses EventBridge allows schema registry "},{"id":37,"href":"/docs/programming-core/python/format-string/","title":"format-string","section":"Python","content":" Format String in Python # f-strings # f-strings provide a concise and readable way to embed expressions inside string literals. They are prefixed with the letter f or F and use curly braces {} as placeholder for expressions\nQuick Start\nname = \u0026#34;Alice\u0026#34; age = 30 greeting = f\u0026#34;Hello, my name is {name} and I am {age} years old\u0026#34; print(greeting) Output\nHello, my name is Alice and I am 30 years old Usage # Expressions\nresult = f\u0026#34;The sum of 2 and 3 is {2 + 3}.\u0026#34; print(result) Output\nThe sum of 2 and 3 is 5. Functions\ndef greet(name): return f\u0026#34;Hello, {name}!\u0026#34; message = f\u0026#34;{greet(\u0026#39;Alice\u0026#39;)}\u0026#34; print(message) Output\nHello, Alice! .format() # .format() is a method of str. It uses curly braces {} as placeholders within the string, which are replaced by the values passed to the format method.\nQuick View\nname = \u0026#34;Alice\u0026#34; age = 30 greeting = \u0026#34;Hello, my name is {} and I am {} years old.\u0026#34;.format(name, age) print(greeting) Output\nHello, my name is Alice and I am 30 years old. Usages # Positional Arguments: Values are inserted into the placeholders in the order they appear. Values can be used more than once.\nstring = \u0026#34;{0} is {1} years old. {0} wants to eat banana\u0026#34;.format(\u0026#34;Tom\u0026#34;, 30) print(string) Output\nTom is 30 years old. Tom wants to eat banana Named Arguments: You can use named placeholders and pass values using keyword. Values can be reused also.\nstring = \u0026#34;{name} is {age} years old. {name} wants to eat banana\u0026#34;.format(name=\u0026#34;Tom\u0026#34;, age=30) print(string) Output\nTom is 30 years old. Tom wants to eat banana Modifiers # Modifiers in Python string formatting allow you to control the way your data is presented in strings. Both f-strings and the .format() method support a variety of modifiers for formatting numbers, strings, dates, and more.\nCommon Modifiers\nWidth and Alignment Precision Type Specifiers Width and Alignment # These modifiers control the width of the field and the alignment of the content within the field.\nSyntax\n{value:width} # width syntax {value:\u0026lt;} # left alignment {value:\u0026gt;} # right alignment {value:^} # center alignment Example\nAlignment modifier :\u0026lt; need to be used before width :width\nprint(f\u0026#34;{\u0026#39;Alice\u0026#39;:\u0026lt;10}\u0026#34;) # \u0026#34;Alice \u0026#34; print(f\u0026#34;{\u0026#39;Alice\u0026#39;:\u0026gt;10}\u0026#34;) # \u0026#34; Alice\u0026#34; print(f\u0026#34;{\u0026#39;Alice\u0026#39;:^10}\u0026#34;) # \u0026#34; Alice \u0026#34; Precision # Precision is mainly used with floating-point numbers to control the number of digits after the decimal point.\nSyntax\n{value:precisionf} **Example\nprint(f\u0026#34;{3:.2f}\u0026#34;) # \u0026#34;3.00\u0026#34; print(\u0026#34;{:.2f}\u0026#34;.format(3)) # \u0026#34;3.00\u0026#34; # Combined with width and alignment print(f\u0026#34;{3:\u0026lt;10.2f}\u0026#34;) # \u0026#34;3.00 \u0026#34; print(f\u0026#34;{3:\u0026gt;10.2f}\u0026#34;) # \u0026#34; 3.00\u0026#34; Type Specifier # Type specifiers allow you to format numbers in different bases (binary, octal, hexadecimal), percentages, and more.\nExample\n# Integer types number = 255 print(f\u0026#34;{number:b}\u0026#34;) # Binary: \u0026#34;11111111 print(f\u0026#34;{number:o}\u0026#34;) # Octal: \u0026#34;377\u0026#34; print(f\u0026#34;{number:x}\u0026#34;) # Hexadecimal: \u0026#34;ff\u0026#34; print(\u0026#34;{:b}\u0026#34;.format(number)) # Binary: \u0026#34;11111111\u0026#34; print(\u0026#34;{:o}\u0026#34;.format(number)) # Octal: \u0026#34;377\u0026#34; print(\u0026#34;{:x}\u0026#34;.format(number)) # Hexadecimal: \u0026#34;ff\u0026#34; # Percentage percentage = 0.85 print(f\u0026#34;{percentage:.2%}\u0026#34;) # \u0026#34;85.00%\u0026#34; print(\u0026#34;{:.2%}\u0026#34;.format(percentage)) # \u0026#34;85.00%\u0026#34; "},{"id":38,"href":"/docs/distributed-system/load-balancing/haproxy/load-balancing-algorithm/","title":"HAProxy: Load Balancing Algorithem","section":"Haproxy","content":" HAProxy: TCP Load Balancing Algorithms # 🔄 What Is a Load Balancing Algorithm? # A load balancing algorithm determines how HAProxy selects a backend server when multiple are available and healthy. Each strategy suits different types of traffic and usage patterns.\n⚖️ Common TCP Load Balancing Strategies # 1. roundrobin # balance roundrobin The default method. HAProxy sends connections to each server in turn. Simple, fair, and works well for uniform connection loads. Use Case:\nStateless backend services Evenly distributed traffic 2. leastconn # balance leastconn Chooses the server with the fewest active connections. More dynamic and adaptive. Helps prevent one server from being overwhelmed if others are idle. Use Case:\nServices where connections live longer (e.g., databases, SSH, WebSocket) 3. source # balance source Uses a hash of the client\u0026rsquo;s IP address. Ensures that the same client IP always goes to the same backend (sticky connection). Helps with session persistence even in TCP mode. Use Case:\nApplications needing session affinity but running at Layer 4 (e.g., TCP login sessions, games) "},{"id":39,"href":"/docs/web/http/http-header-part-1/","title":"HTTP Headers Part 1","section":"HTTP","content":" 📘 HTTP Headers Part 1 # HTTP headers carry metadata between the client and server and are essential for interpreting and handling HTTP requests and responses. This lesson introduces the role of headers in HTTP communication and covers the most common and important general-purpose headers.\n🧠 What Are HTTP Headers? # HTTP headers are key-value pairs sent along with HTTP requests and responses. They:\nDescribe the body content (e.g., format, length, encoding) Control behavior related to caching, authentication, and cookies Provide metadata about the request or response (e.g., user agent, language preferences) Headers help servers and clients interpret how to process the body, route requests, and manage security and performance.\n🔐 Common Request Headers # Header Purpose User-Agent Identifies the client making the request (e.g., browser, curl, Postman) Authorization Sends credentials or tokens to authenticate a user or session Accept Specifies which response formats the client can handle (e.g., JSON, XML) Accept-Language Indicates the client\u0026rsquo;s preferred response language (e.g., en-US) Cache-Control Directs how requests and responses should be cached by intermediaries and browsers 📦 Common Response Headers # Header Purpose Content-Type Declares the format of the response body (e.g., JSON, HTML, plain text) Set-Cookie Sends cookies from the server to the client Cache-Control Specifies caching behavior for the response ETag Provides a unique identifier for a resource to support cache validation 🛠️ Hands-On Practice with FastAPI: Inspecting Headers # To see how headers are transmitted and read in FastAPI, let’s build a simple endpoint that returns all the request headers.\n📄 Start Your Server # Create a new subdirectory 03-http-headers and a new Python file start_header.py:\nmkdir 03-http-headers cd 03-http-headers touch start_header.py Paste in the following Python code:\nfrom fastapi import FastAPI, Request app = FastAPI() @app.get(\u0026#34;/headers\u0026#34;) async def read_headers(request: Request): return {\u0026#34;headers\u0026#34;: dict(request.headers)} This endpoint uses request.headers to dynamically access all incoming headers as a dictionary.\nUse Uvicorn to start your FastAPI app:\nuvicorn start_header:app --reload 🧪 Try It Out # Use the following curl command to simulate a request with custom headers in another terminal:\ncurl -H \u0026#34;User-Agent: MyClient/1.0\u0026#34; -H \u0026#34;Accept-Language: en-US\u0026#34; http://localhost:8000/headers Response:\n{ \u0026#34;headers\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost:8000\u0026#34;, \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;user-agent\u0026#34;: \u0026#34;MyClient/1.0\u0026#34;, \u0026#34;accept-language\u0026#34;: \u0026#34;en-US\u0026#34; } } This shows the full set of headers your server received, including custom and default headers.\n"},{"id":40,"href":"/docs/networking/id-address/","title":"IP Address","section":"Networking","content":" 🌐 Understanding IP Addresses: The Identity System of the Internet # Every device on a network needs an identity — a way for other devices to find it and communicate with it. In computer networking, that identity is an IP address.\n📌 What Is an IP Address? # An IP (Internet Protocol) address is a unique number assigned to each device connected to a network. It functions much like a home address — directing data to the right recipient.\nThere are two main versions in use:\nIPv4: A 32-bit address, written as four decimal numbers separated by dots (e.g., 192.168.0.1). IPv6: A 128-bit address, written in hexadecimal and separated by colons (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334). While IPv4 remains the most common, IPv6 adoption is growing due to the exhaustion of IPv4 address space.\n🧱 Structure of an IP Address # An IP address is composed of two parts:\nThe network portion: identifies the larger network segment. The host portion: identifies the specific device within that segment. For example, in the IPv4 address 192.168.1.10/24:\n192.168.1 identifies the network. .10 identifies the host (device). The /24 (in CIDR notation) means the first 24 bits are reserved for the network portion. This separation helps routers and switches direct traffic efficiently.\n🌍 Public vs. Private IP Addresses # Type Purpose Example Ranges Public Internet-facing devices Assigned by ISPs Private Internal local networks 192.168.0.0/16, 10.0.0.0/8, 172.16.0.0/12 Private IP addresses are not routable on the public internet. To enable internet access, routers use NAT (Network Address Translation) to map private IPs to a shared public IP.\n⚙️ Static vs. Dynamic IP Addresses # Static IP Address: Manually configured and does not change over time. Commonly used for servers, printers, and critical infrastructure. Dynamic IP Address: Assigned automatically by a DHCP (Dynamic Host Configuration Protocol) server. These may change over time and are typically used for desktops, laptops, and mobile devices. In most home networks, dynamic IPs are used by default.\n✅ Key Takeaways # Every device on a network is assigned a unique IP address. IPv4 and IPv6 are the two major formats — with IPv6 created to solve the limitations of IPv4. IP addresses are split into network and host portions for routing purposes. Private IPs are used within local networks; public IPs are globally unique and used on the internet. Devices can use either static or dynamic IP addresses, depending on their role. Understanding IP addresses is foundational to everything in networking — from\n"},{"id":41,"href":"/docs/cloud/docker/fundamentals/lesson-2-commands/","title":"Lesson 2: Docker Commands","section":"Fundamentals","content":" 🐳 Docker Mastery Course – Lesson 2 # 💡 Basic Docker Commands # Welcome to Lesson 2 of the Docker Mastery Course! In this session, you\u0026rsquo;ll learn how to start, stop, inspect, and remove containers using real-world examples. We\u0026rsquo;ll also use the Nginx web server to deepen your hands-on understanding of Docker\u0026rsquo;s command-line interface.\n🎯 Goal # Gain confidence using essential Docker commands to manage container lifecycles and work effectively with real containers.\n📚 Concepts # 🔄 1. Lifecycle of a Docker Container # A typical container lifecycle includes the following steps:\nCreate – using docker create or docker run Start – using docker start Run – the container is now live Stop – using docker stop Remove – using docker rm 🔧 2. Common Docker Commands # Command Description docker run Create and start a container docker ps List running containers docker ps -a List all containers (running, stopped, exited) docker stop \u0026lt;id\u0026gt; Stop a running container docker start \u0026lt;id\u0026gt; Start a stopped container docker rm \u0026lt;id\u0026gt; Remove a stopped container docker rm -f \u0026lt;id\u0026gt; Force-remove a running container (stop and delete) docker logs \u0026lt;id\u0026gt; View logs generated by a container docker exec -it \u0026lt;id\u0026gt; bash Open a bash shell inside a container 💡 ps stands for process status — a familiar command in Linux. In Docker, docker ps lists running containers.\n💻 Hands-On Practice # ✅ Step 1: Run an Nginx Web Server # We’ll begin with Nginx — a lightweight, high-performance web server — to explore practical container operations.\ndocker run -d -p 8080:80 --name webserver nginx -d: Run the container in detached (background) mode -p 8080:80: Map port 8080 on your machine to port 80 inside the container --name webserver: Assign the container a name for easy reference nginx: Use the official Nginx image Now open your browser and go to http://localhost:8080 — you should see the Nginx welcome page.\n✅ Step 2: Inspect and Manage the Container # List running containers:\ndocker ps List all containers, including stopped and exited ones:\ndocker ps -a This also displays each container’s status, such as:\nUp 1 minute Exited (0) (stopped successfully) Exited (1) (stopped with an error) Stop the container:\ndocker stop webserver Start the container again:\ndocker start webserver View container logs:\ndocker logs webserver Access an interactive shell session inside the container:\ndocker exec -it webserver bash ✅ Step 3: Remove the Container # To remove a container, it must be stopped first:\ndocker stop webserver docker rm webserver If the container is still running and you want to force-remove it:\ndocker rm -f webserver ⚠️ This forcibly stops and deletes the container. Use with caution.\nVerify the container is gone:\ndocker ps -a 🏁 Checkpoint # By now, you should be able to:\nStart, stop, and restart containers Access containers using bash View container statuses and logs Safely or forcefully remove containers 🎉 Excellent work — you’ve built a strong foundation for working with containers in real-world environments.\n📅 What’s Next? # Lesson 3: Docker Images\nYou’ll learn how Docker images are built and layered, how to pull them from Docker Hub, and how to explore and optimize them.\nStay tuned — you\u0026rsquo;re progressing fast!\n"},{"id":42,"href":"/docs/cloud/docker/containerize-full-stack-application/par2-database/","title":"Part2: Database","section":"Containerize Full Stack Application","content":" 🧱 Project Part 2: Add PostgreSQL and Connect to FastAPI # 🎯 Goal # In this part of the project, you\u0026rsquo;ll add a PostgreSQL database container to your existing setup and connect it to your FastAPI backend. This is a foundational step toward enabling full backend functionality with persistent data storage.\n🗂️ Updated Project Structure # docker-fullstack-app/ │ ├── backend/ │ ├── app.py │ ├── requirements.txt │ └── Dockerfile │ ├── docker-compose.yml ✅ Step 1: Update the FastAPI App to Use PostgreSQL # First, add the PostgreSQL driver to your project dependencies:\nrequirements.txt # fastapi uvicorn psycopg2-binary Next, update app.py to create a database connection and test it through a basic endpoint:\napp.py # from fastapi import FastAPI import psycopg2 import os app = FastAPI() def get_connection(): return psycopg2.connect(os.getenv(\u0026#34;DATABASE_URL\u0026#34;)) @app.get(\u0026#34;/db-check\u0026#34;) def check_db(): conn = get_connection() cur = conn.cursor() cur.execute(\u0026#34;SELECT \u0026#39;Hello from PostgreSQL!\u0026#39;\u0026#34;) result = cur.fetchone() cur.close() conn.close() return {\u0026#34;message\u0026#34;: result[0]} This endpoint verifies that FastAPI can connect to and query the PostgreSQL container.\n✅ Step 2: Add the PostgreSQL Service to docker-compose.yml # Now modify your Compose file to include the PostgreSQL service:\ndocker-compose.yml # version: \u0026#34;3.9\u0026#34; services: backend: build: context: ./backend ports: - \u0026#34;8000:8000\u0026#34; depends_on: - db environment: - DATABASE_URL=postgresql://postgres:postgres@db:5432/postgres db: image: postgres:14 restart: always environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: postgres POSTGRES_DB: postgres volumes: - pgdata:/var/lib/postgresql/data volumes: pgdata: This setup creates a persistent volume for PostgreSQL and allows the backend to access the database through Docker\u0026rsquo;s internal network.\n🔍 How the DATABASE_URL Is Formed # The environment variable DATABASE_URL follows the format:\npostgresql://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;database\u0026gt; For this project:\nusername: postgres — set by POSTGRES_USER password: postgres — set by POSTGRES_PASSWORD host: db — matches the service name defined in the Compose file. Docker Compose creates a shared network where services can access each other using these names. port: 5432 — the default PostgreSQL port database: postgres — set by POSTGRES_DB Resulting in:\npostgresql://postgres:postgres@db:5432/postgres ✅ Step 3: Build and Run the Services # From your project root, run:\ndocker-compose up --build Wait a few seconds for PostgreSQL to fully initialize, then call the API endpoint to test the database connection:\ncurl http://localhost:8000/db-check You should receive:\n{\u0026#34;message\u0026#34;:\u0026#34;Hello from PostgreSQL!\u0026#34;} 🔍 Step 4: Inspect the PostgreSQL Container # To access the PostgreSQL container manually:\ndocker exec -it docker-fullstack-app-db-1 psql -U postgres Inside the PostgreSQL shell, run:\nSELECT now(); This confirms the database is operational and accepting queries.\n✅ Excellent work! Your FastAPI backend is now successfully connected to a PostgreSQL database using Docker Compose. Your backend supports persistent data and is ready for frontend integration. Continue to Project Part 3: Create and Containerize the React Frontend.\n"},{"id":43,"href":"/docs/distributed-system/design-data-intensive-applications/partition/","title":"Partition","section":"Design Data Intensive Applications","content":" Partition # In the context of databases, partitioning refers to the process of dividing a database or its elements into smaller, more manageable pieces called partitions. Each piece of data (each record, row, or document) belongs to an exact partition.\nThe main benefit for partition is scalability. Different partitions can be placed on different nodes. Thus, a large dataset can be distributed across many disks, and the query loads can be distributed across many processors.\nPartitioning and Replication # Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes. This means that, even though each record belongs to exactly one partition, it may still be stored on several nodes for fault tolerance.\nPartitioning of Key-Value Data # Our goal of partitioning is to spread the data and the query load evenly across nodes.\nIf the partitioning is unfair, some partitions would have more data or queries than others. A partition with disproportionately high load is called a hot spot.\nPartitioning by Key Range # One way of partitioning is to assign a continuous range of keys to each partition. If you know the boundaries between the ranges, you can easily determine which partition contains a given key. If you also know which partition is assigned to which node, then you can make your request directly to the appropriate node\nThe ranges of keys are not necessarily evenly spaced, because your data may not be evenly distributed. In order to distribute the data evenly, the partition boundaries need to adapt to the data\nEven Distribution V.S. Fast Query # When partitioning a dataset, there is often a trade-off between achieving an even distribution of data and ensuring fast query performance.\nFast Query\nRange Scans: Key range partitioning allows for efficient range scans. For instance, storing sensor data with timestamps as keys means that measurements within a specific time period are stored in contiguous partitions. Example: Queries for data within a month can be quickly executed as the relevant data is stored in a few partitions due to the sorted order of timestamps. Even Distribution\nHot Spots: Key range partitioning can lead to hot spots where certain partitions become overloaded. This happens when all measurements within a time period are directed to a single node. Solution: To distribute data more evenly, the key can be prefixed with a sensor ID. This spreads the measurement data across multiple nodes, balancing the load more effectively. Trade-Off: While this approach improves data distribution and reduces hot spots, it complicates queries. Fetching data for multiple sensors within a time range now requires separate range queries for each sensor, potentially increasing query complexity and time. Partitioning by Hash of Key # Because of the risk of hot spots, many distributed datastores use a hash function to determine the partition for a given key. A good hash function takes skewed data and makes it uniformly distributed.\nOnce you have a suitable hash function for keys, you can assign each partition a range of hashes, and every key whose hash falls within a partition\u0026rsquo;s range will be stored in that partition\nLost\nBy using the hash of the key for partitioning we lose a nice property of key-range partitioning: the ability to do efficient range queries. Keys were once adjacent are now scattered across all the partitions, so their sort order is lost\nCompound primary key\nCompound primary key consists of several columns. First part of the key is called hashing key or partitioning key, which determines the partition. Other columns are used as an index for sorting the data. A query can\u0026rsquo;t search for a range of values within the first column of a compound key, but if it specifies a fixed value for the first column, it can perform an efficient range scan over the other columns of the key\nSkewed Workloads and Relieving Hot Spots # As discussed, hashing a key to determine its partition can help reduce hot spots. However, it can’t avoid them entirely: in the extreme case where all reads and writes are for the same key, you still end up with all requests being routed to the same partition. For example, a celebrity with millions of followers in a social media application\nOne solution is to add a random number to the beginning or end of the key. It could split the queries evenly to different partitions. However, that brings problem when reading data from those keys as you have to query the data from many partitions and then combine it\nPartitioning and Secondary Indexes # Secondary Index # A secondary index is an additional data structure that allows you to query the database more efficiently on columns other than the primary key.\nAssume you have a employees table with following columns\nemployee_id (Primary Key) name department salary hire_date Without Secondary Index\nSELECT * FROM employees WHERE department = \u0026#39;Sales\u0026#39;; Without a secondary index, the database might have to perform a full table scan, checking each row to find matches for the department column. This can be inefficient, especially for large tables\nWith Secondary Index\nYou can create a secondary index for department column. Database will create an additional data structure that maps department names to the rows where they appear. When you run the same query, the database can use the secondary index to quickly locate rows whose department is Sales without scanning the entire table\nData Structure\nYou can conceptually think the data structure of secondary index as\nMap\u0026lt;column_value, List\u0026lt;row_pointer\u0026gt;\u0026gt;\nSecondary index key: the value of the column that the secondary index is based on. For example, distinct department names\nList of rows: the index maintains a list of POINTERS (not rows) to the rows in the table where the column value matches the key. These pointers could be row ID or primary key values that uniquely identify each row\nLocal Secondary Index # A local secondary index is a type of secondary index in partitioned databases where the index is partitioned in the same manner as the base table. Each partition of the index corresponds directly to a partition of the base table, and the index entries within each partition only reference the rows within that partition.\nDisadvantages of Local Indexes\nSecondary index query could be expensive when you need to access data distributed across many or all partitions. For example, the employee table is partitioned by primary key employee_id, and you create a local index for department. When you need to query all employees whose department is Sales, you need to query all partitions\nGlobal Secondary Index # Global secondary index covers entire database regardless how the database is partitioned. That means the index entries can reference rows across multiple partitions\nPartitioning Global Secondary Index\nGlobal secondary indexes (GSIs) can also be partitioned. When a global secondary index is partitioned, it means that the index itself is divided into smaller segments or partitions, each of which can be managed independently. This allows the index to scale more effectively with the underlying data and distribute the indexing loads across multiple nodes.\nIndex update overheads\nThe downside of a global index is that writes are slower and more complex, because a write to a single row or document may affect multiple partitions of the index (a table has many secondary indexes and different secondary index entries may in different partitions)\nIn practice, updates to global secondary indexes are often asynchronous. That won\u0026rsquo;t increase write latency, but the index may not be up-to-date\nRebalancing Partitions # The process of moving loads from one node in the cluster to another is called rebalancing.\nMinimum Requirements of Rebalancing\nAfter rebalancing, the loads (data storage, read and write requests) should be shared fairly between the nodes in the cluster\nWhile rebalancing is happening, the database should continue accepting reads and writes\nNo more data than necessary should be moved between nodes\nStrategies for Rebalancing # Not Recommended: Hash Mod N # Assume we have N nodes. Hash(key) mod N would return a number between 0 and N-1. We can number nodes from 0 to N-1, and then use mod to assign the key to corresponding node.\nThe problem with mod N approach is that if the number of nodes N changes, most of the keys will need to be moved from one node to another node (including both existing nodes and new nodes). Such frequent moves make rebalancing excessively expensive.\nThis approach moves data around more than necessary.\nFix Number of Partitions # Create many more partitions than nodes, and assign several partitions to each node. For example, we have 10 nodes, and we create 100 partitions, and assign 10 partitions to each node.\nIf a new node is added to the cluster, the new node can \u0026ldquo;steal\u0026rdquo; a few partitions from every existing nodes until partitions are fairly distributed once again.\nIn this configuration, it’s possible to split and merge partitions, but a fixed number of partitions is operationally simpler, so many fixed-partition databases choose not to implement partition splitting.\nThus, the number of partitions configured at the outset is the maximum number of nodes you can have, so you need to choose it high enough to accommodate future growth.\nEach partition contains a fixed fraction of the total data, the size of each partition grows proportionally to the total amount of data in the cluster. If partitions are very large, rebalancing and recovery from node failures become expensive.\nHowever, each partition also has management overhead, so it’s counterproductive to choose too high a number.\nDynamic Partitioning # For databases that use key range partitioning, a fixed number of partitions with fixed boundaries would be very inconvenient when you need to reconfigure the partition boundaries\nDynamic partitioning would split a partition exceeding a configured size into two partitions and merge shrunk partitions into one partition.\nEach partition is assigned to one node, and each node can handle multiple partitions. After a large partition has been split, one of its two halves can be transferred to another node in order to balance the load.\nDynamic partitioning is not only suitable for key range-partitioned data, but can equally well be used with hash-partitioned data.\nPartitioning Proportionally to Nodes # With dynamic partitioning, the number of partitions is proportional to the size of the dataset, since the splitting and merging processes keep the size of each partition between some fixed minimum and maximum. On the other hand, with a fixed number of partitions, the size of each partition is proportional to the size of the dataset. In both of these cases, the number of partitions is independent of the number of nodes.\nA third option is to make the number of partitions proportional to the number of nodes \u0026ndash; in other words, to have fixed number of partitions per node.\nThe size of each partition grows proportionally to the dataset size while the number of nodes remains unchanged\nWhen you increase the number of nodes, the partitions become smaller again\nNew Node Joins\nWhen a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of on half of each of those split partitions while leaving the other half of each partition in place\nRequest Routing # Service Discover: When a client wants to make a request, how does it know which node to connect to?\nAllow clients to send the request to any node, and the node either handles the request directly or forward it to appropriate node\nSend all requests from clients to a routing layer, like load balancer\nRequire that clients be aware of the partitioning and the assignment of partitions to nodes. In this case, clients can connect directly to the appropriate node\n"},{"id":44,"href":"/docs/programming-core/java/netty/pipeline-handler/","title":"Pipeline \u0026 Handler","section":"Netty","content":" Handler # Role # A Handler in Netty is a component that contains the business logic for processing inbound and outbound data as well as various network (channel) events.\nTypes # ChannelInboundHandler:\nPurpose: Deals with inbound data and events. It processes incoming data and reacts to channel events Key Methods: channelRead, channelActive, channelInactive, channelRegistered, channelUnregistered. Usage: You override these methods to perform actions like reading data from a network socket, reacting to channel activation or deactivation, etc. ChannelOutboundHandler:\nPurpose: Responsible for handling outbound operations, such as writing data to the network or closing the channel. Key Methods: write, flush, close, bind, connect. Usage: These methods are overridden to intercept and execute operations that modify the state of the channel or write data to it. Pipeline # What is pipeline? # A Netty pipeline is an ordered list of handlers. It represents a sequence of operations that are applied to inbound and outbound data.\nStructure and Flow # ChannelPipeline: Each Channel in Netty has its own ChannelPipeline. This pipeline is automatically created when the channel is created. Ordered Sequence: Handlers are placed in the pipeline in a specific order, and the data or events are processed in this sequence. Data Flow: Inbound Data: Flows head-to-tail through the pipeline. Each inbound handler processes the data and passes it to the next handler. Outbound Data: Flows tail-to-head. Each outbound handler processes the data and passes it back towards the head of the pipeline. Example # Server # 1. EventLoopGroup boss = new NioEventLoopGroup(); 2. EventLoopGroup worker = new NioEventLoopGroup(2); 3. ChannelInboundHandlerAdapter inboundHandler1 = new ChannelInboundHandlerAdapter() { 4. @Override 5. public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { 6. ByteBuf buf = (ByteBuf) msg; 7. String message = buf.toString(StandardCharsets.UTF_8); 8. log.debug(\u0026#34;InboundHandler_1 receives msg: [{}]\u0026#34;, message); 9. super.channelRead(ctx, message); 10. } 11. }; 12. ChannelInboundHandlerAdapter inboundHandler2 = new ChannelInboundHandlerAdapter() { 13. @Override 14. public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { 15. log.debug(\u0026#34;InboundHandler_2 receives msg: [{}]\u0026#34;, msg); 16. ctx.channel().writeAndFlush(ctx.alloc().buffer().writeBytes(\u0026#34;Hello Client\u0026#34;.getBytes())); 17. super.channelRead(ctx, msg); 18. } 19. }; 20. ChannelOutboundHandlerAdapter outboundHandler1 = new ChannelOutboundHandlerAdapter() { 21. @Override 22. public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception { 23. log.debug(\u0026#34;OutboundHandler_1 writes msg\u0026#34;); 24. super.write(ctx, msg, promise); 25. } 26. }; 27. ChannelOutboundHandlerAdapter outboundHandler2 = new ChannelOutboundHandlerAdapter() { 28. @Override 29. public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception { 30. log.debug(\u0026#34;OutboundHandler_2 writes msg\u0026#34;); 31. super.write(ctx, msg, promise); 32. } 33. }; 34. new ServerBootstrap() 35. .group(boss, worker) 36. .channel(NioServerSocketChannel.class) 37. .childHandler(new ChannelInitializer\u0026lt;NioSocketChannel\u0026gt;() { 38. @Override 39. protected void initChannel(NioSocketChannel nioSocketChannel){ 40. ChannelPipeline pipeline = nioSocketChannel.pipeline(); 41. nioSocketChannel.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); 42. pipeline.addLast(inboundHandler1); 43. pipeline.addLast(inboundHandler2); 44. pipeline.addLast(outboundHandler1); 45. pipeline.addLast(outboundHandler2); 46. } 47. }) 48. .bind(9999); Line 1-2. EventLoopGroups Creation: Two EventLoopGroup instances are created. boss handles accepting new connections, and worker (with 2 threads) handles actual I/O operations for the connections. Line 3-11. First Inbound Handler: A custom inbound handler (inboundHandler1) that reads incoming data (as ByteBuf), converts it to a string, logs it, and forwards it down the pipeline. Line 12-19. Second Inbound Handler: Another inbound handler (inboundHandler2) that logs the received message and writes a response back to the client. Line 20-26. First Outbound Handler: A custom outbound handler (outboundHandler1) that logs a debug message when it writes data. Line 27-33. Second Outbound Handler: Another outbound handler (outboundHandler2) with similar functionality as the first outbound handler, logging when data is written. 34-48. Bootstrap a server. In the channel pipeline, five handlers are added, including a logging handler and the two inbound and two outbound handlers Client # EventLoopGroup group = new NioEventLoopGroup(); ChannelFuture channelFuture = new Bootstrap() .group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer\u0026lt;NioSocketChannel\u0026gt;() { @Override protected void initChannel(NioSocketChannel nioSocketChannel) { nioSocketChannel.pipeline().addLast(new LoggingHandler(LogLevel.DEBUG)); nioSocketChannel.pipeline().addLast(new StringEncoder()); } }) .connect(new InetSocketAddress(\u0026#34;localhost\u0026#34;, 9999)); Channel channel = channelFuture.sync().channel(); channel.writeAndFlush(\u0026#34;Hello World\u0026#34;); Client connects to the server Then client sends a message to the server Execution # Start the server Start the client Execution Result # Server log\n2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] REGISTERED 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] ACTIVE 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] READ: 11B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 57 6f 72 6c 64 |Hello World | +--------+-------------------------------------------------+----------------+ 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG c.w.n.c.pipeline_handler.Server - InboundHandler_1 receives msg: [Hello World] 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG c.w.n.c.pipeline_handler.Server - InboundHandler_2 receives msg: [Hello World] 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG c.w.n.c.pipeline_handler.Server - OutboundHandler_2 writes msg 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG c.w.n.c.pipeline_handler.Server - OutboundHandler_1 writes msg 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] WRITE: 12B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 43 6c 69 65 6e 74 |Hello Client | +--------+-------------------------------------------------+----------------+ 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] FLUSH 2024-01-03 22:16:03 [nioEventLoopGroup-3-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0x9ce2b880, L:/127.0.0.1:9999 - R:/127.0.0.1:46006] READ COMPLETE Client log\n2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712] REGISTERED 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712] CONNECT: localhost/127.0.0.1:9999 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] ACTIVE 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] WRITE: 11B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 57 6f 72 6c 64 |Hello World | +--------+-------------------------------------------------+----------------+ 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] FLUSH 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] READ: 12B +-------------------------------------------------+ | 0 1 2 3 4 5 6 7 8 9 a b c d e f | +--------+-------------------------------------------------+----------------+ |00000000| 48 65 6c 6c 6f 20 43 6c 69 65 6e 74 |Hello Client | +--------+-------------------------------------------------+----------------+ 2024-01-03 22:16:03 [nioEventLoopGroup-2-1] DEBUG i.n.handler.logging.LoggingHandler - [id: 0xa3ce2712, L:/127.0.0.1:46006 - R:localhost/127.0.0.1:9999] READ COMPLETE Pipeline Virtualization # Double-Linked List:\nEach handler in the pipeline is a node in a double-linked list, maintaining references to its previous and next handlers. This allows for efficient traversal in both directions – towards the head for outbound events and towards the tail for inbound events. Dynamic Modification:\nThe double-linked list structure allows for handlers to be dynamically added, removed, or replaced at runtime. This is crucial for Netty\u0026rsquo;s flexibility, enabling the pipeline to adapt to different protocol requirements or runtime conditions. Ordered Processing:\nThe order of handlers in the list determines the order of processing. Inbound data flows through the pipeline from the first (head) to the last (tail) handler, while outbound data flows in the opposite direction. Contextual Linkage:\nEach handler is associated with a ChannelHandlerContext, which provides the handler with its position (context) in the pipeline. This context is used to interact with the pipeline, such as forwarding events, accessing the channel, or modifying the pipeline itself. Head and Tail Contexts:\nSpecial HeadContext and TailContext nodes are typically present in the beginning and at the ends of the pipeline. These nodes handle some standard operations, like the initial entry of events into the pipeline or the termination of event propagation. How Does It Work? # Client\nClient Registration, Connection, and Activation The client registers a new channel (REGISTERED), connects to the server (CONNECT), and becomes active (ACTIVE). Server\nServer Registration and Activation The server registers a new channel (REGISTERED) and then becomes active (ACTIVE). This indicates that the server accepts client\u0026rsquo;s connection Client\nSending Data to Server: The client writes (WRITE) the \u0026ldquo;Hello World\u0026rdquo; message (11 bytes) and then flushes it (FLUSH) to the server. Server\nReceiving Client Data:\nThe server reads (READ) 11 bytes of data from the client. This data corresponds to the string \u0026ldquo;Hello World\u0026rdquo;. Inbound Handler Processing:\nInboundHandler_1 on the server receives the \u0026ldquo;Hello World\u0026rdquo; message, logs it, and passes it along. InboundHandler_2 also receives the \u0026ldquo;Hello World\u0026rdquo; message, logs it, and then writes a response (\u0026ldquo;Hello Client\u0026rdquo;) back to the client. Outbound Handler Processing:\nOutboundHandler_2 and OutboundHandler_1 log the message as it passes through the outbound pipeline. The server then writes 12 bytes, which is the \u0026ldquo;Hello Client\u0026rdquo; response, and flushes it (FLUSH) to the client. Completion of Read Operation:\nThe READ COMPLETE event signifies the end of the read operation for this round of communication. Client\nReceiving Server Response: The client reads (READ) 12 bytes of data, which is the \u0026ldquo;Hello Client\u0026rdquo; message sent by the server. Completion of Read Operation: Similar to the server, the READ COMPLETE event on the client side indicates the end of the read operation. "},{"id":45,"href":"/docs/distributed-system/technology/redis/pub-sub/","title":"Redis Pub/Sub","section":"Redis","content":" Redis Pub/Sub # Redis Pub/Sub (Publish/Subscribe) is a lightweight messaging system that allows clients to send and receive messages in real time. It’s designed for event-driven communication where publishers broadcast messages to channels, and subscribers listening to those channels receive the messages instantly.\nKey Components # Channels: Channels are message hubs in Redis, responsible for receiving and broadcasting messages. Publishers: Clients that send messages to channels. Subscribers: Clients that listen to channels and receive messages. How It Works # Subscribers Subscribe to Channels:\nA subscriber issues the SUBSCRIBE command to start listening to one or more channels. Example: # Subscriber listens to \u0026#39;news\u0026#39; and \u0026#39;sports\u0026#39; channels SUBSCRIBE news sports Publishers Publish Messages:\nA publisher sends messages to a specific channel using the PUBLISH command. Example: # Publisher sends a message to the \u0026#39;news\u0026#39; channel PUBLISH news \u0026#34;Breaking news! Redis is awesome.\u0026#34; Message Broadcast:\nRedis instantly delivers the message to all subscribers of the channel. Example: # Subscribers receive the message in real time \u0026#34;news: Breaking news! Redis is awesome.\u0026#34; Dynamic Subscriptions:\nClients can subscribe or unsubscribe from channels at any time without interrupting other operations. Example: # Dynamically unsubscribe from \u0026#39;sports\u0026#39; channel UNSUBSCRIBE sports Key Properties of Redis Pub/Sub # Push-Based Messaging:\nMessages are pushed to subscribers as soon as they are published, unlike pull-based systems like Kafka. Synchronous Communication:\nMessages are broadcasted to all active subscribers immediately after being published. No Acknowledgment:\nRedis Pub/Sub does not require explicit acknowledgment of message delivery. No Message Persistence:\nMessages are only delivered to active subscribers. If a subscriber is offline, it will miss messages sent during its downtime. Dynamic Subscribe/Unsubscribe:\nClients can dynamically subscribe or unsubscribe from channels at runtime without interfering with other subscribers or the overall Pub/Sub system. "},{"id":46,"href":"/docs/web/web-securities/tls-handshake/","title":"TLS Handshake","section":"Web Securities","content":" TLS/SSL and HTTPS # TLS (Transport Layer Security) and its predecessor, SSL (Secure Sockets Layer), are integral to HTTPS, which stands for Hypertext Transfer Protocol Secure. HTTPS is the secure version of HTTP, the primary protocol used for transmitting web pages over the internet. This post will introduce the process of TLS handshake, and the TLS version is TLS 1.2\nTLS Handshake # ![client-hello](/docs/programming/web/security/tls-handshake/c `lient-hello.svg)\nClient to Server: ClientHello\nThe client starts the handshake by sending a ClientHello message. This message includes\nthe TLS version the client supports, a list of cryptographic algorithms (cipher suites) it can use, and a random byte string (client random) for key generation purposes. Server to Client: ServerHello The server responds with a ServerHello message. This message indicates the chosen TLS protocol version (compatible with the client\u0026rsquo;s version), selects a cipher suite from the client\u0026rsquo;s list, and includes a server-generated random byte string (server random).\nServer to Client: Certificate The server sends its digital certificate to the client. This certificate, typically issued by a trusted Certificate Authority (CA), contains the server\u0026rsquo;s public key and is used to authenticate the server to the client.\nServer to Client: Server Key Exchange This message is sent by the server only if the server\u0026rsquo;s digital certificate does not contain enough data to allow the client to exchange a pre-master secret. It provides cryptographic parameters necessary for the client to establish the pre-master secret.\nServer to Client: Server Hello Done This message indicates the end of the ServerHello and associated messages. After sending this message, the server waits for the client\u0026rsquo;s response.\nClient to Server: Client Key Exchange, Change Cipher Spec, Encrypted Handshake Message\nClient Key Exchange: The client responds with key exchange data, which typically includes a pre-master secret encrypted with the server\u0026rsquo;s public key. Change Cipher Spec: The client then sends a message indicating that subsequent messages from the client will be encrypted using the newly agreed cipher suite and keys. Encrypted Handshake Message: This is a \u0026lsquo;Finished\u0026rsquo; message encrypted with the new encryption settings. It provides a cryptographic check that ensures the integrity of the handshake thus far. 7. Server to Client: Encrypted Handshake Message\nThe server decrypts the client\u0026rsquo;s \u0026lsquo;Finished\u0026rsquo; message and verifies it. Then, it sends its own \u0026lsquo;Finished\u0026rsquo; message, encrypted with the agreed cipher suite and keys. This message serves to confirm that the server part of the handshake is complete and that the server will begin to use the new security settings for all subsequent messages.\nSymmetric Encryption in TLS Handshake # Establishing a Session Key: After the asymmetric exchange of the pre-master secret, both the client and the server independently compute the master secret from the pre-master secret and the random numbers exchanged in the ClientHello and ServerHello messages.\nGeneration of Session Keys: From the master secret, both parties derive a set of symmetric session keys using a secure key derivation function. These keys include separate encryption and MAC (Message Authentication Code) keys for both client-to-server and server-to-client communication.\nData Encryption: Once the symmetric session keys are established, all subsequent data transmitted during the TLS session is encrypted using symmetric encryption. This includes the final part of the handshake, where both the client and server exchange \u0026lsquo;Finished\u0026rsquo; messages encrypted with the symmetric keys.\nReference # Bilibili Video\n"},{"id":47,"href":"/docs/programming-core/java/nio/unblocking-mode/","title":"Unblocking Mode","section":"Nio","content":" Unblocking Mode # In last post, we discussed blocking mode and its problems. In this post, we will discuss unblocking mode and its problems\nCode Example # Server\n@Slf4j public class Server { public static void main(String[] args) throws IOException { ByteBuffer buffer = ByteBuffer.allocate(32); ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(9999)); ssc.configureBlocking(false); List\u0026lt;SocketChannel\u0026gt; channels = new ArrayList\u0026lt;\u0026gt;(); while (true) { log.debug(\u0026#34;Server connecting\u0026#34;); SocketChannel sc = ssc.accept(); if (sc != null) { log.debug(\u0026#34;Connect to client \u0026#34; + sc.getRemoteAddress()); sc.configureBlocking(false); channels.add(sc); } for (SocketChannel channel : channels) { log.debug(\u0026#34;Server reading\u0026#34;); int read = channel.read(buffer); if (read != 0) { buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); log.debug(\u0026#34;Read data from client \u0026#34; + channel.getRemoteAddress()); } } } } } Comparing with server code of blocking mode, we set blocking configuration of the ServerSocketChannel and SocketChannel to false ssc.configureBlocking(false); sc.configureBlocking(false); Also we add a conditional check before adding the SocketChannel to the list and reading data from the channel. That is because in unblocking mode, method scc.accept() will not be blocked, instead it returns a SocketChannel object or null depending on if a client connects to the server. Hence, we should only add the SocketChannel to the list if it is not null, in other words, a client connects to the server. Method channel.read(buffer) will always read from the Channel no matter clients send data or not. What the method returns is the length of bytes it reads. Here we only handle the data if len \u0026gt; 0 if (sc != null) { ... } if (read != 0) { ... } Client\n@Slf4j public class Client { public static void main(String[] args) throws IOException { SocketChannel sc = SocketChannel.open(); log.debug(\u0026#34;Connecting to server\u0026#34;); sc.connect(new InetSocketAddress(InetAddress.getLocalHost(), 9999)); log.debug(\u0026#34;Connected with server\u0026#34;); while (true) { String input = Scanner_.scanLine(\u0026#34;Input: \u0026#34;); log.debug(\u0026#34;Sending data [{}] to server\u0026#34;, input); sc.write(StandardCharsets.UTF_8.encode(input)); log.debug(\u0026#34;Sent data [{}] to server\u0026#34;, input); } } } Client code has no difference with that code in blocking mode Demo # Start the server # Server log:\n... 09:26:19.287 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:26:19.287 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:26:19.287 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:26:19.287 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting ... The log Server connecting is printed infinitely because of unblocking mode Client connects to the server # Server log:\n... 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server reading 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server reading 09:28:40.194 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Server connecting ... Logs are continuously printed, but this time, new log Server Reading is printed also because after the client connects to the server, the SocketChannel is added in the list, and the server unblockingly reads the channel Client log:\n09:28:33.989 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Connecting to server 09:28:33.993 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Connected with server Input: Client sends data to the server # To better demonstrate unblocking mode, I removed some logs in the server code to avoid logs from continuously being printed. Then I restarted the server, and connects to the client\nClient log:\n09:40:28.973 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Connecting to server 09:40:28.976 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Connected with server Input: Hello Server from ClientA! 09:40:45.721 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Sending data [Hello Server from ClientA!] to server 09:40:45.722 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Sent data [Hello Server from ClientA!] to server Input: Hello Server from ClientA Again! 09:40:57.604 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Sending data [Hello Server from ClientA Again!] to server 09:40:57.605 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Client - Sent data [Hello Server from ClientA Again!] to server Input: A client connects to the server and sends two strings to the server. In blocking mode, the server should only receive first string and receive second string until another client connects to the server Server log\n09:40:28.989 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Connect to client /127.0.0.1:50930 09:40:45.723 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server from ClientA! 09:40:45.723 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Read data from client /127.0.0.1:50930 09:40:57.605 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server from ClientA Again! 09:40:57.605 [main] DEBUG com.whatsbehind.netty_.nio.nonblocking.Server - Read data from client /127.0.0.1:50930 The server connects to the client Further more, it receives all strings from the client. As we can see here, because the server is running in unblocking mode, it can handle client connection and data communication concurrently Summary # In unblocking mode, the blocking methods get executed, so the server can handle multiple connections and receive data from multiple clients concurrently. However, different from blocking mode in which the CPU is blocked for most time, CPU in unblocking mode is busy and always running. That\u0026rsquo;s the problem of unblocking mode and it is a waste of resource cause in most time, CPU doesn\u0026rsquo;t do any useful work but run the while loop\u0026hellip; In the next post, we will discuss how NIO uses Selector to handle this issue "},{"id":48,"href":"/docs/programming-core/java/thread/synchornization/","title":"Synchronization","section":"Thread","content":" Thread Interference # Imagine you have an object that maintains a hit count for a website. If two threads increment the hit counter at the same time, they might read the same value, say 100. Both threads then increment it and set it back to the object. The result should be 102 hits, but because there was no synchronization, you only get 101 - one hit is lost. This is a simple example of a race condition.\npublic class ThreadInterference { public static void main(String[] args) throws InterruptedException { InterferenceThread thread1 = new InterferenceThread(); InterferenceThread thread2 = new InterferenceThread(); thread1.start(); thread2.start(); thread1.join(); thread2.join(); int count = InterferenceThread.getCount(); System.out.println(\u0026#34;The value of count in InterferenceThread class is supposed to be 2_000_000, but the real value is: \u0026#34; + count); } } class InterferenceThread extends Thread { static int count = 0; int loopCount = 0; public static int getCount() { return count; } @Override public void run() { while (loopCount++ \u0026lt; 1_000_000) { count++; } } } /* * Executing result: * The value of count InterferenceThread class is supposed to be 2_000_000, but the real value is: 1563643 * */ Introduction to Synchronization # In Java, synchronization is a mechanism that allows us to control the access of multiple threads to any shared resource. It\u0026rsquo;s critical because it helps prevent thread interference and consistency problems.\nWhy do we need synchronization? # To avoid thread interference which occurs when multiple threads try to access and modify the shared resource concurrently. To prevent consistency problems by making sure that the shared resource is in a consistent state. To achieve thread safety when creating applications that are intended to be used in a multithreaded environment. The Concept of a Monitor (Intrinsic Locks) # Every Java object has an intrinsic lock associated with it. When a thread wants to execute a synchronized method on an object, it first needs to obtain the intrinsic lock. Here\u0026rsquo;s how it works:\nWhen a synchronized method is called, the thread automatically acquires the lock before executing the method. Other threads that attempt to call a synchronized method on the same object are blocked until the first thread exits the method and releases the lock. Synchronized Methods/Block # Synchronized methods allow us to create thread-safe operations by ensuring that only one thread can execute a synchronized method on an instance at a time. When a thread is executing a synchronized method, all other threads that invoke synchronized methods on the same instance will be blocked until the first thread exits the method.\nClass-Level Synchronization # You can synchronize static methods or code snippets containing static fields, which locks the Class object associated with the class. This means it locks at the class level, and only one thread can execute any of the synchronized static methods for that class.\npublic class ClassLevelSynchronization { public static void main(String[] args) throws InterruptedException { ClassLevelSyncThread thread1 = new ClassLevelSyncThread(); ClassLevelSyncThread thread2 = new ClassLevelSyncThread(); thread1.start(); thread2.start(); thread1.join(); thread2.join(); int count = ClassLevelSyncThread.getCount(); System.out.println(\u0026#34;The value of count in InterferenceThread class is supposed to be 2_000_000, but the real value is: \u0026#34; + count); } } class ClassLevelSyncThread extends Thread { static int count = 0; int loopCount = 0; public static int getCount() { return count; } @Override public void run() { while (loopCount++ \u0026lt; 1_000_000) { synchronized (ClassLevelSyncThread.class) { count++; } } } } /* * The value of count in InterferenceThread class is supposed to be 2_000_000, but the real value is: 2000000 * */ In above example, code count++ is wrapped by a synchronized block, and the lock for this block is SynchronizedThread.class, which means at any time only one SynchronizedThread thread can execute count++ code.\nWhy use SynchronizedThread.class as the lock? That is because count is a static field, and its value is associated to SynchronizedThread class instead of any concrete objects. If we use a concrete object as the lock like this, cause the lock belongs to different objects, the scenario that multiple threads access the same variable at a moment will happen again. You can replace the SynchronizedThread.class with this and check the executing results.\nInstance-Level Synchronization # When you synchronize an instance method, you are locking the instance for which the method was called. Only one thread per instance can execute any of the synchronized instance methods.\npublic class InstanceLevelSynchronization { public static void main(String[] args) throws InterruptedException { InstanceLevelSyncThread instanceLevelSyncThread = new InstanceLevelSyncThread(); Thread thread1 = new Thread(instanceLevelSyncThread); Thread thread2 = new Thread(instanceLevelSyncThread); Thread thread3 = new Thread(instanceLevelSyncThread); thread1.start(); thread2.start(); thread3.start(); thread1.join(); thread2.join(); thread3.join(); int count = instanceLevelSyncThread.getCount(); System.out.println(\u0026#34;The value of count in InterferenceThread class is supposed to be 3_000_000, but the real value is: \u0026#34; + count); } } class InstanceLevelSyncThread implements Runnable { int count = 0; @Override public void run() { for (int loopCount = 0; loopCount \u0026lt; 1_000_000; loopCount++) { synchronized (this) { count++; } } } public int getCount() { return count; } } /* * Executing results: * The value of count in InterferenceThread class is supposed to be 3_000_000, but the real value is: 3000000 * */ Here we initiate a InstanceLevelSyncThread instance and create three threads with the instance passed in. The lock for the synchronized block here is this, which is the instance we create.\nWe craete three threads, why are they synchronized by InstanceLevelSyncThread instance?\n/* What will be run. */ private Runnable target; public Thread(Runnable target) { this(null, target, \u0026#34;Thread-\u0026#34; + nextThreadNum(), 0); } @Override public void run() { if (target != null) { target.run(); } } Let\u0026rsquo;s first look at the source code of Thread. Thread has a Runnable filed target and a constructor taking Runnable instance as parameter. Look at the run() method in Thread, the code what will be run is not the Thread itself but the Runnable instance target. In above example, alother we create three Thread instance, but we pass same target to them, so the codes run by each thread are all from same instance, which is target. That\u0026rsquo;s why, count is synchronized by this.\nWhy do we use for loop here, any problem using while loop with loopCount? loopCount is a instance-level variable which is a shared field to different threads. If we use a while loop with loopCount like below, cause it is not synchronized, its value could be implemented by different threads at same moment. In other words, loopCount is not thread safe.\nwhile (loopCount++ \u0026lt; 1_000_000) { synchronized (this) { count++; } } However, when using a for loop, we create a new int varibale each time when run() method is executed, which won\u0026rsquo;t be shared by multiple threads, so it is thread safe.\nfor (int loopCount = 0; loopCount \u0026lt; 1_000_000; loopCount++) { synchronized (this) { count++; } } "},{"id":49,"href":"/docs/networking/cird/","title":"CIRD","section":"Networking","content":" 📏 Introduction to CIDR (Classless Inter-Domain Routing) # CIDR — short for Classless Inter-Domain Routing — is a modern way to define IP address ranges in networking. It offers more flexibility and efficiency than the old class-based IP system.\nThis post breaks down what CIDR is, why it was created, and how software engineers can understand and apply it in real-world situations.\n🔹 Why CIDR Was Introduced # Before CIDR, IPs were allocated in fixed-size blocks (Class A, B, and C), which often led to:\nWasted IPs — organizations got more IPs than needed. Routing table bloat — routers had to store many redundant routes. CIDR solves these issues by:\nAllowing arbitrary-sized networks, e.g., /26, /27, /29, not just /8, /16, /24 Supporting route aggregation, where multiple small networks are represented as one larger block (e.g., four /24s can be represented as a single /22) 🔹 CIDR Notation Explained # 💡 Clarification: CIDR is not a network itself — it\u0026rsquo;s a way to define a network. Think of CIDR as a label that describes a group of IP addresses that belong to the same subnet. For example, 192.168.1.0/24 defines a network that includes IPs from 192.168.1.0 to 192.168.1.255. CIDR tells us where the network starts, and how many IPs are in it, based on the prefix length.\n🧠 Note: The IP address used in a CIDR block (e.g., 192.168.1.0/24) always has its host bits set to 0. This means it represents the network address, not an individual host.\nCIDR uses the format:\n\u0026lt;IP address\u0026gt;/\u0026lt;prefix length\u0026gt; For example:\n192.168.1.0/24 The /24 means the first 24 bits define the network portion The remaining bits are used for host addresses Each prefix length maps to a specific number of addresses:\nCIDR IP Count Usable Hosts Subnet Mask /30 4 2 255.255.255.252 /29 8 6 255.255.255.248 /28 16 14 255.255.255.240 /24 256 254 255.255.255.0 /22 1024 1022 255.255.252.0 🔎 You lose 2 IPs per block: one for the network address, and one for the broadcast address.\nFor example: In network 192.168.1.0/24, 192.168.1.0 is network address, and 192.168.1.255 is broadcast address. They are reserved and can\u0026rsquo;t be used as IP address.\n🔹 Practical Example for Software Engineers # You’re deploying a microservice in AWS and assigning it to a private subnet:\nCIDR block: 10.0.1.0/24 This gives your subnet:\n256 IPs total 254 usable IPs for EC2, Lambda, containers, etc. If you split that into /26 subnets, you can create 4 smaller subnets with 64 IPs each.\n✅ Knowing CIDR helps when defining subnets, firewall rules, VPC peering ranges, and more.\n"},{"id":50,"href":"/docs/distributed-system/load-balancing/haproxy/layer7-load-balancing/","title":"HAProxy: HTTP (Layer 7) Load Balancing","section":"Haproxy","content":" HAProxy: HTTP (Layer 7) Load Balancing # Once you’ve mastered TCP (Layer 4) load balancing with HAProxy, it\u0026rsquo;s time to unlock even more control with Layer 7 (HTTP) routing. In this post, you\u0026rsquo;ll learn how to configure HAProxy to route web traffic intelligently based on paths.\n🏋️ What is Layer 7 Load Balancing? # Layer 7 load balancing means HAProxy inspects the application layer (HTTP) of incoming requests and makes routing decisions based on that. It allows you to:\nRoute based on URL path (e.g. /api, /admin) Load balance real web servers (e.g. Python, Node.js) 🔧 Basic HTTP Load Balancer Setup # Set Up Python Server\nRun these simple HTTP servers in two terminals:\nmkdir server-a cd server-a mkdir api python3 -m http.server 9001 # Server A mkdir server-b cd server-b mkdir default python3 -m http.server 9002 # Server B Sample haproxy.cfg\nglobal log stdout format raw daemon defaults mode http log stdout format raw daemon option httplog option logasap timeout connect 5s timeout client 10s timeout server 10s frontend http_front bind *:8080 default_backend web_backends backend web_backends balance roundrobin server s1 host.docker.internal:9001 check server s2 host.docker.internal:9002 check Use http mode Bind to port 8080 Docker Port Mapping\nMake sure docker-compose.yml maps 8080:8080 to access HAProxy via http://localhost:8080.\nStart HAProxy container\n# Navigate to dir containing file docker-compose.yml docker-compose up 🚀 Testing It Out\nOpen http://localhost:8080 Refresh the page You should alternate between content from 9001 and 9002 Watch logs in docker-compose output Example log entry:\nhaproxy-1 | 172.20.0.1:59868 [06/Apr/2025:03:37:54.338] http_front web_backends/s1 0/0/1/2/+3 200 +483 - - ---- 2/2/1/0/0 0/0 \u0026#34;GET / HTTP/1.1\u0026#34; haproxy-1 | 172.20.0.1:59868 [06/Apr/2025:03:37:54.503] http_front web_backends/s2 0/0/2/2/+4 200 +489 - - ---- 2/2/1/0/0 0/0 \u0026#34;GET / HTTP/1.1\u0026#34; 🔄 Routing Based on Path # Let’s say you want to send /api/* requests to one backend, and everything else to another:\nfrontend http_front bind *:8080 acl is_api path_beg /api use_backend api_backends if is_api default_backend default_backends backend api_backends server api host.docker.internal:9001 check backend default_backends server default host.docker.internal:9002 check Breakdown:\nacl is_api path_beg /api – This defines an ACL (Access Control List) named is_api that becomes true when the request path begins with /api. It’s useful for routing REST API traffic separately from static or frontend content. use_backend api_backends if is_api: Send matched traffic to the API backend Everything else goes to the default backend 🚀 Testing It Out\nRun below commands in terminal\ncurl http://localhost:8080 The request is routed to default backend server\nhaproxy-1 | 172.20.0.1:55650 [06/Apr/2025:06:23:05.217] http_front default_backends/default 0/0/1/1/+2 200 +491 - - ---- 1/1/1/0/0 0/0 \u0026#34;GET / HTTP/1.1\u0026#34; curl http://localhost:8080/api/ The request is routed to api backend server\nhaproxy-1 | 172.20.0.1:55206 [06/Apr/2025:06:27:00.935] http_front api_backends/api 0/0/1/1/+2 200 +491 - - ---- 1/1/1/0/0 0/0 \u0026#34;GET /api/ HTTP/1.1\u0026#34; "},{"id":51,"href":"/docs/web/http/http-header-part-2/","title":"HTTP Headers Part 2: Content-Type \u0026 Accept","section":"HTTP","content":" 📘 Mastering Content-Type and Accept Headers with FastAPI # This lesson focuses on two essential HTTP headers: Content-Type and Accept. These headers play a critical role in defining the data format being sent or expected in API communication. Understanding how to use them correctly is key to building robust, flexible APIs.\n🔹 Content-Type: Declaring the Format of Sent Data # Direction: Request and Response Purpose: Informs the recipient how to parse the message body. Common Values\nValue Description application/json JSON format (default in most APIs) text/plain Plain text application/x-www-form-urlencoded URL-encoded form data multipart/form-data Used for file uploads with forms Example:\nContent-Type: application/json This tells the server that the request body is formatted as JSON.\n🔸 Accept: Indicating the Desired Response Format # Direction: Request only Purpose: Tells the server which content types the client is willing to accept. Common Values\nValue Description application/json Requests a JSON response text/plain Requests a plain text response */* Accepts any response format (default) Example:\nAccept: application/json This tells the server the client prefers a response formatted as JSON.\n🛠️ Hands-On Practice with FastAPI # The following FastAPI app demonstrates how to inspect the Content-Type and Accept headers and respond accordingly.\n📄 Start Your Server # Change directory to 03-http-headers and create a new Python file content_type_and_accept.py:\ncd 03-http-headers touch content_type_and_accept.py Paste in the following Python code:\nfrom fastapi import FastAPI, Header, Request from fastapi.responses import JSONResponse, PlainTextResponse from typing import Optional app = FastAPI() @app.post(\u0026#34;/echo\u0026#34;) async def echo_content_type( request: Request, content_type: Optional[str] = Header(None) ): body = await request.body() return { \u0026#34;received_content_type\u0026#34;: content_type, \u0026#34;raw_body\u0026#34;: body.decode() } @app.get(\u0026#34;/greet\u0026#34;) async def greet( accept: Optional[str] = Header(\u0026#34;application/json\u0026#34;) ): if \u0026#34;text/plain\u0026#34; in accept: return PlainTextResponse(\u0026#34;Hello, plain world!\u0026#34;) return JSONResponse({\u0026#34;message\u0026#34;: \u0026#34;Hello, JSON world!\u0026#34;}) Use Uvicorn to start your FastAPI app:\nuvicorn content_type_and_accept:app --reload 🧪 Try These curl Commands # Run the following curl command in another terminal:\n# Send plain text body with Content-Type header curl -X POST http://localhost:8000/echo -H \u0026#34;Content-Type: text/plain\u0026#34; -d \u0026#34;hello\u0026#34; # Request plain text response using Accept header curl -H \u0026#34;Accept: text/plain\u0026#34; http://localhost:8000/greet # Request JSON response using Accept header curl -H \u0026#34;Accept: application/json\u0026#34; http://localhost:8000/greet You should see response:\n{\u0026#34;received_content_type\u0026#34;:\u0026#34;text/plain\u0026#34;,\u0026#34;raw_body\u0026#34;:\u0026#34;hello\u0026#34;} Hello, plain world! {\u0026#34;message\u0026#34;:\u0026#34;Hello, JSON world!\u0026#34;} "},{"id":52,"href":"/docs/cloud/docker/fundamentals/lesson-3-images/","title":"Lesson 3: Docker Images","section":"Fundamentals","content":" 🐳 Docker Mastery Course – Lesson 3 # 🔬 Mastering Docker Images: Understand and Use Them Effectively # Welcome to Lesson 3 of the Docker Mastery Course! In this lesson, you\u0026rsquo;ll dive into the concept of Docker images — what they are, how to retrieve and inspect them, and how to run containers based on these images.\n🎯 Learning Goal # By the end of this lesson, you will:\nUnderstand what Docker images are and how they work Learn how to pull, inspect, and use images Get hands-on experience running containers from official images 📚 Core Concepts # 1. What Is a Docker Image? # A Docker image is a read-only template used to create containers. It contains:\nOS-level files (e.g., a Linux base layer) Your application code Required runtime (e.g., Python, Node.js) Configuration and environment variables ✅ Think of the runtime as the software required to run your application, such as the Python interpreter or Node.js engine.\nIn simple terms: Images are blueprints, and containers are the products built from them.\n2. Where Do Docker Images Come From? # Docker images are commonly pulled from Docker Hub, which is the default public image registry.\nFor example:\ndocker pull nginx This command downloads the nginx image from Docker Hub.\n3. Image Naming Format # Docker images typically follow this naming format:\n[registry]/[username_or_org]/[repository]:[tag] However, many parts can be omitted and Docker will automatically apply default values:\ndocker pull nginx is interpreted as: docker.io/library/nginx:latest docker.io: default registry library: default namespace for official images latest: default tag So yes — image names can often be shortened thanks to these sensible defaults.\n4. Docker Images Are Layered # Each instruction in a Dockerfile (we’ll cover this in the next lesson) adds a new layer to the image.\nLayers are cached, which speeds up builds Layers can be shared across different images This layered approach makes Docker fast and space-efficient.\n💻 Hands-On Practice # ✅ Step 1: Pull an Image from Docker Hub # Run the following command in your terminal:\ndocker pull python:3.11 This downloads the Python 3.11 image.\nTo view all local images:\ndocker images ✅ Step 2: Inspect the Image # To view detailed information about the image:\ndocker inspect python:3.11 This will return a JSON output containing the image’s configuration, creation time, ID, and more.\n✅ Step 3: Run a Container from the Image # Launch a container using the image and enter an interactive Python shell:\ndocker run -it python:3.11 Once inside, try running:\nprint(\u0026#34;Hello from inside a Docker container!\u0026#34;) Exit the shell with exit() or Ctrl+D.\n✅ Step 4: Clean Up # List all containers:\ndocker ps -a Remove a specific container:\ndocker rm \u0026lt;container_id\u0026gt; Remove the image:\ndocker rmi python:3.11 🏁 Checkpoint # You can now confidently:\nPull Docker images from Docker Hub Understand and interpret Docker image naming conventions Inspect image metadata and internal configuration Run containers from pulled images and clean them up 👏 Excellent work! You’ve just taken a major step toward mastering Docker.\n📆 Coming Up Next # Lesson 4: Building Your Own Image with a Dockerfile\nYou’ll learn how to write your first Dockerfile and build a custom image from your application code.\nYes — we will absolutely dive into Dockerfiles in the next lesson. Stay tuned!\n"},{"id":53,"href":"/docs/distributed-system/technology/redis/replication-and-sentinel/","title":"Redis Replication \u0026 Sentinel","section":"Redis","content":" Redis Replication \u0026amp; Sentinel # High availability is critical for any modern system, and Redis achieves this through its master-slave architecture and Redis Sentinel. Let’s explore how these components work together to ensure seamless operation, even in the face of failures.\nMaster and Slave Architecture # The master-slave architecture is a classic replication model commonly used in distributed systems. Its primary purpose is to enhance system scalability and fault tolerance by replicating data from a central master node to multiple slave nodes.\nRedis supports a master-slave replication model, where:\nMaster Node:\nHandles all write operations. Automatically propagates data changes to the slave nodes. Slave Nodes:\nServe as read replicas, improving scalability by handling read operations. Act as backups for the master in case of failure. What Happens If the Master is Down?\nWhen the master node goes down, the entire system can become unusable for write operations. This raises the critical question: How does Redis handle master failures?\nThis is where Redis Sentinel comes into play.\nRedis Sentinel: The Key to High Availability # Redis Sentinel is a system designed to monitor Redis nodes and ensure high availability. It:\nMonitors Redis Nodes:\nSentinels continuously check the health of the master and slave nodes by sending PING commands. If a node doesn’t respond within the configured timeout (down-after-milliseconds), it is marked as subjectively down (SDOWN). Performs Automatic Failover:\nIf a master is declared objectively down (ODOWN) by a quorum of Sentinels, Sentinel promotes one of the slaves to master. It reconfigures the other slaves to replicate from the new master. Acts as a Configuration Provider:\nSentinel informs clients about the new master, ensuring they always connect to the correct instance. What Happens If a Sentinel Instance is Down?\nSentinel is designed to operate as a cluster of multiple Sentinel nodes. Even if one or more Sentinel instances fail, as long as the remaining Sentinels meet the quorum requirement, they can continue to monitor the Redis nodes and perform failovers.\nHow Does Sentinel Ensure Clients Always Discover the Latest Master? # Sentinel achieves consistency using quorum, which is a leaderless replication mechanism. It ensures clients always connect to the latest master by leveraging quorum-based decision-making and state propagation. To understand this, let’s break the process down step-by-step.\n1. Detecting Master Failure # Each Sentinel node periodically sends PING commands to the master, slaves, and other Sentinels to monitor their health. If a Sentinel node does not receive a response within the configured down-after-milliseconds time, it marks the master as subjectively down (SDOWN). Sentinels communicate this status with each other using a gossip protocol, exchanging their view of the cluster state. 2. Declaring the Master Objectively Down (ODOWN) # A quorum of Sentinels (e.g., 3 out of 5 nodes) must agree that the master is unresponsive to declare it objectively down (ODOWN). It’s important to note that quorum is only used for the failover decision, not for updating clients with the new master information. Once quorum is achieved, the failover process begins. 3. Electing a Leader Sentinel for Failover # During failover, one Sentinel is temporarily elected to coordinate the failover process. 4. Promoting a New Master # The leader Sentinel selects the best candidate slave based on: Replication offset (to ensure minimal data loss). Connection health. Priority configuration (replica-priority). The selected slave is promoted to master, and other slaves are reconfigured to replicate from the new master. 5. Synchronizing the New Master # Once the failover is complete, the Sentinel leader synchronizes the new master node information to other Sentinels using the gossip protocol. As the state propagates through the Sentinel cluster, it is important to note that the Sentinels involved in the failover decision (write quorum) do not necessarily have the latest master information immediately, as the information is propagated asynchronously after the failover process. 6. Communicating the New Master to Clients # Clients query Sentinels to discover the current master node.\nWhile some Sentinels may have stale information during this brief propagation period, clients can query multiple Sentinels to get the latest master information, making a best-effort attempt to ensure accurate and consistent data.\n"},{"id":54,"href":"/docs/programming-core/java/nio/selector/","title":"Selector","section":"Nio","content":" Selector in Java NIO # A Selector in Java NIO is a special type of object that can check one or more NIO channels and determine which channels are ready for data operations (such as reading or writing). This is crucial in scenarios like servers handling multiple client connections.\nKey Features: # Multiplexing: A single selector can handle multiple channels. Non-blocking Mode: Channels registered with a selector are usually in non-blocking mode. Efficiency: Instead of using multiple threads to handle channels, one thread can handle multiple channels using a selector. How Selectors Work: # Create a Selector: Obtain a selector using the Selector.open() method. Register Channels with Selector: Channels (like SocketChannel) need to be registered with the selector. Selection: Use the select() method to check which channels are ready. This method blocks until at least one channel is ready. Below is the code example of server. Let\u0026rsquo;s read the code line by line to understand how selector works\n@Slf4j public class Server { public static void main(String[] args) throws IOException { Selector selector = Selector.open(); log.debug(\u0026#34;Create a selector\u0026#34;); ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(9999)); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT, null); log.debug(\u0026#34;Register ServerSocketChannel under selector\u0026#34;); while (true) { log.debug(\u0026#34;Start selecting\u0026#34;); selector.select(); log.debug(\u0026#34;Selected\u0026#34;); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = selector.selectedKeys().iterator(); while(keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); log.debug(\u0026#34;Connected to client {}\u0026#34;, sc.getRemoteAddress()); } } } } Create a Selector\nSelector selector = Selector.open(); Here we create a new Selector which has two important collections, keys and selectedKeys. At this time, the two collections are still empty. Let\u0026rsquo;s go ahead to understand what those two collection hold Register a channel with the selector\nServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(9999)); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT, null); We create a new ServerSocketChannel Then register the ServerSocketChannel under the selector In register method, we pass in the selector and interested operation. For the ServerSocketChannel, it only interests in ACCEPT operation. We will introduce more operations later After registration, a new SelectionKey is created and added into collection keys Select channels with event\nselector.select(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = selector.selectedKeys().iterator(); After invoking select method, the selector will monitor all registered channels. The method will be blocked here until channels have event Once channels have event happened, the selector will add those channels to another collection selectedKeys Handle channel operations\nSelectionKey key = keyIterator.next(); ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); Only one channel is registered under the selector, so I cast the channel to ServerSocketChannel and let it accept client\u0026rsquo;s connection Demo # Start the server\n08:45:22.261 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Create a selector 08:45:22.268 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register ServerSocketChannel under selector 08:45:22.268 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Start selecting The selector starts to monitor registered channels. select method is blocked until event happens in the channel ClientA connects\n08:46:01.420 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 08:46:01.421 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47074 08:46:01.422 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Start selecting ClientB connects\n08:46:31.762 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 08:46:31.762 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47080 08:46:31.762 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Start selecting "},{"id":55,"href":"/docs/web/web-securities/tls-handshake-client-hello/","title":"TLS Handshake: Client Hello","section":"Web Securities","content":" TLS Handshake: Client Hello # When establishing a secure HTTPS connection, the TLS handshake is the very first step that sets the foundation for encryption and trust. One of the most critical messages in this process is the ClientHello. In this post, we\u0026rsquo;ll walk through how to view and analyze the ClientHello message, and break down its key contents and purposes.\nStep 1: Capture the TLS Handshake Using OpenSSL # You can observe the raw TLS handshake between your machine and a server (like Google) using the following OpenSSL command:\nopenssl s_client -connect www.google.com:443 -servername www.google.com -msg This command:\nConnects to www.google.com over port 443 Sends the correct domain via SNI (Server Name Indication) Displays TLS messages, including the raw ClientHello in hexadecimal Example output:\n\u0026gt;\u0026gt;\u0026gt; TLS 1.3 Handshake [length 0131], ClientHello 01 00 01 2d 03 03 a1 c6 ed 3f 78 88 21 0c 32 40 ... Step 2: Save and Parse the ClientHello Message with Python # To analyze the hex data, first store it in a file called client_hello.hex:\n01 00 01 2d 03 03 a1 c6 ed 3f 78 88 21 0c 32 40 ... Now, use Python to parse it. Create file tls-parser.py:\nfrom scapy.all import * load_layer(\u0026#34;tls\u0026#34;) with open(\u0026#34;client_hello.hex\u0026#34;, \u0026#34;r\u0026#34;) as file: hex_str = file.read().replace(r\u0026#34;\\s+\u0026#34;, \u0026#34;\u0026#34;) pkt = TLSClientHello(bytes.fromhex(hex_str)) pkt.show() Run the Python script to view the ClientHello message:\npython3 tls-parser.py Step 3: Key Contents in the ClientHello Message # The ClientHello message includes several critical fields. Here\u0026rsquo;s a breakdown of the most important parts:\nField Purpose TLS Version Specifies the highest TLS version the client supports (e.g., TLS 1.3) Random Number 32 bytes of client-generated randomness used for key generation Session ID Used for session resumption (often empty in TLS 1.3) Cipher Suites List of encryption algorithms the client supports Compression Methods Legacy field, usually set to \u0026rsquo;null\u0026rsquo; (no compression) Extensions Extra information including key exchange, server name, etc. Notable Extensions\nExtension Description Key Share Contains the client\u0026rsquo;s ephemeral public key used in Diffie-Hellman key exchange. The server will respond with its own key share to derive a shared symmetric key for encrypted communication. Supported Groups Tells the server what kinds of keys the client can use to help create a shared symmetric key. Signature Algorithms Algorithms client supports to verify the server certificate Server Name Indication (SNI) Indicates the domain the client wants to connect to (e.g., www.google.com) ALPN Tells the server which application protocols are supported (e.g., HTTP/2, HTTP/1.1) 💡 Explainer: Key Share vs Supported Groups\nSupported Groups: These define what kinds of mathematical structures (like elliptic curves or finite fields) the client is capable of using in a key exchange. They are not algorithms, but parameters that describe the math. Key Share: This is the actual public key the client sends, created using one of the supported groups. It\u0026rsquo;s part of the ephemeral Diffie-Hellman exchange. During the handshake, the server picks a matching group, sends its own key share, and both sides compute the same shared symmetric key independently. Summary # The ClientHello message is the first and most foundational message in a TLS handshake. It tells the server:\nWhat the client supports (TLS version, cipher suites, extensions) What domain it wants (via SNI) What keys and algorithms it proposes to use In TLS 1.3, the key share and supported groups are crucial to securely deriving a shared symmetric encryption key between client and server. This symmetric key is what ultimately encrypts all future HTTPS traffic.\n"},{"id":56,"href":"/docs/distributed-system/design-data-intensive-applications/transactions/","title":"Transactions","section":"Design Data Intensive Applications","content":" Transaction # A transaction is a way for an application to group several reads and writes together into a logical unit. Either the entire transaction succeeds (commits) or it fails (aborts, rollbacks). This can eliminate partial failure\u0026ndash;i.e., the case where some operations succeed and some fail\nThe Meaning of ACID # ACID, which stands for Atomicity, Consistency, Isolation, and Durability, are the safety guarantees provided by transactions\nAtomicity # Definition\nThe ability to abort a transaction on error and have all writes from that transaction discarded\nIf the writes are grouped together into an atomic transaction, and the transaction cannot be completed (committed) due to a fault, then the transaction is aborted, and the database must discard or undo any writes it has made so far in that transaction.\nConsistency # Consistency: Database should be in valid states before and after transactions\nInvariants: Rules or conditions that define what constitutes a valid state in the database\nInvariant Examples\nUnique and valid email address Non-negative product price Non-negative account balance Application Property\nMany consistency rules are specific to the business logic of the application. For example, ensuring that a customer’s order is valid according to various business rules (e.g., inventory availability, discount application, shipping constraints) often requires logic that is best handled in the application layer. Thus, the letter C doesn\u0026rsquo;t really belong to ACID\nIsolation # Concurrently running transactions shouldn’t interfere with each other. For example, if one transaction makes several writes, then another transaction should see either all or none of those writes, but not some subset.\nDurability # Durability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten.\nIn a single node system, it means that data has been written to nonvolatile storages\nIn a replicated system, it means that data has been copied to some number of nodes\nSingle-Object and Multi-Object Operations # Single-Object Operations # Atomicity and isolation are essential requirements for single-object operations.\nThe need for multiple-object transactions # Many distributed databases have abandoned multiple-object transactions because they are difficult to implement across partitions. However, there are still some scenarios requiring multiple-object transactions\nWhen updating denormalized information in a document data model, you need to update multiple documents in one go\nIn databases with secondary indexes (almost used everywhere except pure key-value stores), the indexes also need to be updated every time you change a value. These indexes are different database objects from a transaction point of view\nSecondary Index: In many databases, particularly relational databases, secondary indexes are used to improve the performance of queries. Unlike the primary index, which is typically based on the primary key, secondary indexes are built on other columns. Whenever you change a value in the database that is a part of a secondary index, the secondary index needs to be updated to reflect this change\nHandling Errors and Aborts # Retry Principles\nIt is only worth retrying after transient errors (for example due to deadlock, isolation violation, temporary network interruptions, and failover). A retry for permanent error (e.g., constraint violation) is pointless\nIf the error is due to overload, retrying the transactions will make the problem worse. In this case, consider limiting the number of retries, and use exponential backoff\nWeak Isolation Levels # Read Committed # The most basic level of transaction isolation. It makes two guarantees:\nWhen reading from the database, you will only see data that has been committed (no dirty reads).\nWhen writing to the database, you will only overwrite data that has been committed (no dirty writes).\nDirty Reads # Definition\nA transaction READS data that has been modified by another transaction but not yet committed\nRead committed isolation\nAny writes by a transaction only become visible to others when that transaction commits\nDirty Writes # Definition\nA transaction WRITES data that has been modified by another transaction but not yet committed.\nDirty writes happen when two transactions modify same data concurrently\nLimitation\nRead committed only works for two direct writes, but it can\u0026rsquo;t prevent race condition between two read-modify-write operations.\nFor example, there is a counter whose value is 1 in a table, and client 1 and client 2 read its value simultaneously. Then they increment the counter value concurrently. Even with read committed, the final value of the counter is 2 instead of 3\nImplementation # No dirty writes\nUsing row-level lock. When a transaction wants to modify a particular object (a row or a document), it must first acquire a lock on that object. It must then hold the lock until the transaction is committed or aborted. Only one transaction can hold the lock\nNo dirty reads\nUsing lock to prevent dirty reads is not realistic, cause one long-running writes would block all reads to that object\nThe workaround is that for every object that is written, the database remembers both the old committed value and the new value set by the transaction that is currently holding the write lock. While the transaction is ongoing, any other transactions that read the object are simply given the old value\nSnapshot Isolation and Repeatable Read # Non-Repeatable Read # A transaction reads the same data item more than once and gets different values each time. This inconsistency happens because another concurrent transaction modifies the data between the reads\nExample\nInitial State:\nAccount balance: $1000 Alice Transaction:\nStarts and reads the balance of account Reads balance as $1000 Transfer Transaction:\nStarts and updates the balance of account (e.g., deposits $100). New balance: $1100. Commits the change. Alice Transaction:\nReads the balance of account Now reads balance as $1100. Read committed can\u0026rsquo;t solve above problem, because in second read of Alice transaction, transfer transaction has been committed. No dirty read happens\nSnapshot Isolation # The transaction sees all data that was committed in the database at the start of the transaction. Any changes made after that time are not visible to the read transaction\nImplementing Snapshot Isolation # Writes\nImplementation of snapshot isolation normally uses write lock to prevent dirty writes, which means transaction that makes a write can block the progress of another transaction that writes to the same object\nReads\nMVCC (multi-version concurrency control): The database must potentially keeps several committed versions of an object to allow various in-progress transactions to see the state of the database at different points in time.\nThe benefit of MVCC is that readers won\u0026rsquo;t block any writers and vise verse.\nComparison with read committed\nRead committed only needs to keep two versions of an object: latest committed version and modified-but-not-yet-committed version, but snapshot isolation requires multiple versions of an object\nDatabase can also use MVCC to support read committed isolation. Read committed uses a separate snapshot for each QUERY (one transaction may contain multiple queries), while snapshot isolation uses a separate snapshot for each TRANSACTION.\nImplementation Example # Each transaction has a unique transaction ID (txid).\nEach row in the table has a created_by field, containing the ID of the transaction that inserted the row into the table\nMoreover, each row has a deleted_by field, which is initially is empty. If a transaction deletes a row, the row is not deleted from the table, but it is marked for deletion by setting the field deleted_by to the ID of the transaction that requested for deletion. When it is certain that no queries would access the deleted data, a garbage collection process removes rows that are marked for deletion\nAn update is translated to a create and a delete. For example, in above picture, transaction 13 update balance of the account to $1100. Then the row with balance of $1000 is marked as deleted by transaction 13 and a new row with balance of $1100 is created by transaction 13\nVisibility rules\nAt the start of each transaction, the database makes a list of all other transactions that are in progress at that time. Any writes made by those transactions are ignored\nAny writes made by aborted transactions are ignored\nAny writes made by transactions with a later txid are ignored\nAll other writes are visible\nEquivalent rules\nWhen reader transaction queries the database, below rows are visible:\nCreated rows (not deleted): the transactions that created the rows had already committed at the time when reader transaction started\nDeleted rows: the transaction that deleted the row are NOT yet committed at the time when reader transaction started\nIndexes of Snapshot Isolation # One option is to have the index simply point to all versions of an object and require an index query to filter out any object versions that are not visible to the current transaction. When garbage collection removes old object versions that are no longer visible to any transaction, the corresponding index entries can also be removed\nPreventing Lost Updates # The read committed and snapshot isolation levels we’ve discussed so far have been primarily about the guarantees of what a read-only transaction can see in the presence of concurrent writes.\nWe have only discussed dirty writes. There are several other kinds of conflicts that can occur between concurrently writing transactions:\nLost updates # Definition\nLost updates can occur when an application reads some value from the database, modifies it and writes back the modified value (a read-modify-write cycle). If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification\nScenarios\nIncrementing a counter (requires querying the current value, calculating the new value and writing back the updated value)\nMaking a change to a JSON document (requires parsing the document, making the change and writing back the modified document)\nTwo users editing a wiki page at the same time\nSolutions # Atomic write operations # An atomic write operation ensures that the entire read-modify-write sequence is executed as a single, indivisible unit. This means that once a transaction starts modifying a value, no other transaction can see or affect the intermediate states (blocks both writes and reads)\nImplementation Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied.\nExplicit Locking # If the built-in atomic operations can\u0026rsquo;t handle the use case, the application explicitly lock the object when it starts a read-modify-write cycle. If any other transactions try to concurrently read the object, they are forced to wait until the first read-modify-write cycle has completed\nAutomatically Detecting Lost Updates # This approach doesn\u0026rsquo;t lock the object being updated and allow in parallel updates. If the transaction manager detects a lost update, it aborts the transaction and force it to retry the read-modify-write cycle\nCompare-and-Set # Compare-and-set operation allows an update to happen only the value has NOT changed since you last read it.\nLimitation\nIf the value read is from an old snapshot, it may not prevent lost updates, because this condition may be true even though another concurrent write is occurring (this updates is not visible to the transaction)\nLost Updates in Replicated System # Locks and compare-and-set operations don\u0026rsquo;t work in a replicated system, because they can only prevent lost updates in a single node\nWe discussed before that replicated system allows concurrent writes in different nodes, and we can use application code to resolve and merge those conflicts\nAtomic, commutative operations can work well in a replicated context\nCommutative operations are those where the order of execution does not affect the final result. Examples include incrementing a counter or adding elements to a set\nFor instance, if you increment a counter by 1 on Replica A and by 1 on Replica B, the final result is the same regardless of the order in which these increments are applied when the replicas synchronize.\nWrite Skew and Phantoms # In previous sections, we discussed dirty writes and lost updates, two kinds of race conditions can occur when different transactions try to write to the SAME objects concurrently.\nWrite Skew Example # Consider a hospital shift management system where two doctors should be on call for every 12 hours. On call doctors can take a break but at least one doctor should be on call, in other words, at most one doctor can take a break\nInitial state:\nAlice\u0026rsquo;s status: on_call Bob\u0026rsquo;s status: on_call Transaction A (Alice):\nReads the number of doctors who are on call (database returns 2) Plans to take a break Transaction B (Bob):\nReads the number of doctors who are on call (database returns 2) Plans to take a break Transaction A (Alice):\nUpdates Alice\u0026rsquo;s status to in_break and commits Transaction B (Bob):\nUpdates Bob\u0026rsquo;s status to in_break and commits Final state:\nAlice\u0026rsquo;s status: in_break Bob\u0026rsquo;s status: in_break The rule that at least one doctor should be on call is violated\nWrite Skew # Write skew pattern:\nA query checks whether a condition is satisfied. In above example, the number of on call doctors should be larger than 1\nIf the condition is satisfied and the application decides to go ahead, it then makes a write to other objects in the database. In above example, doctor\u0026rsquo;s status is updated from on_call to in_break\nPhantom\nDifferent transactions query same data. When the condition is satisfied, all transactions make a write to different objects. If the write to the objects breaks the condition in the query step, then write skew will occur.\nIn above example, no matter Alice or Bob updates their status from on_call to in_break, the condition that number of on call doctors should be larger than 1 is not satisfied anymore. In that case, the write made later should be aborted instead be committed.\nThis effect, where a write in one transaction changes the result of a search query in another transaction, is called a phantom\nWrite Skew Solution # Serializable isolation\nLocks all objects that are queried in the first step\nUnfortunately, this approach doesn\u0026rsquo;t work if the search query if checking the absence of rows matching some search conditions, and the write adds a row matching the same condition.\nFor example, in a meeting room booking system, before booking a room in the requested time spots, you need to search if any entries with the same room ID and time spots exist in the database, if not, then you insert a new entry with the room ID and time spots. In this example, you can\u0026rsquo;t lock any entries in the search query because no entries are matched.\nSerializability # Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency.\nActual Serial Execution # The simplest way of avoiding concurrency problems is to remove the concurrency entirely: to execute only one transaction at a time, in serial order, on a single thread.\nA system designed for single-threaded execution can sometimes perform better that a system that supports concurrency, because it can avoid the coordination overhead of locking.\nEncapsulating transactions in stored procedures # Interactive multiple-statement transaction\nAn interactive multiple-statement transaction is a type of transaction where multiple SQL statements are executed as part of a single transaction, and these statements are sent interactively from application code over the network to the database server. This means that there is significant network overhead involved, as each statement requires a round trip between the application and the database server.\nIn the interactive style of transactions, a lot of time is spent in network communication between the application and the database. If a database disallow concurrency, then interactive transactions will make the throughput and the performance dreadful.\nStored procedure\nFor this reason (Multiple network roundtrips in one transaction overloads the database server), systems with single-threaded serial transaction processing disallow interactive multiple-statement transactions. Applications need to store the transactional codes in the database ahead of time, as stored procedure, which contains multiple read or write statements and business logics. This approach \u0026ndash; one request is sent from the application to the database server, and then database executes the stored procedure \u0026ndash; eliminates the overhead of network communications.\nExample of stored procedure\nWrite Overheads # Systems with Single-threaded serial transactions can perform well for applications with high read throughput and low write throughput. Compared with read operations, write operations bring more overheads:\nData modifications and persistence\nData modified by write operations need to be durable. That typically involves writing to disk or other permanent storage.\nMost databases use write-ahead logging (WAL) to ensure durability and atomicity. Each write operation generate log entries which must be written to disk before the actual data modification.\nIndex maintenance\nWrite operations require updates to one or more indexes to keep them in sync with the underlying data Resource utilization\nWrite operations often involve advanced operations like constraint checking, trigger execution, and logging Error handling and rollback\nIf a write operation encounters an error, the database must roll back the entire change and retry in some cases Concurrent writes\nConflict resolution: Concurrent writes can lead to conflicts that the database must detect and resolve Locking: Write operations often require locking on the data modified to support concurrent writes Partitioning # Considering so many write overheads (concurrent writes are not applied here), high write throughput would overwhelm single-threaded systems. One solution is to partition your data, so you can scale your system linearly with the number of partitions.\nLimitation\nThis approach only works well if applications read and write the data in one single partition. If cross-partition transactions are required, then linear scalability is not realistic\nTwo-Phase Locking (2PL) # Two-phase locking requires strong locking on objects. It allows multiple transactions to read same objects, but if any transactions want to write an object, exclusive access is required\nReaders block other writers\nWriters block other readers and writers\nImplementation of Two-Phase Locking # Every object in the system has a lock. The lock could either be in shared mode or in exclusive mode\nIf a transaction wants to read an object, it must acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if a transaction already has an exclusive lock on the object, these transactions must wait\nIf a transaction wants to write, it must acquire the lock in exclusive mode. No other transaction can hold the lock at the same time (either in shared or in exclusive mode). In other words, readers block writes\nIf a transaction first reads and then writes an object, it must upgrade its lock from shared mode to exclusive mode. The upgrade works the same as getting an exclusive lock directly\nAfter a transaction has the acquired lock, it must hold it until the transaction commits or aborts\nPoor Performance of Two-Phase Locking # Overhead of acquiring and releasing locks. The isolation level is too strong. If two transactions try to do anything that may in any way result in a race condition, one transaction has to wait for the other to complete\nDeadlock. Because every object has a lock, and both read and write operations need to acquire the lock, the deadlock can happen quite easily that transaction A is waiting for transaction B to release the lock and vise verse.\nFor example, in the on call shift management system\nDoctor Alice and Bob are on call Transaction A (Alice) searches doctors that are on call, and it acquires the locks of two entries: Alice and Bob\u0026rsquo;s on call entries Transaction B (Bob) searches doctor that are on call, and it also acquires locks in shared mode for the same entries When either transaction tries to upgrade its lock from shared mode to exclusive mode, it will be blocked by the other transaction, and vise verse. Here, the deadlock occurs. When database detects this, it must abort all other transactions to allow only one transaction to acquire the exclusive lock\nPredicate Locks # In some types of phantoms, the rows in the query is absent, serializable isolation should potentially handle this problem. In the meeting room booking example, the condition of booking a room is no entries with same room ID and time spots. How could two-phase locking achieve this?\nPredicate locks\nPredicate locks are a type of database lock that lock search conditions or predicates instead of specific data objects (like rows or tables). For below search statements:\nSELECT * FROM bookings WHERE room_id = 123 AND end_time \u0026gt; \u0026#39;2018-01-01 12:00\u0026#39; AND start_time \u0026lt; \u0026#39;2018-01-01 13:00\u0026#39;; The transaction must first acquire shared lock on the search conditions. If a transaction already has the exclusive lock on the same search conditions, others have to wait until the lock is released\nLimitation of predicate locks\nGranularity and lock explosion\nWhen search conditions are highly specific, each unique condition potentially requires a separate lock. That may lead to a situation known as \u0026ldquo;lock explosion\u0026rdquo;, where many locks are created\nLock management and checking\nThe database must track each lock, which consumes memory and processing power. Each operation must be checking against all relevant predicate locks to ensure no conflicts\nIndex-range locks # The root cause of above limitations is that the search conditions associated with predicate locks are too granular causing too many locks are created. The optimization is to lock against less specific conditions.\nIndex-range locks simplify a predicate by making it match a greater set of object. For example, if you have a predicate lock for bookings of room 123 between noon and 1 p.m., you can approximate it by locking bookings for room 123 at any time, or you can approximate it by locking all rooms (not just room 123) between noon and 1 p.m. This is safe, because any write that matches the original predicate will definitely also match the approximations\nIn the room booking system, you may have an index on the room_id, and database uses this index to find existing bookings for room 123. Now the database can simply attach a shared lock to this index entry, indicating that a transaction has searched for bookings of room 123.\nA lock is attached to the index. When a transaction wants to make a write to an entry, it must update the index. In the processing of doing so, it will encounter a shared lock, and it will be forces to wait until the lock is released\nTrade-offs\nIndex-range locks are less precise as predicate locks would be, so they lock a larger range of objects, but it creates less number of locks\nSerializable Snapshot Isolation # Pessimistic Versus Optimistic Concurrency Control # Two-phase locking is a so-called pessimistic concurrency control mechanism. Its principle is that if anything might go wrong (as indicated by a lock held by another transaction), it\u0026rsquo;s better to wait until the situation is safe again before doing anything\nSerial execution is pessimistic to the extreme: it is equivalent to each transaction having an exclusive lock on the entire database (or one partition of the database) for the duration of the transaction\nserializable snapshot isolation\nBy contrast, serializable snapshot isolation is an optimistic concurrency control technique. If anything could go wrong, instead of blocking, transactions continue anyway, in the hope that everything will turn out all right. When a transaction wants to commit, the database checks if anything bad happened; if so, the transaction is aborted and has to be retried.\nSSI is based on snapshot isolation \u0026ndash; that is, all reads within the transaction are from a consistent snapshot of the database. This is the main difference compared to optimistic concurrency control technique.\nDecision Based on An Outdated Premise # Write skew pattern in snapshot isolation: a transaction reads some data from the database, examines the result of the query, and decide to take some action based on the results. However, the result from the original query may not be up-to-date by the time the transaction commits, because the data may have been modified in the meantime by other transactions\nHow does the database know if a query result might have changed? There are cases to consider:\nDetecting reads of a stale MVCC object version (uncommitted write occurred before the read)\nDetecting writes that affect prior reads (the write occurs after the right)\nDetecting stale MVCC reads\nSnapshot isolation is usually implemented by multi-version concurrency control (MVCC). When a transaction reads from a consistent snapshot, it ignores writes that were made but not yet committed at the time when the snapshot was taken.\nIn order to prevent this anomaly, the database needs to track writes that were ignored by the transaction due to MVCC visibility rule. When the transaction wants to commit, the database check if any of the ignored writes have now been committed. If so, the transaction must be aborted\nDetecting writes that affect prior reads\nWhen a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the modified data. The process is similar to acquire a write lock, but rather than blocking until the readers have committed, the transaction continues its process and notify other transaction the data they read is not up-to-date after it commits\nPerformance of Serializable Snapshot Isolation # As always, many engineering details affect how well an algorithm works in practice. For example, one trade-off is the granularity at which transactions’ reads and writes are tracked. If the database keeps track of each transaction’s activity in great detail, it can be precise about which transactions need to abort, but the bookkeeping overhead can become significant. Less detailed tracking is faster, but may lead to more transactions being aborted than strictly necessary.\nCompared to two-phase locking, the big advantage of serializable snapshot isolation is that read-only queries can run on a consistent snapshot without requiring any locks, which is very appealing for read-heavy workloads.\n"},{"id":57,"href":"/docs/distributed-system/load-balancing/haproxy/http-sticky-sessions-cookies/","title":"HAProxy: Sticky Sessions Using Cookies","section":"Haproxy","content":" HAProxy: Sticky Sessions Using Cookies # When running multiple backend servers, sticky sessions (also known as session persistence) ensure that a user\u0026rsquo;s requests are always routed to the same server. This is especially useful for apps that store session state in memory (e.g., login sessions, carts, temporary data).\nIn this post, you\u0026rsquo;ll learn how to implement sticky sessions in HAProxy using HTTP cookies.\n🔧 Backend Setup with Python Servers # Use this Python script to spin up identifiable HTTP servers:\n📄 http_server.py\nfrom http.server import BaseHTTPRequestHandler, HTTPServer import sys message = sys.argv[1] if len(sys.argv) \u0026gt; 1 else \u0026#34;Hello\u0026#34; port = int(sys.argv[2]) if len(sys.argv) \u0026gt; 2 else 9001 class Handler(BaseHTTPRequestHandler): def do_GET(self): self.send_response(200) self.send_header(\u0026#34;Content-type\u0026#34;, \u0026#34;text/plain\u0026#34;) self.end_headers() self.wfile.write(f\u0026#34;{message}\\n\u0026#34;.encode()) def log_message(self, format, *args): return if __name__ == \u0026#39;__main__\u0026#39;: print(f\u0026#34;Serving {message} on port {port}\u0026#34;) HTTPServer((\u0026#34;0.0.0.0\u0026#34;, port), Handler).serve_forever() 🧪 Run Two Instances in Two Terminals:\npython3 http_server.py \u0026#34;From Server A\u0026#34; 9001 python3 http_server.py \u0026#34;From Server B\u0026#34; 9002 ⚙️ HAProxy Config for Sticky Sessions # Here’s the haproxy.cfg to enable cookie-based stickiness:\nglobal log stdout format raw daemon defaults mode http log stdout format raw daemon option httplog timeout connect 5s timeout client 10s timeout server 10s frontend http_front bind *:8080 default_backend app_backends backend app_backends cookie SERVERID insert indirect nocache server app1 host.docker.internal:9001 check cookie A server app2 host.docker.internal:9002 check cookie B 🧠 Key Directives:\ncookie SERVERID insert: Tells HAProxy to insert a cookie named SERVERID in the response indirect: Ensures this cookie is not forwarded to the backend nocache: Prevents shared proxies from caching the response with the cookie server ... cookie A: Associates cookie value A with server app1 🧪 Testing It # First Request:\ncurl -i http://localhost:8080/ Returns something like:\nHTTP/1.0 200 OK server: BaseHTTP/0.6 Python/3.9.6 date: Sun, 06 Apr 2025 06:56:24 GMT content-type: text/plain set-cookie: SERVERID=A; path=/ From Server A Second Request (with cookie):\ncurl -i --cookie \u0026#34;SERVERID=A\u0026#34; http://localhost:8080/ You’ll always hit Server A now.\nYou can also open the url http://localhost:8080/ in browser and refresh the page multiple times\n"},{"id":58,"href":"/docs/web/http/http-header-part-3/","title":"HTTP Headers Part 3: Cache-Control","section":"HTTP","content":" 📘 Lesson: HTTP Cache-Control – Concepts and Browser Testing with FastAPI # HTTP caching plays a crucial role in improving performance, reducing latency, and minimizing server load. One of the most versatile headers for managing caching behavior is Cache-Control. In this lesson, we\u0026rsquo;ll explore how it works from both the server and client perspectives, and demonstrate how to test it using FastAPI and Chrome DevTools.\n🔄 Why Cache? # Caching helps eliminate redundant requests to the server by allowing clients or intermediaries to reuse stored responses. It:\nReduces page load times Decreases bandwidth usage Improves scalability and responsiveness 🔑 Understanding Cache-Control # ✅ Server-Side Cache-Control # Direction: Response Purpose: Tells clients, browsers, and CDNs how the response should be cached. Header Example Description Cache-Control: public, max-age=10 Response is cacheable by any cache and remains fresh for 10 seconds. Cache-Control: no-store Response must not be cached or stored by any system. 📥 Client-Side Cache-Control # Direction: Request Purpose: Instructs caches how to treat previously stored responses. Directive Description no-cache Forces revalidation before using cached data. Cached data may still be stored. max-age=0 Immediately treats any cached data as stale and triggers revalidation. no-store Prevents caching of the request and its response entirely. only-if-cached Uses the cached response only; fails if no cached version exists. 🔍 Note: no-cache allows caching but requires validation, whereas no-store forbids storage altogether.\n🛠️ FastAPI Hands-On: Two Endpoints for Caching # 📄 Start Your Server # Change directory to 03-http-headers and create a new Python file cache-control.py:\ncd 03-http-headers touch cache-control.py Paste in the following Python code:\nfrom fastapi import FastAPI, Response from datetime import datetime app = FastAPI() @app.get(\u0026#34;/cached\u0026#34;) def cached_resource(response: Response): response.headers[\u0026#34;Cache-Control\u0026#34;] = \u0026#34;public, max-age=10\u0026#34; return { \u0026#34;message\u0026#34;: \u0026#34;This response is cached for 10 seconds.\u0026#34;, \u0026#34;timestamp\u0026#34;: datetime.utcnow().isoformat() } @app.get(\u0026#34;/no-store\u0026#34;) def no_store_resource(response: Response): response.headers[\u0026#34;Cache-Control\u0026#34;] = \u0026#34;no-store\u0026#34; return { \u0026#34;message\u0026#34;: \u0026#34;This response is never cached.\u0026#34;, \u0026#34;timestamp\u0026#34;: datetime.utcnow().isoformat() } Use Uvicorn to start your FastAPI app:\nuvicorn cache-control:app --reload 🧪 Test It Out Using the Browser Console # You can test caching behavior directly in the browser using Chrome DevTools:\nOpen Chrome DevTools (F12) Go to the Console tab Use the fetch() API to simulate HTTP requests with custom Cache-Control headers. 🔁 Test with max-age=0 # Paste below commands in the Console tab:\nfetch(\u0026#34;http://localhost:8000/cached\u0026#34;).then(res =\u0026gt; res.json()).then(console.log); fetch(\u0026#34;http://localhost:8000/cached\u0026#34;).then(res =\u0026gt; res.json()).then(console.log); fetch(\u0026#34;http://localhost:8000/cached\u0026#34;, { headers: { \u0026#34;Cache-Control\u0026#34;: \u0026#34;max-age=0\u0026#34; } }).then(res =\u0026gt; res.json()).then(console.log); The first two responses should show same timestamp, because the second request is returned by the browser\u0026rsquo;s cache. Header max-age=0 makes the cache stale, and the request is sent to the server for validation. Because our server code doesn\u0026rsquo;t do validation, new response instead of cache is returned. You should only see two requests logs in the server terminal, and they are from the first and the third requests.\nBrowser\u0026rsquo;s responses:\n{message: \u0026#39;This response is cached for 10 seconds.\u0026#39;, timestamp: \u0026#39;2025-04-13T06:48:13.860974\u0026#39;} {message: \u0026#39;This response is cached for 10 seconds.\u0026#39;, timestamp: \u0026#39;2025-04-13T06:48:13.860974\u0026#39;} {message: \u0026#39;This response is cached for 10 seconds.\u0026#39;, timestamp: \u0026#39;2025-04-13T06:48:13.865528\u0026#39;} Server terminal responses:\nINFO: 127.0.0.1:57927 - \u0026#34;GET /cached HTTP/1.1\u0026#34; 200 OK INFO: 127.0.0.1:57927 - \u0026#34;GET /cached HTTP/1.1\u0026#34; 200 OK 🚫 Test with no-store # Paste below commands in the Console tab:\nfetch(\u0026#34;http://localhost:8000/no-store\u0026#34;).then(res =\u0026gt; res.json()).then(console.log); fetch(\u0026#34;http://localhost:8000/no-store\u0026#34;).then(res =\u0026gt; res.json()).then(console.log); Because header no-store is set by the server, the response won\u0026rsquo;t be cached by the browser. Hence, every request is sent to the server. You should see two different timestamps in the responses and two request logs in the server terminal.\nBrowser\u0026rsquo;s responses:\n{message: \u0026#39;This response is never cached.\u0026#39;, timestamp: \u0026#39;2025-04-13T06:49:49.211935\u0026#39;} {message: \u0026#39;This response is never cached.\u0026#39;, timestamp: \u0026#39;2025-04-13T06:49:49.216012\u0026#39;} Server terminal responses:\nINFO: 127.0.0.1:58336 - \u0026#34;GET /no-store HTTP/1.1\u0026#34; 200 OK INFO: 127.0.0.1:58336 - \u0026#34;GET /no-store HTTP/1.1\u0026#34; 200 OK "},{"id":59,"href":"/docs/cloud/docker/fundamentals/lesson-4-dockerfile/","title":"Lesson 4: Dockerfile","section":"Fundamentals","content":" 🧠 Docker Mastery Course – Lesson 4 # 🏗️ Building Your Own Image with a Dockerfile # Welcome to Lesson 4 of the Docker Mastery Course! In this lesson, you\u0026rsquo;ll learn how to create your own Docker image using a Dockerfile, understand how file paths are interpreted during the build process, and explore the difference between shell and exec forms of the CMD instruction.\n🎯 Goal # By the end of this lesson, you’ll be able to:\nUnderstand the purpose and structure of a Dockerfile Build and tag a custom Docker image Interpret relative paths in Docker build context Run your custom image using both best practices and practical understanding 📚 Concepts # What Is a Dockerfile? # A Dockerfile is a plain text file containing a sequence of instructions that tell Docker how to build an image. Each instruction creates a new layer in the final image. Dockerfiles are essential for defining reproducible, portable containers.\nCommon Dockerfile Instructions # Instruction Purpose FROM Specifies the base image (e.g., python:3.11) WORKDIR Sets the working directory inside the container COPY Copies files/folders from the build context into the image RUN Executes commands during image build time CMD Sets the default command to run when the container starts ENV Defines environment variables EXPOSE Documents which port the container will listen on ENTRYPOINT Sets a fixed command that runs when the container starts ✅ For this lesson, we\u0026rsquo;ll focus on the most essential instructions: FROM, WORKDIR, COPY, and CMD.\n💻 Hands-On Practice # ✅ Step 1: Create a Simple Python App # Start by creating a directory for your project and saving the following Python file as app.py:\nprint(\u0026#34;Hello from my custom Docker image!\u0026#34;) ✅ Step 2: Write a Dockerfile # In the same directory, create a file named Dockerfile and add the following:\n# Use a base image FROM python:3.11 # Set working directory inside the container WORKDIR /app # Copy source code into the image COPY app.py . # Set the default startup command (exec form) CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] Explanation:\nCOPY app.py . copies app.py from your local system into the /app directory inside the image. The local path is relative to the build context, which you’ll define during the docker build command. CMD [\u0026quot;python\u0026quot;, \u0026quot;app.py\u0026quot;] is written in exec form, which runs the command directly without using a shell. This form avoids common pitfalls, handles signals better, and is preferred for most containerized applications. ✅ Step 3: Build the Image # In your terminal, navigate to the folder containing your Dockerfile, then run:\ndocker build -t my-python-app . -t my-python-app tags the image so you can refer to it by name later. . specifies the build context — everything in the current directory is accessible during the build. Verify your image exists:\ndocker images ✅ Step 4: Run the Container # Now run the image as a container:\ndocker run my-python-app Expected output:\nHello from my custom Docker image! 🏁 Checkpoint # At this point, you’ve:\nWritten and structured a Dockerfile Built a custom Docker image from your local code Understood how COPY and WORKDIR affect file placement inside the container Used exec form in CMD for better performance and control 🎉 Congratulations — you’ve built and run your very first Dockerized app!\n📆 What’s Next? # Lesson 5: Understanding Docker Networking You’ll learn how Docker containers communicate, how to expose ports to the host system, and how to connect multiple containers with Docker networks.\nStay tuned!\n"},{"id":60,"href":"/docs/programming-core/java/nio/selector-read/","title":"Selector Read","section":"Nio","content":" Read Operation in Selector # In last post, we discussed Selector and how it works, also introduced an interested operation OP_ACCEPT. In this post, I will introduce another interested operation OP_READ and how to gracefully read data from SocketChannel and handle closed socket channels\nServer Code # @Slf4j public class Server { public static void main(String[] args) throws IOException { ByteBuffer buffer = ByteBuffer.allocate(32); Selector selector = Selector.open(); log.debug(\u0026#34;Create a selector\u0026#34;); ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.bind(new InetSocketAddress(9999)); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT, null); log.debug(\u0026#34;Register ServerSocketChannel with selector\u0026#34;); while (true) { Print.printDelimiter(); log.debug(\u0026#34;Listen to events from selection keys\u0026#34;); selector.select(); Set\u0026lt;SelectionKey\u0026gt; selectionKeys = selector.selectedKeys(); log.debug(\u0026#34;Selected {} key(s)\u0026#34;, selectionKeys.size()); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = selectionKeys.iterator(); while(keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); sc.configureBlocking(false); log.debug(\u0026#34;Connected to client {}\u0026#34;, sc.getRemoteAddress()); SelectionKey scKey = sc.register(selector, 0, null); scKey.interestOps(SelectionKey.OP_READ); log.debug(\u0026#34;Register SocketChannel with selector\u0026#34;); } else if (key.isReadable()) { SocketChannel channel = (SocketChannel) key.channel(); log.debug(\u0026#34;Read from client {}\u0026#34;, channel.getRemoteAddress()); channel.read(buffer); buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); } keyIterator.remove(); } } } } Compared with server code in post, the main change is inside the second while loop. Let\u0026rsquo;s analyze line by line to see the difference.\nConnect to clients\nif (key.isAcceptable()) { ServerSocketChannel channel = (ServerSocketChannel) key.channel(); SocketChannel sc = channel.accept(); ... } Here I first check if the key is an acceptable operation. If so, then I cast the channel from the selected key to ServerSocketChannel and let server accepts client\u0026rsquo;s connection Register read operation for ServerSocket\nif (key.isAcceptable()) { ... sc.configureBlocking(false); SelectionKey scKey = sc.register(selector, 0, null); scKey.interestOps(SelectionKey.OP_READ); } After creating a new SocketChannel, I register read operation for the channel with the selector by invoking SelectionKey.interestOps(int ops) method A new SelectionKey is added into keys Client sends messages\nelse if (key.isReadable()) { SocketChannel channel = (SocketChannel) key.channel(); log.debug(\u0026#34;Read from client {}\u0026#34;, channel.getRemoteAddress()); channel.read(buffer); buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); } Assume a client sends messages to the server. In that case, the SocketChannel key is selected and added to selectedKeys The data is read from the channel to the ByteBuffer and messages are printed in the terminal Demo # Start the server 23:25:01.524 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Create a selector 23:25:01.531 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register ServerSocketChannel with selector ---------------------------------------------------- 23:25:01.531 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys A client connects 23:25:25.658 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 23:25:25.660 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47346 23:25:25.660 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register SocketChannel with selector ---------------------------------------------------- 23:25:25.660 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys The client sends messages 23:26:32.911 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 23:26:32.911 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Read from client /127.0.0.1:47346 23:26:32.911 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello Server! ---------------------------------------------------- 23:26:32.912 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys Open Questions: Why need to remove selected keys? # keyIterator.remove(); select method only adds items to SelectedKeys, but it doesn\u0026rsquo;t remove items. Hence, each time after handling event, we need to remove it. That\u0026rsquo;s why I use Iterator instead of for loop here. The better explain the problem if items are not removed, let\u0026rsquo;s comment the remove code and run above demo again.\nStart the server\n20:44:26.948 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Create a selector 20:44:26.955 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register ServerSocketChannel with selector ---------------------------------------------------- 20:44:26.956 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys When starting the server, there is one key created, but no keys is added to SelectedKeys Client connects\n21:16:52.573 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 21:16:52.582 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47438 21:16:52.582 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register SocketChannel with selector ---------------------------------------------------- 21:16:52.582 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys A client connects to the server, the ssc key is added to SelectedKeys. select method is not blocked. A new ServerSocket is created and a new SelectionKeys is registered with the selector Cause this time we didn\u0026rsquo;t remove the ssc key, so it is still in the SelectedKeys Client sends messages\n21:23:20.865 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 2 key(s) Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at com.whatsbehind.netty_.nio.selector_.Server.main(Server.java:39) The client sends a message to the server, the sc (server socket) key is added to SelectedKeys, and two keys are selected When handle Accept event in the ssc key, this time there is no connection from clients, and cause the ServerSocketChannel is set as non-blocking mode, in line #39, what channel.accept() returns is null. Hence line #39 throws NullPointerException #37 ServerSocketChannel channel = (ServerSocketChannel) key.channel(); #38 SocketChannel sc = channel.accept(); #39 sc.configureBlocking(false); Clients disconnect # When clients disconnect, no matter it is normal SocketChannel close or exit with code -1, the connected SocketChannelin the server will receive a read event, but length of the message body is 0. Hence, current implementation channel.read(buffer); can\u0026rsquo;t handle client disconnection properly, and selector will keep selecting key of that channel and it causes infinite loop.\nMethod channel.read(buffer) returns an int value which is length of the byte read from the channel. When the read event is client disconnection, the method returns -1, so we can modify our code like below\nSocketChannel channel = (SocketChannel) key.channel(); int len = channel.read(buffer); if (len == -1) { key.cancel(); } else { buffer.flip(); ByteBufferReader.readAll(buffer); buffer.clear(); } We explicitly check the returned int from channel.read(buffer). If the value is -1, then we invoke key.cancel() method to deregister the SocketChannel from the selector After cancel, the selector stops listening to event from that channel, in other words, the key is removed from the keys Let\u0026rsquo;s take a look at the execution results:\n09:57:38.118 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Create a selector 09:57:38.128 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register ServerSocketChannel with selector ---------------------------------------------------- 09:57:38.129 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys 09:57:40.136 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 09:57:40.137 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Connected to client /127.0.0.1:47628 09:57:40.137 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Register SocketChannel with selector ---------------------------------------------------- 09:57:40.137 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys 09:57:42.913 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Selected 1 key(s) 09:57:42.913 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Read from client /127.0.0.1:47628 09:57:42.913 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Client /127.0.0.1:47628 is closed ---------------------------------------------------- 09:57:42.914 [main] DEBUG com.whatsbehind.netty_.nio.selector_.Server - Listen to events from selection keys "},{"id":61,"href":"/docs/networking/subnet/","title":"Subnet","section":"Networking","content":" 🧱 Introduction to Subnetting # Subnetting is the process of dividing a larger IP network into smaller, more manageable sub-networks — called subnets. It helps organize networks, improve routing efficiency, and control IP allocation.\nIf CIDR gives us flexible blocks of IPs, subnetting is how we slice those blocks up.\n🔹 Why Subnet? # Separate traffic by department, app, or security level Reduce broadcast domain size Plan efficient IP usage (e.g., don’t give 254 IPs to a team that only needs 10) 📐 Concrete Example: Subnetting a /24 # Let’s say you have the following CIDR block:\n192.168.1.0/24 This gives:\n256 total IPs 254 usable IPs (after reserving network and broadcast addresses) You want to create 4 subnets of equal size.\n🔁 Step 1: Borrow Bits\n/24 = 8 host bits (32 total - 24 prefix bits) To make 4 subnets, you need 2 more bits → new prefix = /26 This gives 2⁶ = 64 IPs per subnet 📊 Step 2: Calculate the Subnets\nSubnet Range Broadcast Usable Hosts 192.168.1.0/26 192.168.1.0 – .63 .63 .1 – .62 192.168.1.64/26 192.168.1.64 – .127 .127 .65 – .126 192.168.1.128/26 192.168.1.128 – .191 .191 .129 – .190 192.168.1.192/26 192.168.1.192 – .255 .255 .193 – .254 Each subnet:\nHas 64 IPs total 62 usable IPs (excluding .0 and .63, etc.) "},{"id":62,"href":"/docs/programming-core/java/thread/thread-safety/","title":"Thread Safety","section":"Thread","content":" Example and Best Practice # Shared Mutable State # Unsafe Use public class Counter { private int count = 0; public void increment() { count++; } public int getCount() { return count; } } Better Practice public class Counter { private int count = 0; public synchronized void increment() { count++; } public synchronized int getCount() { return count; } } - Adding `synchronized` on the methods ensure each method "},{"id":63,"href":"/docs/web/web-securities/tls-handshake-server-hello/","title":"TLS Handshake: Server Hello","section":"Web Securities","content":" TLS Handshake: Server Hello # After a client sends the ClientHello message to initiate a secure connection, the server responds with ServerHello. This message finalizes the parameters needed to establish an encrypted session, and it\u0026rsquo;s the server\u0026rsquo;s way of saying:\n\u0026ldquo;Here’s what I’ve selected from your offers. Let’s proceed securely.\u0026rdquo;\nIn this post, we’ll break down the ServerHello message, how to view it, and what its key components mean.\nStep 1: Capture ServerHello Using OpenSSL # To view the TLS handshake, including ServerHello, use:\nopenssl s_client -connect www.google.com:443 -servername www.google.com -msg Once connected, look for:\n\u0026lt;\u0026lt;\u0026lt; TLS 1.3 Handshake [length ****], ServerHello To analyze the hex data, first store it in a file called server_hello.hex:\n01 00 01 2d 03 03 a1 c6 ed 3f 78 88 21 0c 32 40 ... Now, use Python to parse it. Create file tls-parser.py:\nfrom scapy.all import * load_layer(\u0026#34;tls\u0026#34;) with open(\u0026#34;server_hello.hex\u0026#34;, \u0026#34;r\u0026#34;) as file: hex_str = file.read().replace(r\u0026#34;\\s+\u0026#34;, \u0026#34;\u0026#34;) pkt = TLSServerHello(bytes.fromhex(hex_str)) pkt.show() Run the Python script to view the ServerClient message:\npython3 tls-parser.py Step 2: Key Contents of ServerHello (TLS 1.3) # Here’s a real parsed example and what each field means:\nField Purpose Version The version of TLS chosen by the server (usually TLS 1.3) Random Bytes Server-generated random number, used in key generation Session ID Optional field used for session resumption (32 bytes here) Cipher Suite The encryption algorithm selected by the server (e.g., CHACHA20_POLY1305_SHA256) Extensions Additional fields, described below Extensions in ServerHello\nTLS 1.3 simplifies and minimizes the extensions in ServerHello. Here\u0026rsquo;s what we typically see:\n🔐 Key Share\nContains the server\u0026rsquo;s public key and the group (e.g., x25519) used to generate it. Used with the client’s key to derive the shared symmetric key for encryption. 🔢 Supported Versions\nIndicates that the server selected TLS 1.3. This confirms the TLS version chosen from the client’s supported list. Example: TLS 1.3 ServerHello\n###[ TLS Handshake - Server Hello ]### msgtype = server_hello msglen = 118 version = TLS 1.2 gmt_unix_time= Thu, 02 Jan 2031 02:05:04 (1925085904) random_bytes= 77759f2424f6ad... sidlen = 32 sid = b\u0026#39;\\xe4\\xff\\x88,\\xe6\\xc7r\\xcb...\u0026#39; cipher = TLS_CHACHA20_POLY1305_SHA256 comp = null extlen = 46 \\ext \\ |###[ TLS Extension - Key Share (for ServerHello) ]### | type = key_share | len = 36 | \\server_share\\ | |###[ Key Share Entry ]### | | group = x25519 | | kxlen = 32 | | key_exchange= 02c394022ce15ba... |###[ TLS Extension - Supported Versions (for ServerHello) ]### | type = supported_versions | len = 2 | version = TLS 1.3 Summary # The ServerHello is the server’s response to the client’s handshake offer. It:\nSelects the final TLS version, cipher, and key parameters Sends the server\u0026rsquo;s key share (group and key) so that both sides can compute a shared encryption key Confirms the protocol version with extensions like supported_versions Together with the ClientHello, this message lays the foundation for encrypted HTTPS communication using a symmetric key agreed upon securely during the handshake.\n"},{"id":64,"href":"/docs/web/http/http-header-part-4/","title":"HTTP Headers Part 4: Cache Validation","section":"HTTP","content":" 📘 Lesson: HTTP Cache Validation – ETag and Last-Modified with FastAPI # In the previous lesson, we learned how to control whether and how content is cached using Cache-Control. In this lesson, we focus on cache validation, which allows the server to determine whether a client\u0026rsquo;s cached response is still valid. If so, it can return a lightweight 304 Not Modified instead of resending the full resource.\nWe\u0026rsquo;ll cover how ETag and Last-Modified headers work, how clients use their corresponding validation headers (If-None-Match and If-Modified-Since), and how servers determine which one takes precedence. We\u0026rsquo;ll also walk through a complete FastAPI example and test it using the browser console.\n🔍 Why Validate Cached Content? # Cache validation reduces unnecessary data transfer and accelerates response times. Rather than re-downloading content that hasn\u0026rsquo;t changed, the client asks the server:\n\u0026ldquo;Has this changed since I last received it?\u0026rdquo;\nIf the server determines it has not changed, it returns:\nHTTP/1.1 304 Not Modified This saves bandwidth and improves speed.\n🔗 What Are ETag and Last-Modified? # When responding to a request, a server can include one or both of the following headers to help the client validate the content later:\n🔹 ETag (Entity Tag)\nA unique identifier for the content (usually a hash or version string). If the content changes, the ETag value changes. The client stores this value and uses it in the If-None-Match header on future requests. Example: ETag: \u0026#34;abc123\u0026#34; 🔹 Last-Modified\nA timestamp indicating when the resource was last changed. Sent in the Last-Modified header. The client uses this value in a future If-Modified-Since header. Example: Last-Modified: Wed, 10 Apr 2024 12:00:00 GMT When sending a request, the client can include one or both following headers to allow the server to validate the cache:\n🔸 If-None-Match\nSent by the client to check if the server\u0026rsquo;s current ETag differs from what the client has. If the values match, the server returns 304 Not Modified. Example: If-None-Match: \u0026#34;abc123\u0026#34; 🔸 If-Modified-Since\nSent by the client to check if the resource has changed since the given timestamp. If it has not, the server responds with 304 Not Modified. Example: If-Modified-Since: Wed, 10 Apr 2024 12:00:00 GMT 🔄 How Do These Headers Work Together? # Header Role Sent By Paired With ETag Version ID Server If-None-Match If-None-Match Validator Client ETag Last-Modified Timestamp Server If-Modified-Since If-Modified-Since Validator Client Last-Modified 🧭 Precedence Rules # If both If-None-Match and If-Modified-Since are present: The server must evaluate If-None-Match first. If it matches → return 304 Not Modified If it does not match, evaluate If-Modified-Since If the timestamp is still valid → return 304 Not Modified Otherwise → return new content (200 OK) 💡 ETag is generally more precise than Last-Modified and preferred for fine-grained cache validation.\n🛠️ FastAPI Hands-On Example # In this example, we dynamically generate a response string that includes a counter updated every 10 seconds. The server sets both an ETag (based on content hash) and a Last-Modified timestamp. The client can use either or both to validate the cached content.\nChange directory to 03-http-headers and create a new Python file cache-validation.py:\ncd 03-http-headers touch cache-validation.py Paste in the following Python code:\nfrom fastapi import FastAPI, Request, Response from fastapi.responses import JSONResponse from datetime import datetime import hashlib app = FastAPI() @app.get(\u0026#34;/validate\u0026#34;) def validate_cache(request: Request): # Calculate counter and last_modified from current time now = datetime.utcnow() epoch_seconds = int(now.timestamp()) counter = epoch_seconds // 100 content = f\u0026#34;This is the cached data #{counter}\u0026#34; last_modified = datetime.utcfromtimestamp(counter * 100) etag = hashlib.md5(content.encode()).hexdigest() # 1. ETag validation if_none_match = request.headers.get(\u0026#34;if-none-match\u0026#34;) if if_none_match == etag: return Response( status_code=304, headers={\u0026#34;X-Cache-Validated-By\u0026#34;: \u0026#34;etag\u0026#34;} ) # 2. Last-Modified validation if_modified_since = request.headers.get(\u0026#34;if-modified-since\u0026#34;) if if_modified_since: since = datetime.strptime(if_modified_since, \u0026#34;%a, %d %b %Y %H:%M:%S GMT\u0026#34;) if last_modified \u0026lt;= since: return Response( status_code=304, headers={\u0026#34;X-Cache-Validated-By\u0026#34;: \u0026#34;last-modified\u0026#34;} ) headers = { \u0026#34;ETag\u0026#34;: etag, \u0026#34;Last-Modified\u0026#34;: last_modified.strftime(\u0026#34;%a, %d %b %Y %H:%M:%S GMT\u0026#34;), \u0026#34;Cache-Control\u0026#34;: \u0026#34;public, max-age=0\u0026#34; } return JSONResponse({ \u0026#34;data\u0026#34;: content }, headers=headers) 🧪 Test It Out Using the Browser Console # You can test cache validation directly in the browser using Chrome DevTools:\nOpen Chrome DevTools (F12) Go to the Console tab Step 1: Make an Initial Request\nRun below command in the Console:\nfetch(\u0026#34;http://localhost:8000/validate\u0026#34;).then(res =\u0026gt; { console.log(\u0026#34;ETag:\u0026#34;, res.headers.get(\u0026#34;etag\u0026#34;)); console.log(\u0026#34;Last-Modified:\u0026#34;, res.headers.get(\u0026#34;last-modified\u0026#34;)); console.log(\u0026#34;Validated By:\u0026#34;, res.headers.get(\u0026#34;X-Cache-Validated-By\u0026#34;)); const status = res.status; console.log(\u0026#34;Response Status Code:\u0026#34;, status); if (res.status === 200) { res.json().then(data =\u0026gt; { console.log(\u0026#34;Response Content:\u0026#34;, data); }); } }); Console logs:\nETag: 8fc1ceef992cc55a62591b537d1ec7fa Last-Modified: Mon, 14 Apr 2025 01:30:00 GMT Validated By: etag Response Status Code: 200 Response Content: {data: \u0026#39;This is the cached data #17445942\u0026#39;} The server will:\nReturn 200 status code Return content hash in ETag header Return content modified timestamp in Last-Modified header Step 2: Validate with If-None-Match\nSubstitute \u0026lt;your-etag-value\u0026gt; in below command with ETag from the response in last step:\nfetch(\u0026#34;http://localhost:8000/validate\u0026#34;, { headers: { \u0026#34;If-None-Match\u0026#34;: \u0026#34;\u0026lt;your-etag-value\u0026gt;\u0026#34; } }).then(res =\u0026gt; { console.log(\u0026#34;ETag:\u0026#34;, res.headers.get(\u0026#34;etag\u0026#34;)); console.log(\u0026#34;Last-Modified:\u0026#34;, res.headers.get(\u0026#34;last-modified\u0026#34;)); console.log(\u0026#34;Validated By:\u0026#34;, res.headers.get(\u0026#34;X-Cache-Validated-By\u0026#34;)); const status = res.status; console.log(\u0026#34;Response Status Code:\u0026#34;, status); if (res.status === 200) { res.json().then(data =\u0026gt; { console.log(\u0026#34;Response Content:\u0026#34;, data); }); } }); You should see console logs as below. If the server returns 200, it means that the content has been updated. Rerun same command using ETag value in latest 200 response.\nETag: null Last-Modified: null Validated By: etag Response Status Code: 304 The server will:\nReturn 304 status code Indicate the cache is validated by ETag Step 3: Validate with If-Modified-Since\nSubstitute \u0026lt;your-last-modified-date\u0026gt; in below command with Last-Modified from the response in last step:\nfetch(\u0026#34;http://localhost:8000/validate\u0026#34;, { headers: { \u0026#34;If-Modified-Since\u0026#34;: \u0026#34;\u0026lt;your-last-modified-date\u0026gt;\u0026#34; } }).then(res =\u0026gt; { console.log(\u0026#34;ETag:\u0026#34;, res.headers.get(\u0026#34;etag\u0026#34;)); console.log(\u0026#34;Last-Modified:\u0026#34;, res.headers.get(\u0026#34;last-modified\u0026#34;)); console.log(\u0026#34;Validated By:\u0026#34;, res.headers.get(\u0026#34;X-Cache-Validated-By\u0026#34;)); const status = res.status; console.log(\u0026#34;Response Status Code:\u0026#34;, status); if (res.status === 200) { res.json().then(data =\u0026gt; { console.log(\u0026#34;Response Content:\u0026#34;, data); }); } }); Console logs:\nETag: null Last-Modified: null Validated By: last-modified Response Status Code: 304 The server will:\nReturn 304 status code Indicate the cache is validated by Last-Modified Step 4: Use Both Headers Together\nfetch(\u0026#34;http://localhost:8000/validate\u0026#34;, { headers: { \u0026#34;If-None-Match\u0026#34;: \u0026#34;\u0026lt;your-etag-value\u0026gt;\u0026#34;, \u0026#34;If-Modified-Since\u0026#34;: \u0026#34;\u0026lt;your-last-modified-date\u0026gt;\u0026#34; } }).then(res =\u0026gt; { console.log(\u0026#34;ETag:\u0026#34;, res.headers.get(\u0026#34;etag\u0026#34;)); console.log(\u0026#34;Last-Modified:\u0026#34;, res.headers.get(\u0026#34;last-modified\u0026#34;)); console.log(\u0026#34;Validated By:\u0026#34;, res.headers.get(\u0026#34;X-Cache-Validated-By\u0026#34;)); const status = res.status; console.log(\u0026#34;Response Status Code:\u0026#34;, status); if (res.status === 200) { res.json().then(data =\u0026gt; { console.log(\u0026#34;Response Content:\u0026#34;, data); }); } }); ETag: null Last-Modified: null Validated By: etag Response Status Code: 304 The server will:\nReturn 304 if the ETag matches If not, check timestamp If still valid, return 304 Otherwise, return updated content (200 OK) "},{"id":65,"href":"/docs/programming-core/java/nio/io-model-summary/","title":"I/O  Model Summary","section":"Nio","content":" I/O Model # We have discussed some I/O models, like blocking I/O model and non-blocking I/O model. These two I/O models have their own problems which are low efficient.\nThen I introduce another I/O model Multiplexing which utilizes Selector to monitor registered keys and handle operations in a batch. This model provides high efficiency and handles multiple operations like accept (client connection) and read concurrently.\nIn this post, I will introduce two more I/O models, synchronous and asynchronous.\nSynchronous I/O Model # Definition # In the synchronous I/O model, the thread initiates the input or output operations and then it is blocked until the operation is completed. The thread remains inactive and can not perform any other tasks during this time. The thread resumes its executions after the operation has finished\nKey factors # Only one thread is involved to initiate the operation and get the result The thread is idle or blocked when waiting for the operation completing Linear execution flow Example # Code\npublic static void main(String[] args) throws IOException { FileChannel fc = FileChannel.open(Paths.get(\u0026#34;/path/to/file/data.txt\u0026#34;), StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(16); fc.read(buffer); buffer.flip(); ByteBufferReader.readAll(buffer); } Execution result\n16:18:51.605 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello World! The example of synchronous I/O model is quite straight forward and easy. Only one thread is involved and the thread is blocked at blocking method fc.read(buffer) until data is loaded from the disk and copied to memories\nAsynchronous I/O Model # Definition # In asynchronous I/O model, when a thread initiates an I/O operation, it is not blocked and wait until the operation completes. Instead, the thread continues to execute, and another thread works on the I/O operation. Once the operation is complete, the initiating thread will be notified through mechanism like callbacks\nKey factors # Multiple threads are involved.\nOne thread is called main thread, which initiates the I/O operation, and it is not blocked during the operation. Instead, it continues to execute other tasks. Another thread is called working thread, its main task is to complete the I/O operation and calls back the main thread when the operation is complete. Main thread is not blocked\nConcurrency: Main thread can handle multiple I/O operations concurrently\nExample # Code\npublic static void main(String[] args) throws IOException, InterruptedException { AsynchronousFileChannel afc = AsynchronousFileChannel.open(Paths.get(\u0026#34;/path/to/file/data.txt\u0026#34;), StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(16); log.debug(\u0026#34;Start reading...\u0026#34;); afc.read(buffer, 0, buffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override public void completed(Integer integer, ByteBuffer buffer) { buffer.flip(); ByteBufferReader.readAll(buffer); } @Override public void failed(Throwable throwable, ByteBuffer buffer) { throwable.printStackTrace(); } }); log.debug(\u0026#34;Finish reading...\u0026#34;); Thread.sleep(1000); } In afc.read(...) method of the async file channel, I pass in a callback object which has two methods completed and failed which are invoked by the working thread when the read operation completes or throws. I insert three logs before and after the read method and inside the completed method In the end of the code snippet, I force the main thread to sleep 1s. Because the working thread is a daemon thread, it terminates when main thread finishes all tasks. Sleeping 1s allows the work thread to complete Execution result\n16:34:14.824 [main] DEBUG com.whatsbehind.netty_.nio.asynchronous_.CallBackFileReader - Start reading... 16:34:14.826 [main] DEBUG com.whatsbehind.netty_.nio.asynchronous_.CallBackFileReader - Finish reading... 16:34:14.828 [Thread-0] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello World! Two threads are involved, main thread (main) and working thread (Thread-0) Logs Start reading... and Finish reading... are printed first and the thread is main thread. That means after initiating the read operation afc.read(...), the read operation is allocated to the working thread and the main thread is not blocked but continues to execute other tasks The data read from the disk later is printed and its executing thread is the working thread. That means the two threads are running concurrently Is Future asynchronous? # Code\npublic static void main(String[] args) throws IOException, InterruptedException, ExecutionException { AsynchronousFileChannel afc = AsynchronousFileChannel.open(Paths.get(\u0026#34;/path/to/file/data.txt\u0026#34;), StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(16); log.debug(\u0026#34;Start reading...\u0026#34;); Future\u0026lt;Integer\u0026gt; future = afc.read(buffer, 0); // Blocking method: main thread is blocked here until read operation completes future.get(); buffer.flip(); ByteBufferReader.readAll(buffer); log.debug(\u0026#34;Finish reading...\u0026#34;); Thread.sleep(1000); } Instead of utilizing callbacks to notify the main thread. We can use Future object to get the data after the read operation After calling afc.read(...), I invoke a blocking method future.get() to let the main thread wait here until the read operation completes. During this time, the working thread is working on the read operation and the main thread is blocked Execution result\n16:46:47.948 [main] DEBUG com.whatsbehind.netty_.nio.asynchronous_.FutureFileReader - Start reading... 16:46:47.952 [main] DEBUG com.whatsbehind.netty_.utility.ByteBufferReader - Hello World! 16:46:47.952 [main] DEBUG com.whatsbehind.netty_.nio.asynchronous_.FutureFileReader - Finish reading... The above code works as synchronous I/O model. Codes are executed like a linear way and all logs are printed in the main thread Is it asynchronous?\nIn my opinion, NO! This way has two threads involved, but comparing with synchronous I/O model, main thread only transfers its read job to the working thread. Main thread is still blocked at method future.get(), and it can\u0026rsquo;t execute other tasks. Further more, comparing with synchronous I/O model, this way has one more thread involved but doesn\u0026rsquo;t allow main to execute multiple tasks concurrently.\n"},{"id":66,"href":"/docs/cloud/docker/fundamentals/lesson-5-networking/","title":"Lesson 5: Docker Networking","section":"Fundamentals","content":" 🧰 Docker Mastery Course – Lesson 5 # 📡 Understanding Docker Networking # Welcome to Lesson 5 of the Docker Mastery Course! In this lesson, you\u0026rsquo;ll learn how Docker networking works, how to expose container services, and how containers communicate with each other and with the outside world.\n🎯 Goal # By the end of this lesson, you’ll understand how to:\nExpose container ports to your host machine Use Docker bridge networks Connect multiple containers Resolve container names using Docker DNS Access public internet domains from within containers 📚 Concepts # What Is Docker Networking? # Docker creates virtual networks that allow containers to:\nCommunicate with each other securely Be isolated from or connected to external systems Default Docker Network Types # Network Mode Description bridge Default for user-created containers; provides isolation and DNS resolution within the same network host Shares the host’s network stack (Linux only) none Disables all networking overlay Connects containers across multiple Docker hosts (advanced use cases) This lesson focuses on the bridge network, the default and most commonly used setup.\n📁 Hands-On Practice # ✅ Step 1: Expose a Container to the Host # Run a public-facing container and map its internal port to your machine:\ndocker run -d --name mynginx -p 8080:80 nginx -d: runs the container in detached (background) mode --name mynginx: names the container -p 8080:80: maps host port 8080 to container port 80 Visit http://localhost:8080 in your browser to view the Nginx welcome page.\nTo clean up:\ndocker stop mynginx docker rm mynginx ✅ Step 2: Create a Custom Docker Network # Create a user-defined bridge network:\ndocker network create demo-net Verify it exists:\ndocker network ls A custom network like this allows containers to communicate by name through Docker’s internal DNS resolver.\n✅ Step 3: Connect Two Containers by Name # Start an Nginx container in the custom network:\ndocker run -d --name web --network demo-net nginx -d: runs the container in detached mode --name web: names the container \u0026ldquo;web\u0026rdquo; --network demo-net: connects the container to the custom network nginx: the image being used Now run an Alpine container in the same network and ping the Nginx container:\ndocker run -it --rm --network demo-net alpine ping web -it: opens an interactive terminal session --rm: automatically removes the container after it exits --network demo-net: connects the container to the same network alpine: the image used (a lightweight Linux distro) ping web: runs the ping command inside the container, using web as the hostname 🧠 Why does this work? When containers are on the same user-defined network, Docker injects a custom DNS configuration. This allows containers to resolve each other by name. In this case, web is resolved to the internal IP address of the Nginx container.\n✅ Step 4: Ping a Public Domain from a Container # Containers can access the internet by default:\ndocker run -it --rm alpine ping google.com Docker uses NAT (Network Address Translation) to route outbound traffic Public DNS queries like google.com are forwarded to your host’s DNS resolver This works even on the default bridge network ❗ Containers can access the internet by default, but the internet cannot access containers unless you explicitly expose ports (-p) or configure a reverse proxy.\n🏃️ Checkpoint # By now, you’ve learned how to:\nExpose container services to your host Create and use custom Docker networks Connect containers and communicate using container names Understand how Docker\u0026rsquo;s DNS system works Access external resources from inside a container 🎉 Great job! You’re now equipped to build multi-container apps that communicate efficiently and securely.\n📆 What’s Next? # Lesson 6: Data Persistence with Volumes Learn how to manage persistent data in Docker, make data survive container restarts, and understand the difference between volumes and bind mounts.\nStay tuned!\n"},{"id":67,"href":"/docs/web/web-securities/tls-handshake-certificate/","title":"TLS Handshake: Certificate","section":"Web Securities","content":" TLS Handshake: Certificate # What is a TLS Certificate? # During the TLS handshake, the server sends its certificate to the client to prove its identity. This certificate contains the server’s public key and information about who issued the certificate (a Certificate Authority). The client uses this certificate to verify that it\u0026rsquo;s really talking to the correct server, and then uses the public key to help create a secure, encrypted connection.\nThe whole purpose of Certificate is to authenticate server identity\nView the Server Certificate # Use OpenSSL to connect to a server and retrieve its certificate:\nopenssl s_client -connect www.google.com:443 -servername www.google.com In the output, look for the section that starts with:\n-----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- This is the server’s TLS certificate, including -----BEGIN CERTIFICATE----- and -----END CERTIFICATE-----.\nSave server cert to file server.crt:\nopenssl s_client -connect www.google.com:443 -servername www.google.com \u0026lt;/dev/null | sed -n \u0026#39;/-----BEGIN CERTIFICATE-----/,/-----END CERTIFICATE-----/p\u0026#39; \u0026gt; server.crt You can inspect the certificate\u0026rsquo;s contents using:\nopenssl x509 -in server.crt -text -noout Key fields in the certificate include:\nSubject: The domain the certificate is for Issuer: Who issued the certificate (a CA) Validity: Start and end dates Public Key: Used in the handshake to establish secure communication Signature: A hash of the certificate data encrypted by the issuer\u0026rsquo;s private key Decoded server certificate example\nCertificate: Data: Version: 3 (0x2) Serial Number: b9:32:9c:bd:a9:ef:7f:44:12:09:8e:72:2a:68:45:d3 Signature Algorithm: sha256WithRSAEncryption Issuer: C=US, O=Google Trust Services, CN=WR2 Validity Not Before: Mar 20 11:20:31 2025 GMT Not After : Jun 12 11:20:30 2025 GMT Subject: CN=www.google.com Subject Public Key Info: Public Key Algorithm: id-ecPublicKey Public-Key: (256 bit) pub: 04:a7:a6:c6:52:... X509v3 extensions: ... Signature Algorithm: sha256WithRSAEncryption 2c:40:49:11:97... Chain of Certificates # A full certificate chain typically includes:\nLeaf Certificate: The server’s certificate (e.g., server.crt) Intermediate CA Certificates: Certificates used to sign the leaf certificate Root Certificate: Trusted by operating systems or browsers View the Full Certificate Chain Use the following command to view the entire certificate chain:\nopenssl s_client -connect www.google.com:443 -servername www.google.com -showcerts \u0026lt;/dev/null Save Certificates Separately From the output:\nSave the first certificate block (from -----BEGIN CERTIFICATE----- to -----END CERTIFICATE-----) into a file named server.crt. This is the leaf certificate. Save the remaining certificates into a file named chain.pem. These are the intermediate certificates. Download Root Certificates\nDownload Google\u0026rsquo;s root certificates:\ncurl -o roots.pem https://pki.goog/roots.pem Then append them to the chain file:\necho \u0026gt;\u0026gt; chain.pem \u0026amp;\u0026amp; cat roots.pem \u0026gt;\u0026gt; chain.pem Verify the Certificate Chain\nNow verify the certificate chain:\nopenssl verify -CAfile chain.pem server.crt If the chain is complete and valid, you\u0026rsquo;ll see:\nserver.crt: OK How is One Certificate Verified? # When a client verifies a certificate, it needs to make sure the certificate was truly issued by a trusted authority and hasn\u0026rsquo;t been tampered with. Here\u0026rsquo;s how it works:\nUse the issuer\u0026rsquo;s public key (from the next certificate in the chain) to decrypt the signature found in the certificate being verified. This reveals a hash value. Independently compute a hash of the certificate\u0026rsquo;s \u0026ldquo;To Be Signed\u0026rdquo; (TBS) section using the same hash algorithm. Compare the two hashes: If they match, the certificate is confirmed to be authentic and untampered. This method works recursively: the client starts by verifying the server’s certificate using the intermediate certificate’s public key, then verifies the intermediate using the next certificate in the chain, and so on until it reaches a root certificate that is already trusted by the client (pre-installed in the OS or browser).\n"},{"id":68,"href":"/docs/web/http/cookie/","title":"Cookie","section":"HTTP","content":" What is cookie # An HTTP cookie (web cookie, browser cookie) is a small piece of data that a server sends to a user\u0026rsquo;s web browser. The browser may store the cookie and send it back to the same server with later requests. Typically, an HTTP cookie is used to tell if two requests come from the same browser—keeping a user logged in, for example. It remembers stateful information for the stateless HTTP protocol.\nCookies are created by server Cookies are stored in client\u0026rsquo;s browser Cookies are sent with every request to the same domain (Explain more below) Deprecation of cookie # Cookies were once used for general client-side storage. While this made sense when they were the only way to store data on the client, modern storage APIs are now recommended. Cookies are sent with every request, so they can worsen performance (especially for mobile data connections). Modern APIs for client storage are the Web Storage API (localStorage and sessionStorage) and IndexedDB.\nUse case of cookie # Session management Logins, shopping carts, game scores, or anything else the server should remember\nPersonalization User preferences, themes, and other settings\nYou visited a weather website and entered your address to get weather information. The server set an address cookie under the domain of the weather website. Next time when you revisit the website, the address cookie will be sent in the request\u0026rsquo;s header, and the website will show weather information of same address. Tracking Recording and analyzing user behavior (ADs)\nHow to set cookies # Cookies can be set and modified at the server level using\nthe Set-Cookie HTTP header, or with JavaScript using document.cookie. After receiving an HTTP request, a server can send one or more Set-Cookie headers with the response. The browser usually stores the cookie and sends it with requests made to the same server inside a Cookie HTTP header.\nSet-Cookie and Cookie headers # The Set-Cookie HTTP response header sends cookies from the server to the user agent. A simple cookie is set like this:\nSet-Cookie: \u0026lt;cookie-name\u0026gt;=\u0026lt;cookie-value\u0026gt; Example:\nHTTP/2.0 200 OK Content-Type: text/html Set-Cookie: yummy_cookie=choco Set-Cookie: tasty_cookie=strawberry [page content] Define the lifecycle of a cookie # The lifetime of a cookie can be defined in two ways:\nSession cookies are deleted when the current session ends. The browser defines when the \u0026ldquo;current session\u0026rdquo; ends, and some browsers use session restoring when restarting. This can cause session cookies to last indefinitely. Permanent cookies are deleted at a date specified by the Expires attribute, or after a period of time specified by the Max-Age attribute. Example: Set-Cookie: id=a3fWa; Expires=Thu, 31 Oct 2021 07:28:00 GMT; Restrict access to cookies # Secure attribute A cookie with the Secure attribute is only sent to the server with an encrypted request over the HTTPS protocol. HttpOnly attribute A cookie with the HttpOnly attribute is inaccessible to the JavaScript Document.cookie API; it\u0026rsquo;s only sent to the server. For example, cookies that persist in server-side sessions don\u0026rsquo;t need to be available to JavaScript and should have the HttpOnly attribute. This precaution helps mitigate cross-site scripting (XSS) attacks. Example: Set-Cookie: id=a3fWa; Expires=Thu, 21 Oct 2021 07:28:00 GMT; Secure; HttpOnly Define where cookies are sent # The Domain and Path attributes define the scope of a cookie: what URLs the cookies should be sent to.\nDomain attribute # The Domain attribute specifies which hosts can receive a cookie. If the server does not specify a Domain, the browser defaults the domain to the same host that set the cookie, excluding subdomains. If Domain is specified, then subdomains are always included. Therefore, specifying Domain is less restrictive than omitting it. However, it can be helpful when subdomains need to share information about a user.\nFor example, if you set Domain=mozilla.org, cookies are available on domains like\nmozilla.orgdeveloper.mozilla.org If the the cookie was set by server with domain example.com, and no domain attribute was specified. Then the cookie will only be send to domain example.com.\nPath attribute # The Path attribute indicates a URL path that must exist in the requested URL in order to send the Cookie header. The (\u0026quot;/\u0026quot;) character is considered a directory separator, and subdirectories match as well. For example, if you set Path=/docs, these request paths match:\n/docs/docs//docs/Web//docs/Web/HTTP But these request paths don\u0026rsquo;t:\n//docsets/fr/docs TODO # There are still some other attributes and concepts regarding cookie, will dive deeper later is necessary Attach some picture about Set-Cookie and Cookie headers, and attributes of a cookie Dive deeper for HttpOnly attribute Reference # Cookie Definition Nice YouTube video "},{"id":69,"href":"/docs/cloud/docker/fundamentals/lesson-6-volumes/","title":"Lesson 6: Docker Volumes","section":"Fundamentals","content":" 💾 Docker Mastery Course – Lesson 6 # 📦 Data Persistence with Volumes and Bind Mounts # Welcome to Lesson 6 of the Docker Mastery Course! In this lesson, you\u0026rsquo;ll learn how to persist data in Docker containers using volumes and bind mounts, and understand the differences between them.\n🎯 Goal # By the end of this lesson, you’ll be able to:\nUnderstand why container data is ephemeral by default Use volumes to persist data across container lifecycles Use bind mounts to sync data between host and container Explain the differences between volumes and bind mounts clearly Understand how multiple containers can share the same volume Know how Docker handles (or doesn’t handle) concurrent writes 📘 Concepts # 1. Why Data Persistence Matters # Docker containers are designed to be ephemeral — meaning:\nIf a container is deleted or restarted, any data written inside it is lost. This makes it hard to store logs, databases, or application-generated files. To solve this, Docker supports two methods:\nMethod Description Volumes Docker-managed persistent storage. Recommended for most cases. Bind mounts User-specified paths on the host. Great for development workflows. 2. Volume vs Bind Mount: What\u0026rsquo;s the Difference? # Feature Volume Bind Mount Managed by Docker ✅ Yes ❌ No — user defines path Host path visible ❌ Abstracted (Docker manages it) ✅ Directly maps to a host folder Created by docker volume create or automatically User simply provides a path Location /var/lib/docker/volumes/... (Linux/Mac) Anywhere on the host Use case Databases, persistent files Syncing source code in development 🔎 Are bind mounts like symlinks? Not quite — they’re actual kernel-level filesystem mounts. Bind mounts behave like “real-time mirrors” of host folders.\n🔎 Are volumes like managed storage? Yes. Volumes are Docker-controlled storage resources with automatically assigned host paths. You can inspect or back them up, but Docker handles them for you.\n🛠️ Hands-On Practice # ✅ Step 1: Create a Volume # docker volume create mydata Check:\ndocker volume ls docker volume inspect mydata This creates a managed resource with its own storage path.\n✅ Step 2: Use the Volume in a Container # docker run -it --name voltest -v mydata:/data alpine mydata:/data means: mount Docker volume mydata into /data inside the container Inside, run: echo \u0026#34;Hello Volume!\u0026#34; \u0026gt; /data/hello.txt exit Now remove the container:\ndocker rm voltest Start a new container to access the same volume:\ndocker run -it -v mydata:/data alpine cat /data/hello.txt ✅ You’ll see the message again — the data persisted.\n✅ Step 3: Use a Bind Mount # Create a folder on your host:\nmkdir ~/docker-test echo \u0026#34;hello from host\u0026#34; \u0026gt; ~/docker-test/host.txt Run this container:\ndocker run -it -v ~/docker-test:/data alpine ls /data You should see host.txt. This is your local folder mounted into the container at runtime.\n✅ Key Syntax: What does mydata:/data mean? # This is used in the -v option:\nmydata → volume name (Docker-managed) /data → path inside the container So it means: “Mount the Docker volume mydata at /data in the container.”\nFor bind mounts, the syntax looks like:\n-v /host/path:/container/path Example:\n-v ~/project:/app This binds your host’s ~/project folder to the container’s /app directory.\n📍 With volumes, Docker chooses the host path and manages it. With bind mounts, you define the host directory.\n🔄 Volume Sharing and Deletion # 🧩 Can a Volume Be Used by Multiple Containers? # ✅ Yes. A Docker volume can be mounted into multiple containers — they all share the same underlying data.\nExample:\ndocker run -d --name c1 -v mydata:/shared alpine sleep 300 docker run -it --name c2 -v mydata:/shared alpine sh Both containers see the same files inside /shared.\nYou can even mount the same volume at different paths:\ndocker run -v mydata:/app1 alpine docker run -v mydata:/backup alpine 🚫 What Happens If a Volume Is Deleted? # If a volume is in use, Docker will block deletion: docker volume rm mydata # → Error: volume is in use If not in use, deleting the volume removes its data permanently. Anonymous volumes can be removed automatically when containers are deleted using --volumes. ⚠️ Concurrent Writes and Volume Safety # Docker does not coordinate concurrent access to volumes.\nAll containers writing to the same volume access the same underlying filesystem. Docker relies on the host OS filesystem to handle concurrent reads/writes. Scenario Safe? Notes Multiple containers reading ✅ Yes Safe and common One writer, multiple readers ✅ Usually Still depends on app-level logic Multiple writers ❌ Risky May lead to corruption or race issues 🧠 If you have multiple writers, use application-level coordination (e.g., file locks, DBs, queues).\n🏁 Checkpoint # You now know how to:\nCreate and manage Docker volumes Use volumes to persist data between containers Bind host folders into containers for development Distinguish clearly between volumes and bind mounts Share volumes across multiple containers Handle deletion scenarios and avoid data loss Understand Docker’s lack of built-in concurrency control 🎉 Your containers are now capable of retaining, sharing, and coordinating data like full-scale applications.\n📆 What’s Next? # Lesson 7: Docker Compose for Multi-Container Apps You’ll learn how to define multi-service apps using a YAML file and bring up the entire app stack with one command.\nStay tuned!\n"},{"id":70,"href":"/docs/programming-core/java/nio/multiple-threads/","title":"Multiple Threads","section":"Nio","content":"In our previous posts of using Selector for connection and communication (read/write) between server and clients, we only utilize only one thread. Multiplexing is highly efficient, but some time-consumed tasks would affect the overall performance.\nIn this post, I will mimic some functions from Netty which allocates tasks to different threads to improve the performance\nComponent # We will focus on the components in the server side\nBoss # Boss runs under one thread Boss maintains one selector which only listens to ACCEPT events, in other words, boss is only responsible to accept client connections Worker # Worker runs in a separate thread Each worker has one selector which listens to READ and WRITE events After client\u0026rsquo;s connection, the newly created SocketChannel will be registered with one worker Architecture # Connect\nA client connects to the server Boss accepts the connection and creates a SocketChannel Register\nBoss registers the SocketChannel with one worker Worker starts to listen to READ and WRITE events from the newly registered channel Boss is decoupled with the channel Binding After registration, the SocketChannel is bound to the worker, and decoupled from the boss I/O operations including READ and WRITE are monitored and handled by the worker Implementation # Worker # @Getter @Setter @Slf4j public class Worker implements Runnable { private Thread thread; private Selector selector; private String name; private boolean start; public Worker(String name) { this.name = name; } public void register(SocketChannel sc) throws IOException { if (!start) { start = true; selector = Selector.open(); thread = new Thread(this, this.name); thread.start(); log.debug(\u0026#34;Start selector and thread in {}\u0026#34;, name); } sc.configureBlocking(false); sc.register(selector, SelectionKey.OP_READ, null); log.debug(\u0026#34;Register client [{}] with {}\u0026#34;, sc.getRemoteAddress(), name); } @Override public void run() { while (true) { try { log.debug(\u0026#34;{} is listening...\u0026#34;, name); selector.select(); Iterator\u0026lt;SelectionKey\u0026gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); if (key.isReadable()) { SocketChannel sc = (SocketChannel) key.channel(); log.debug(\u0026#34;Reading message from client [{}]\u0026#34;, sc.getRemoteAddress()); ByteBuffer buffer = ByteBuffer.allocate(16); sc.read(buffer); buffer.flip(); ByteBufferReader.readAll(\u0026#34;Message: \u0026#34;, buffer); } } } catch (IOException e) { throw new RuntimeException(e); } } } } Field Selector: Monitors and handles read/write operations from registered channels Thread: Task executor Method run(): Worker implements Runnable. This method only does one thing: selector.select(): Listens to channel\u0026rsquo;s events Handle events register(): First time call: Create the Selector Create and start the thread Register the channel with the Selector Problems: # Let\u0026rsquo;s look at the two main operations of worker:\nmonitor (selector.select()); registration (sc.register(selector, SelectionKey.OP_READ, null);) Monitor is executed by the worker thread, cause method register() is invoked by other thread, so registration is executed by other thread (boss).\nWhile worker thread is blocked at line selector.select(), even though boss registers a new channel, the worker can\u0026rsquo;t start to monitor events from the new channel immediately until next round of monitor\nFix:\n@Getter @Setter @Slf4j public class Worker implements Runnable { private Thread thread; private Selector selector; private String name; private boolean start; + private ConcurrentLinkedQueue\u0026lt;Runnable\u0026gt; tasks = new ConcurrentLinkedQueue\u0026lt;\u0026gt;(); public Worker(String name) { this.name = name; } public void register(SocketChannel sc) throws IOException { if (!start) { start = true; selector = Selector.open(); thread = new Thread(this, this.name); thread.start(); log.debug(\u0026#34;Start selector and thread in {}\u0026#34;, name); } sc.configureBlocking(false); - sc.register(selector, SelectionKey.OP_READ, null); - log.debug(\u0026#34;Register client [{}] with {}\u0026#34;, sc.getRemoteAddress(), name); + tasks.add(() -\u0026gt; { + try { + sc.register(selector, SelectionKey.OP_READ, null); + } catch (IOException e) { + throw new RuntimeException(e); + } + }); + selector.wakeup(); } @Override public void run() { while (true) { try { log.debug(\u0026#34;{} is listening...\u0026#34;, name); selector.select(); + if (!tasks.isEmpty()) { + tasks.poll().run(); + log.debug(\u0026#34;Register client with {}\u0026#34;, name); + } Iterator\u0026lt;SelectionKey\u0026gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); iterator.remove(); if (key.isReadable()) { SocketChannel sc = (SocketChannel) key.channel(); log.debug(\u0026#34;Reading message from client [{}]\u0026#34;, sc.getRemoteAddress()); ByteBuffer buffer = ByteBuffer.allocate(16); ByteBuffer buffer = ByteBuffer.allocate(32); sc.read(buffer); buffer.flip(); ByteBufferReader.readAll(\u0026#34;Message: \u0026#34;, buffer); } } } catch (IOException e) { throw new RuntimeException(e); } } } } I create a Queue as the bridge between two threads\nInstead let other thread execute the registration, I add a Runnable task in the queue, and let the worker thread execute the registration\nTo fix the bug that the selector can\u0026rsquo;t immediately monitor newly registered channels while it is monitoring (blocked at selector.select()), I invoke method selector.wakeup() which unblocks the selector and let it continue to execute codes after adding the task to the queue,\nBefore handling events from selected keys, worker will first check if there are any registration tasks in the queue and do the registration if yes\n"},{"id":71,"href":"/docs/web/web-securities/tls-handshake-certificate-verify/","title":"TLS Handshake: Certificate Verify","section":"Web Securities","content":" TLS Handshake: Certificate Verify # What is CertificateVerify? # During the TLS handshake, the server sends a CertificateVerify message to prove it owns the private key corresponding to the public key in its certificate. This step prevents attackers from using a stolen certificate without having the private key.\nWhat Happens in This Step? # The server creates a hash of all previous handshake messages (called the \u0026ldquo;handshake transcript\u0026rdquo;). The server signs this hash using its private key. The server sends the CertificateVerify message to the client. It includes: The signature algorithm used (e.g., sha256+rsa_pss_rsae) The digital signature value Example:\n###[ TLS Handshake - Certificate Verify ]### msgtype = certificate_verify msglen = 75 \\sig \\ |###[ TLS Digital Signature ]### | sig_alg = sha256+rsaepss | sig_len = 71 | sig_val = ... # The actual signature How Does the Client Verify It? # Extracts the public key from the server\u0026rsquo;s certificate. Decrypts the signature using the public key to recover the hash. Recomputes the hash of the handshake transcript using the specified algorithm. Compares the two hashes: If they match: ✅ the server is verified. If they don’t match: ❌ the handshake fails. What If CertificateVerify Fails? # If the signature can\u0026rsquo;t be verified:\nThe client assumes the server is not trustworthy. The handshake is terminated immediately. No encrypted communication is established. Why It Matters # Even if a malicious actor has a server\u0026rsquo;s certificate, they cannot complete the handshake without the private key. CertificateVerify ensures that the server genuinely controls the private key, making the TLS connection secure.\n"},{"id":72,"href":"/docs/cloud/docker/fundamentals/lesson-7-compose/","title":"Lesson 7: Docker Compose","section":"Fundamentals","content":" 🧰 Docker Mastery Course – Lesson 7 # 🧩 Multi-Container Apps with Docker Compose (FastAPI + Redis) # Welcome to Lesson 7 of the Docker Mastery Course! In this lesson, you\u0026rsquo;ll learn how to orchestrate multi-container applications using Docker Compose, and build a working FastAPI + Redis system.\n🎯 Goal # By the end of this lesson, you’ll be able to:\nDefine multiple services in a docker-compose.yml file Understand how Compose builds images and sets up networks Run, inspect, and tear down multi-service apps Understand container-to-container networking using service names 📘 Concepts # What is Docker Compose? # Docker Compose is a tool for defining and running multi-container Docker applications. It lets you:\nDefine your services in a YAML file Build images from a Dockerfile Spin up everything with a single command: docker-compose up Compose handles networking, service startup order, shared volumes, and more. It replaces the older --link method by automatically creating a dedicated user-defined bridge network where all services can communicate using their service names.\n🛠️ Hands-On Practice: FastAPI + Redis # We’ll build a simple web app that counts how many times a page is visited, using Redis as a backend store.\n✅ Step 1: Project Structure # docker-compose-fastapi/ ├── app.py ├── requirements.txt ├── Dockerfile └── docker-compose.yml ✅ Step 2: app.py # from fastapi import FastAPI import redis app = FastAPI() r = redis.Redis(host=\u0026#39;redis\u0026#39;, port=6379) @app.get(\u0026#39;/\u0026#39;) def read_root(): count = r.incr(\u0026#39;hits\u0026#39;) return {\u0026#34;message\u0026#34;: f\u0026#34;Hello! This page has been viewed {count} times.\u0026#34;} The host='redis' setting works because Compose creates a private DNS entry that maps the redis service name to its container IP. The port 6379 is Redis\u0026rsquo;s default internal port — used inside the container for the Redis server to listen on. Since Redis is only accessed by the FastAPI container within the same Docker network, there\u0026rsquo;s no need to expose it publicly. ✅ Step 3: requirements.txt # fastapi uvicorn[standard] redis ✅ Step 4: Dockerfile # FROM python:3.11-slim WORKDIR /app COPY . . RUN pip install -r requirements.txt CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] This sets up the FastAPI app to run on port 8000 inside the container. ✅ Step 5: docker-compose.yml # version: \u0026#39;3\u0026#39; services: web: build: . ports: - \u0026#34;8000:8000\u0026#34; depends_on: - redis redis: image: redis:alpine The web service is built from the local Dockerfile. The redis service uses the official redis:alpine image from Docker Hub. depends_on ensures Redis starts before FastAPI. Compose automatically creates a dedicated network (e.g., docker-compose-fastapi_default) so the containers can talk to each other by service name. ✅ Step 6: Run It # docker-compose up 🔍 What Happens Under the Hood # When you run docker-compose up, Docker Compose performs the following steps:\nParses the docker-compose.yml:\nIdentifies the defined services: web and redis Builds the image for web (if needed):\ndocker build -t docker-compose-fastapi-web . Pulls the image for redis (if not present):\ndocker pull redis:alpine Creates a dedicated Docker network:\ndocker network create docker-compose-fastapi_default Starts the containers:\ndocker run --name docker-compose-fastapi-web-1 --network docker-compose-fastapi_default -p 8000:8000 ... docker run --name docker-compose-fastapi-redis-1 --network docker-compose-fastapi_default ... Streams logs from both containers to the terminal for live monitoring.\nNow, visit: http://localhost:8000\nYou should see the visit counter increase with each refresh.\nTo stop the application:\ndocker-compose down This command removes the containers and the associated network.\n🧠 Additional Notes # The number of containers created equals the number of services defined. In this case, two: one for web (FastAPI), and one for redis.\nRedis’s 6379 port is its internal port, meaning it\u0026rsquo;s only used within the container. It’s not mapped to the host unless explicitly declared.\nSince Redis is an internal service accessed only by FastAPI, there\u0026rsquo;s no need to expose this port to the host or external traffic.\nDocker Compose sets up an internal network with DNS support, allowing service names like redis to be automatically resolved to their corresponding container IPs.\nCompose uses your project folder name to generate default container and network names. For example:\nContainer: docker-compose-fastapi-web-1 Network: docker-compose-fastapi_default You can override this naming convention using the -p flag:\ndocker-compose -p myproject up Under the hood, Compose orchestrates the equivalent of docker build and docker run commands for each service.\n🏁 Checkpoint # You now know how to:\nBuild and run multi-container apps using Docker Compose Wire together FastAPI and Redis without exposing internal ports Use Compose’s built-in networking and service discovery Understand container lifecycle, port usage, and project naming behavior 🎉 This sets the foundation for more complex, production-ready stacks.\n📆 What’s Next? # Lesson 8: Logging, Healthchecks, and Container Lifecycle You’ll learn how to monitor and manage the health and behavior of running containers.\n"},{"id":73,"href":"/docs/web/web-securities/tls-handshake-certificate-finished/","title":"TLS Handshake: Finished","section":"Web Securities","content":" TLS Handshake: Finished # What is the Finished Message? # After the client verifies the server’s identity (with CertificateVerify), both the client and server send a Finished message. This message is the final step in the handshake that proves everything up to this point has not been tampered with.\nPurpose # The Finished message ensures that:\nAll previous handshake steps were received and processed correctly. The handshake wasn\u0026rsquo;t modified or intercepted by an attacker. Both parties derived the same symmetric encryption key. Final Symmetric Key # By this point in the handshake:\nBoth the client and server have derived the same symmetric key using the earlier exchanged key share and supported group. This key is used to encrypt and decrypt all future HTTPS communication. The Finished message is the first message encrypted with this symmetric key. What Happens in This Step? # The sender (server or client) computes a hash of all previous handshake messages. This hash is then encrypted using the derived symmetric key. The resulting value is sent in the Finished message as verify_data. Example:\n###[ TLS Handshake - Finished ]### msgtype = finished msglen = 32 vdata = ... # encrypted hash of all previous handshake messages How the Finished Message is Verified # When the recipient receives the Finished message:\nThey recompute the hash of all previous handshake messages. They decrypt the received verify_data using the symmetric key. They compare the decrypted hash with the recomputed one. If they match: ✅ handshake is complete. If they don’t match: ❌ the connection is terminated. Why It Matters # The Finished message ensures the entire handshake is valid and hasn’t been tampered with. It’s a critical security step that wraps up the TLS handshake and marks the beginning of secure communication.\n"},{"id":74,"href":"/docs/cloud/docker/fundamentals/lesson-8-health-checks/","title":"Lesson 8: Docker Logging, Healthchecks, and Container Lifecycle","section":"Fundamentals","content":" 🩺 Docker Mastery Course – Lesson 8 # 🚦 Logging, Healthchecks, and Container Lifecycle # Welcome to Lesson 8 of the Docker Mastery Course! In this lesson, you’ll learn how to monitor and control container health and behavior using logging, healthchecks, and lifecycle management tools.\n🎯 Goal # By the end of this lesson, you’ll be able to:\nView and interpret container logs using Docker Define and use healthchecks to monitor container health Understand and manage container lifecycle states: created, running, exited, restarting 📘 Concepts # 1. Viewing Logs # Docker captures stdout and stderr output from container processes. You can access logs with:\ndocker logs \u0026lt;container_name\u0026gt; Example:\ndocker logs docker-compose-fastapi-web-1 To follow logs in real time (like tail -f):\ndocker logs -f docker-compose-fastapi-web-1 2. Healthchecks # A healthcheck is a periodic command Docker runs inside the container to verify that the application is working properly.\nIn your Dockerfile:\nHEALTHCHECK --interval=30s --timeout=10s --retries=3 \\ CMD curl -f http://localhost:8000/ || exit 1 Or in docker-compose.yml:\nservices: web: build: . ports: - \u0026#34;8000:8000\u0026#34; depends_on: - redis healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8000/\u0026#34;] interval: 10s timeout: 5s retries: 3 Docker marks the container with one of the following health statuses:\nstarting – healthcheck hasn\u0026rsquo;t succeeded yet healthy – healthcheck passed unhealthy – healthcheck failed for the configured number of retries ✅ Interpreting Healthcheck Results # Use this command to inspect a container’s health:\ndocker inspect --format=\u0026#39;{{json .State.Health}}\u0026#39; \u0026lt;container\u0026gt; This uses a Go template to return only the .State.Health section in JSON format.\nExample output:\n{ \u0026#34;Status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;FailingStreak\u0026#34;: 0, \u0026#34;Log\u0026#34;: [ ... ] } 🧪 How curl -f Works # curl sends an HTTP request to the given address The -f flag causes curl to fail (exit code ≠ 0) on HTTP 400+ status codes This ensures the healthcheck only passes when a successful response is returned 3. Container Lifecycle States # Docker containers transition through the following lifecycle states:\nState Description created Container exists but hasn’t started running Container is executing exited Container has stopped paused Container is temporarily suspended restarting Container is restarting automatically To check a container’s state:\ndocker ps -a docker inspect \u0026lt;container\u0026gt; 4. Restart Policies # Restart policies determine how Docker responds to container failures.\nExample in docker-compose.yml:\nrestart: always Options:\nno – Do not restart (default) on-failure – Restart only if the container exits with a non-zero code always – Restart regardless of exit status unless-stopped – Always restart unless manually stopped 🛠️ Hands-On Practice # Let’s apply a healthcheck to our FastAPI container.\n✅ Step 1: Update Compose and Dockerfile # 🧱 Updated Dockerfile # Ensure your Docker image includes curl, which is required for the healthcheck to function. Here’s the full Dockerfile:\nFROM python:3.11-slim WORKDIR /app # Install curl for healthchecks RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* COPY . . RUN pip install -r requirements.txt CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] Now, define the healthcheck in your docker-compose.yml:\nservices: web: build: . ports: - \u0026#34;8000:8000\u0026#34; depends_on: - redis healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8000/\u0026#34;] interval: 10s timeout: 5s retries: 3 ✅ Step 2: Rebuild and Start # docker-compose down docker-compose up --build ✅ Step 3: Monitor Logs and Health # docker logs -f docker-compose-fastapi-web-1 docker inspect --format=\u0026#39;{{json .State.Health}}\u0026#39; docker-compose-fastapi-web-1 | jq Note: jq is a powerful command-line JSON processor that beautifies and formats JSON output. You can install it with sudo apt install jq (Linux) or brew install jq (macOS).\nThis will show both the real-time logs and the container’s health status.\n🧩 FAQ # ❓ What’s the difference between Docker lifecycle state and health state? # Lifecycle state reflects Docker’s view of the container (e.g., running, exited, etc.) Health state is application-specific and only available if a healthcheck is defined A container can be running but still be unhealthy if its internal application fails the healthcheck ❓ Does Docker restart a container if its healthcheck fails? # No, not automatically Docker restart policies only apply if the container exits, not if it becomes unhealthy To react to healthcheck failure, you must use external logic or an orchestrator like Kubernetes 🏁 Checkpoint # You now know how to:\nMonitor logs using docker logs Define and apply healthchecks in Dockerfiles and Compose files Inspect healthcheck results using templates Use restart policies effectively Distinguish between lifecycle states and health status You\u0026rsquo;re now prepared to build more observable and reliable Docker containers! ✅\n📆 What’s Next? # Lesson 9: Environment Variables and Secrets Learn how to securely configure applications using .env files and Docker secrets.\n"},{"id":75,"href":"/docs/web/web-securities/simulating-https/","title":"Simulating HTTPS","section":"Web Securities","content":" 📘 Simulating HTTPS with TLS in FastAPI # In this lesson, we’ll simulate a secure HTTPS server using FastAPI and a self-signed TLS certificate. This hands-on setup will help you understand how HTTPS works in practice and how certificates establish trust and enable encryption between clients and servers.\n🔐 What Is HTTPS (TLS)? # HTTPS = HTTP + TLS TLS (Transport Layer Security) encrypts communication between a client and a server. It protects data against eavesdropping, tampering, and man-in-the-middle attacks. TLS requires a certificate to authenticate the server\u0026rsquo;s identity and establish a secure connection. 🛠️ Step 1: Generate a Self-Signed Certificate # We’ll use the openssl command to generate a self-signed certificate and private key. This setup is ideal for local development or learning environments.\n🔍 Command Breakdown\nopenssl req -x509 -newkey rsa:2048 -keyout key.pem -out cert.pem -days 365 -nodes Flag Description req -x509 Generate a certificate instead of a certificate signing request (CSR) -newkey rsa:2048 Create a new RSA private key with 2048-bit encryption -keyout key.pem Output path for the private key file -out cert.pem Output path for the public certificate file -days 365 Validity period of the certificate (365 days) -nodes No password encryption on the private key (simplifies local testing) 📌 When prompted, enter localhost as the Common Name (CN) so the browser recognizes it as a match.\n🚀 Step 2: Create a Simple FastAPI App # create a new python project https:\nmkdir https cd https python3 -m venv venv source venv/bin/activate pip install fastapi uvicorn touch main.py Paste below code to main.py:\nfrom fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/\u0026#34;) def read_root(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello over HTTPS!\u0026#34;} ⚙️ Step 3: Launch FastAPI with HTTPS # Use uvicorn to run your FastAPI app with HTTPS enabled by specifying the key and certificate files.\n🔍 Command Breakdown\nuvicorn main:app --host 0.0.0.0 --port 8443 --ssl-keyfile=key.pem --ssl-certfile=cert.pem Option Description main:app Path to the FastAPI app object (app) in main.py --host 0.0.0.0 Listen on all network interfaces (use 127.0.0.1 for localhost only) --port 8443 Use port 8443 (commonly used for HTTPS in dev environments) --ssl-keyfile Path to the private key (key.pem) --ssl-certfile Path to the public certificate (cert.pem) Once the server is running, navigate to:\nhttps://localhost:8443 ✅ A browser security warning will appear. This is expected because the certificate is self-signed and not trusted by default.\n🧪 Step 4: Test HTTPS with curl # To skip certificate verification, use the -k (insecure) option:\ncurl -k https://localhost:8443 ✅ Expected output:\n{\u0026#34;message\u0026#34;: \u0026#34;Hello over HTTPS!\u0026#34;} 🔍 Step 5: Inspect TLS Handshake with OpenSSL # To view TLS handshake details:\nopenssl s_client -connect localhost:8443 This command outputs:\nTLS version (e.g., TLS 1.2 or 1.3) Cipher suite used Certificate details and validity period You should be able to see the server certificate, which is the certificate in file cert.pem\n"},{"id":76,"href":"/docs/cloud/docker/fundamentals/lesson-9-env/","title":"Lesson 9: Docker Env","section":"Fundamentals","content":" 🔐 Docker Mastery Course – Lesson 9 # 🧪 Environment Variables and Secrets # Welcome to Lesson 9 of the Docker Mastery Course! In this lesson, you\u0026rsquo;ll learn how to inject and manage configuration settings and secrets in your containers using environment variables and .env files.\n🎯 Goal # By the end of this lesson, you will be able to:\nUse environment variables to configure container behavior Load configuration from .env files Access environment variables inside a FastAPI application Recognize the risks of storing secrets in environment variables 📘 Concepts # 1. Why Use Environment Variables? # Environment variables allow you to pass dynamic configuration into containers without modifying application code. They\u0026rsquo;re commonly used for:\nDebug or logging flags Database connection strings API keys and service URLs While they are convenient, they are not recommended for storing sensitive secrets.\n2. Methods for Passing Environment Variables # ✅ a. Inline via the Command Line # docker run -e DEBUG=true myapp ✅ b. Defined in docker-compose.yml # services: web: image: myapp environment: - DEBUG=true - DB_URL=postgres://user:pass@host:5432/db ✅ c. Loaded from a .env File # Create a file named .env:\nDEBUG=true DB_URL=postgres://user:pass@host:5432/db Then reference it in your Compose file:\nservices: web: env_file: - .env Docker Compose will automatically load and inject these variables into the container environment.\n3. Accessing Environment Variables in Python # In a FastAPI application, use the os module to read environment variables:\nimport os GREETING = os.getenv(\u0026#34;GREETING\u0026#34;, \u0026#34;Hello\u0026#34;) # Defaults to \u0026#34;Hello\u0026#34; if not set This ensures your app behaves predictably even if the variable is missing.\n4. Are Environment Variables Secure? # Not entirely:\nThey are stored in plain text within container metadata Accessible via docker inspect, docker exec env, and even logs in some cases ✅ Use environment variables for general configuration ❌ Avoid them for storing sensitive secrets like passwords or tokens\n5. Safer Alternatives for Secrets # For handling secrets securely, consider using:\nDocker Swarm Secrets – encrypted and stored securely (not supported in plain Compose) Kubernetes Secrets – base64-encoded and supported by orchestrators AWS Secrets Manager or SSM Parameter Store Bind mounts – mount a file containing secrets to the container and read at runtime 🛠️ Hands-On Practice: Using Env Vars in FastAPI # ✅ Step 1: Update app.py # Modify your FastAPI application to read from an environment variable:\nfrom fastapi import FastAPI import redis import os app = FastAPI() r = redis.Redis(host=\u0026#39;redis\u0026#39;, port=6379) GREETING = os.getenv(\u0026#34;GREETING\u0026#34;, \u0026#34;Hello\u0026#34;) @app.get(\u0026#39;/\u0026#39;) def read_root(): count = r.incr(\u0026#39;hits\u0026#39;) return {\u0026#34;message\u0026#34;: f\u0026#34;{GREETING}! This page has been viewed {count} times.\u0026#34;} ✅ Step 2: Create a .env File # Add the following content to a new file named .env in your project root:\nGREETING=Hi ✅ Step 3: Update docker-compose.yml # Ensure your Compose file includes the .env reference:\nversion: \u0026#39;3\u0026#39; services: web: build: . ports: - \u0026#34;8000:8000\u0026#34; depends_on: - redis env_file: - .env healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8000/\u0026#34;] interval: 10s timeout: 5s retries: 3 redis: image: redis:alpine ✅ Step 4: Rebuild and Run Your Containers # docker-compose down docker-compose up --build Then visit http://localhost:8000 to confirm that the greeting reflects the value in your .env file.\n✅ Step 5: Customize and Test # Change the .env file to:\nGREETING=Howdy Rebuild and run:\ndocker-compose up --build Check that the output message updates accordingly.\n🧩 Tip: Viewing Environment Variables in a Running Container # To list the environment variables inside a running container:\ndocker exec docker-compose-fastapi-web-1 env To view full metadata:\ndocker inspect docker-compose-fastapi-web-1 ⚠️ Be cautious — this output may expose sensitive values.\n🏁 Checkpoint # You now know how to:\nConfigure containers using environment variables and .env files Access those variables from inside a FastAPI application Understand the limitations of env vars for secure secrets Safely test and modify runtime behavior using Docker Compose You\u0026rsquo;re one step closer to building production-grade, configurable Docker containers. ✅\n📆 What’s Next? # Lesson 10: Multi-Stage Builds and Image Optimization Learn how to reduce Docker image size and accelerate builds using multi-stage Dockerfiles.\n"},{"id":77,"href":"/docs/cloud/docker/fundamentals/lesson-10-multi-stage/","title":"Lesson 10: Docker Multi-Stage","section":"Fundamentals","content":" 🚀 Docker Mastery Course – Lesson 10 # 🧰 Multi-Stage Builds and Image Optimization # Welcome to Lesson 10 of the Docker Mastery Course! In this lesson, you’ll learn how to optimize Docker images using multi-stage builds — a powerful technique for separating build-time and runtime concerns.\n🎯 Goal # By the end of this lesson, you’ll be able to:\nUnderstand what multi-stage builds are and why they matter Reduce image size and improve security Apply multi-stage techniques to your FastAPI application 📘 Concepts # 1. Why Optimize Your Docker Images? # A typical Dockerfile may include:\nSource code Build tools like pip, gcc, curl Temporary files such as .env, tests/, and caches This leads to bloated images that are slower to build, ship, and run — and can pose security risks.\n2. What Is a Multi-Stage Build? # Multi-stage builds let you define multiple FROM instructions within a single Dockerfile. Each FROM starts a new, isolated stage. You can build your application in one stage, then copy only the necessary output into a clean, minimal final image.\nBenefits include:\n🚀 Smaller images – faster pulls, deployments, and fewer vulnerabilities 🔐 Cleaner environments – no leftover dev tools or unused files 💡 Simplicity – one Dockerfile for both dev and production 🛠️ Hands-On Practice: Apply Multi-Stage Build to FastAPI # Let’s refactor your Dockerfile using a multi-stage approach.\n✅ Step 1: Current Dockerfile (Before Optimization) # FROM python:3.11-slim WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* COPY . . RUN pip install -r requirements.txt CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] 🔍 This Dockerfile installs dependencies and runs the app in a single stage — meaning everything (even temporary tools) ends up in the final image.\n✅ Step 2: Refactored Multi-Stage Dockerfile # # Stage 1: Builder FROM python:3.11-slim AS builder WORKDIR /app COPY . . RUN pip install --user -r requirements.txt # Stage 2: Runtime FROM python:3.11-slim WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* COPY --from=builder /root/.local /root/.local COPY app.py . ENV PATH=\u0026#34;/root/.local/bin:$PATH\u0026#34; CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] ✅ Only app.py is copied into the final image — not requirements.txt or other development files. This reduces bloat and minimizes the risk of leaking build-time artifacts into production.\n✅ Step 3: Build and Run # To build the image using Compose:\ndocker-compose down docker-compose up --build 📝 By default, Docker and Compose use the last stage of the Dockerfile as the final image.\n✅ Step 4: Validate the Optimization # Let’s demonstrate the benefit of multi-stage builds with a direct comparison:\nCheck what files are included in the image:\ndocker exec -it docker-compose-fastapi-web-1 sh ls /app ✅ You should only see app.py — not requirements.txt, .env, or any development files. This demonstrates that only explicitly copied files are present in the final image.\n💡 Common Questions # ❓ Is docker-compose build automatically triggered in docker-compose up? # Not by default. You must explicitly add --build:\ndocker-compose up --build Without --build, Compose uses any previously built image.\n❓ Is each FROM a separate stage? # ✅ Yes! Each FROM starts a new build stage. You can name stages using AS and refer to earlier ones using COPY --from=stage_name.\n🏁 Checkpoint # You now know how to:\nSplit Dockerfiles into build and runtime stages Produce leaner and more secure Docker images Apply multi-stage builds to real-world FastAPI apps You’ve leveled up your Docker skills significantly! ✅\n📆 What’s Next? # Lesson 11: .dockerignore and Build Context Learn how .dockerignore and build context affect your image content and build performance.\n"},{"id":78,"href":"/docs/cloud/docker/fundamentals/lesson-11-dockerignore/","title":"Lesson 11: Docker Context \u0026 `.dockerignore`","section":"Fundamentals","content":" 🗂️ Docker Mastery Course – Lesson 11 # 📦 Build Context and .dockerignore # Welcome to Lesson 11 of the Docker Mastery Course! In this lesson, you\u0026rsquo;ll learn how to use the .dockerignore file to optimize the build context, improve build speed, and avoid leaking sensitive files into your Docker images.\n🎯 Goal # By the end of this lesson, you\u0026rsquo;ll be able to:\nUnderstand what Docker\u0026rsquo;s build context is and why it matters Use a .dockerignore file to reduce build size and improve security Prevent unnecessary or sensitive files from being copied into Docker images 📘 Concepts # 1. What Is Build Context? # When you run:\ndocker build . Docker sends everything in the current directory (and subdirectories) to the Docker daemon. This sent content is called the build context.\nAny files sent in the build context can be referenced in COPY or ADD commands inside the Dockerfile.\nProblems with a large build context:\nSlower builds Higher risk of accidentally leaking files like .env or .git Larger Docker images if COPY . . is used 2. What Is .dockerignore? # The .dockerignore file tells Docker to exclude specific files or directories from the build context — similar to how .gitignore works with Git.\nExample .dockerignore:\nDockerfile docker-compose.yml .env .dockerignore ✅ This makes builds faster and images cleaner and safer when using broad COPY commands.\n🛠️ Hands-On Practice: Optimize Build Context in docker-compose-fastapi # ✅ Step 1: Create a .dockerignore File # In the root of your docker-compose-fastapi project, create a .dockerignore file with the following:\n__pycache__/ *.pyc *.pyo *.log .env venv/ tests/ .git/ .dockerignore This prevents test data, logs, Python caches, secrets, and Git metadata from being sent to the Docker daemon.\n✅ Step 2: Use a Single-Stage Dockerfile # FROM python:3.11-slim WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* COPY . . RUN pip install -r requirements.txt CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] This Dockerfile copies the entire project directory using COPY . ., so without .dockerignore, all files would end up in the image.\n✅ Step 3: Build and Run the Image # Build the image and start the container:\ndocker-compose down docker-compose up --build This uses the Dockerfile and .dockerignore file together to build a leaner image.\n✅ Step 4: Inspect Files Inside the Container # Open a shell session in the container:\ndocker exec -it docker-compose-fastapi-web-1 sh ls -a /app ✅ You should see only essential files like:\napp.py requirements.txt ❌ You should not see:\n.env .dockerignore Dockerfile docker-compose.yml ✅ Step 5: (Optional) Check Build Context Size # To observe how much data is being sent:\nDOCKER_BUILDKIT=0 docker build . Look for a line like:\nSending build context to Docker daemon 5.12kB ✅ The smaller this number, the better your .dockerignore is working.\n🏁 Checkpoint # You now understand:\nWhat the Docker build context is How to use .dockerignore to exclude unnecessary files How to verify that only intended files are copied into the container This will help you build smaller, faster, and more secure Docker images. ✅\n📆 What’s Next? # Lesson 12: Add PostgreSQL to FastAPI Learn how to connect your FastAPI app to a PostgreSQL container using Docker Compose.\n"},{"id":79,"href":"/docs/programming-core/java/net/tcp-socket/","title":"Tcp Socket","section":"Net","content":" What Is a Socket? # Server socket listens to a port Normally, a server runs on a specific computer and has a socket that is bound to a specific port number. The server just waits, listening to the socket for a client to make a connection request.\nCient connects to the server with server ip and port On the client-side: The client knows the hostname of the machine on which the server is running and the port number on which the server is listening. To make a connection request, the client tries to rendezvous with the server on the server\u0026rsquo;s machine and port. The client also needs to identify itself to the server so it binds to a local port number that it will use during this connection. This is usually assigned by the system.\nServer accepts connection and creates a new socket If everything goes well, the server accepts the connection. Upon acceptance, the server gets a new socket bound to the same local port and also has its remote endpoint set to the address and port of the client. It needs a new socket so that it can continue to listen to the original socket for connection requests while tending to the needs of the connected client.\nSocket is successfully created on client On the client side, if the connection is accepted, a socket is successfully created and the client can use the socket to communicate with the server.\nThe client and server can now communicate by writing to or reading from their sockets. [1]\nExamples # Create Sockets # Server\npublic class Server_ { public static void main(String[] args) throws IOException { // A server socket listens to port 8888 ServerSocket serverSocket = new ServerSocket(8888); // The program is blocked here waiting for client connecting Socket socket = serverSocket.accept(); // Your business logic ... // Close both sockets to release system resource socket.close(); serverSocket.close(); } } First we create a ServerSocket listening to port 8888 Before accepting any connections, the program is blocked at line Socket socket = serverSocket.accept(); A client requests to connect to the server. After the server accepts the request, a new socket will be created in the server After your business logic, close the socket and server socket to release system resources Client\npublic class Client_ { public static void main(String[] args) throws IOException { // Create a new socket and connect to port 8888 on the server Socket socket = new Socket(InetAddress.getByName(\u0026#34;\u0026lt;host-name\u0026gt;\u0026#34;);, 8888); // Your business logic ... // Close the socket to release system resource socket.close(); } } First we create a socket connecting to port 8888 of the host Once the connection is successfully created, a socket will be created After your business logic, close the socket to release system resources Upload File from Client to Server # Client\npublic class Client_ { public static void main(String[] args) throws IOException { // Create a new socket and connect to a specific port of the server Socket socket = new Socket(InetAddress.getLocalHost(), 8888); // Read file from disk String filePath = \u0026#34;/path/to/the/file\u0026#34;; FileInputStream bis = new FileInputStream(filePath); byte[] data = inputStreamToByteArray(bis); bis.close(); System.out.println(\u0026#34;Client reads a file from disk\u0026#34;); // Send byte data of the file to the server OutputStream os = socket.getOutputStream(); os.write(data); // You need to shut down the output stream, otherwise the server will hang up // It\u0026#39;s a signal let server know that you\u0026#39;re done sending data // TODO: What\u0026#39;s the corresponding TCP packet for this? socket.shutdownOutput(); System.out.println(\u0026#34;Client uploads a file to server\u0026#34;); // Close streams and socket os.close(); socket.close(); } } Client reads a file in the disk to a FileInputStream, and invoke an utility method inputStreamToByteArray to convert InputStream to a byte array Then client writes the byte data into its output stream and sends it to the server Close all streams and socket to release system resources Utility method:\npublic class InputStreamUtil { public static byte[] inputStreamToByteArray(InputStream is) throws IOException { byte[] buf = new byte[1024]; ByteArrayOutputStream bos = new ByteArrayOutputStream(); int len; while((len = is.read(buf)) != -1) { bos.write(buf, 0, len); } byte[] result = bos.toByteArray(); bos.close(); return result; } } The utility method creates a ByteArrayOutputStream as a buffer to store data read from the input stream Contantly reads data from the input stream and writes the data into the ByteArrayOutputStream Invokes toByteArray method to get a byte array holding the data Server\npublic class Server_ { public static void main(String[] args) throws IOException { // A server socket waits for requests to come in over the network. ServerSocket serverSocket = new ServerSocket(8888); // Listens to a connection and accepts it. Returns a new socket Socket socket = serverSocket.accept(); // Read data from input stream InputStream is = socket.getInputStream(); byte[] data = inputStreamToByteArray(is); System.out.println(\u0026#34;Server received file uploaded from client\u0026#34;); // Save the file to local disk String filePath = \u0026#34;/path/to/save/the/file\u0026#34;; BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(filePath)); bos.write(data); bos.close(); // Close streams and socket is.close(); socket.close(); serverSocket.close(); } } Server socket listens to port 8888 in local host Creates a new socket to accept connection from the client Reads data from socket\u0026rsquo;s input stream Writes byte data to disk through FileOutputStream Close all streams and sockets to release system resources "},{"id":80,"href":"/docs/programming-core/java/io/decorator-pattern-in-java-io/","title":"Decorator Pattern in Java IO","section":"Io","content":" Node Streams (Low-Level Streams) # Definition:\nNode Streams connect directly with the source of the data They read data from or write data to a specific location (like a file, memory, or network socket). Example: FileInputStream is a node stream that reads byte data from a file.\nFile file = new File(\u0026#34;example.txt\u0026#34;); FileInputStream fis = new FileInputStream(file); Here, FileInputStream is directly reading the bytes from the file \u0026ldquo;example.txt\u0026rdquo;. It\u0026rsquo;s a direct connection between the Java program and the file.\nProcessing Streams (High-Level Streams) # Definition:\nProcessing Streams are built on top of node streams to provide additional functionality, like buffering, filtering, reading objects, etc. They don\u0026rsquo;t write or read data directly to or from data source, but delegate this job to node streams. Example: BufferedInputStream is a processing stream that adds buffering capabilities to another input stream, such as FileInputStream.\nFileInputStream fis = new FileInputStream(\u0026#34;example.txt\u0026#34;); BufferedInputStream bis = new BufferedInputStream(fis); BufferedInputStream does not connect directly to the file. Instead, it wraps the FileInputStream and adds buffering to it. When you read from a BufferedInputStream, it retrieves data from the buffer, and when the buffer is empty, it reads another large chunk of data from the FileInputStream and fills the buffer. This minimizes the number of interactions with the file system, which is much slower than reading from a buffer in memory.\nCombining Node and Processing Streams # Typically, you chain processing streams together to get both their benefits. For instance, wrapping a FileInputStream with a BufferedInputStream.\nFile file = new File(\u0026#34;example.txt\u0026#34;); InputStream is = new BufferedInputStream(new FileInputStream(file)); ... is.close(); In this example, we use FileInputStream to connect to the file and BufferedInputStream to add buffering. The data still comes from the file, but it passes through the buffer, which can be read from more quickly than the file.\nDecorator Pattern in Java IO Stream # Component (Node Stream) # Definition: The component is the primary or underlying object that has the original behavior, which is to read data from data source for node streams. In Java IO: FileInputStream and FileReader are example of components. They provide the basic functionality for byte and character stream handling. Decorator (Processing Stream) # Definition: Decorators implement or extend the same interface as the component they are going to decorate. They compose the instance of component and provide an enhanced interface with added responsibilities.- In Java IO: BufferedInputStream, BufferedOutputStream, BufferedReader, and BufferedWriter are examples of decorators. They add buffering to the streams they decorate. Key Features of Decorator # Delegation: The decorator delegates the work to the component it decorates and then adds its own behavior before or after delegating. This is a key feature because it means the decorator itself doesn\u0026rsquo;t handle the primary operations; it relies on the component to do so.\npublic class BufferedInputStream extends InputStream { private InputStream inner; public BufferedInputStream(InputStream inner) { this.inner = inner; } public int read() throws IOException { ... // own behavior of decorator return inner.read(); } } Add Features Over Component Dynamically: Because decorators implement or extend the same interface as the component they are going to decorate, that means they can be decorated by other decorators. Decorators can be nested and combined in any order to add multiple behaviors.\nInputStream input = new FileInputStream(\u0026#34;file.txt\u0026#34;); InputStream buffered = new BufferedInputStream(input); InputStream dataInput = new DataInputStream(buffered); Q \u0026amp; A of Decorator Pattern # What\u0026rsquo;s the main difference between component and decorator?Component and decorator both implement or extend same interface or superclass. However, components don\u0026rsquo;t compose an instance of the interface or superclass, so it can only perform the original behavior. Decorators compose an instance of the interface or superclass, which can be delegated to perform the original behavior. What\u0026rsquo;s the difference between strategy and decorator pattern?Key feature for decorator and strategy pattern is delegation. However, decorators extend or implements same interface or superclass it decorates, which mean decorators themselves can be decorated also, which add extra features dynamically during runtime. "},{"id":81,"href":"/docs/programming-core/java/io/object-input-and-output-stream/","title":"ObjectInputStream and ObjectOutputStream","section":"Io","content":"This post will introduce two new processing streams, ObjectInputStream and ObjectOutputStream, which are used to deserialize and serialize objects and primitive data.\nObjectInputStream # Purpose: To deserialize objects and primitive data written using ObjectOutputStream. It allows you to read bytes from a source (like a file or network socket) and reconstructs objects from those bytes.\nKey Features: Processing stream: reads serialized objects from an underlying InputStream.\nCommon Use Case: Commonly used in networking (for sending objects across a network) or for persisting objects to files.\nExample of ObjectInputStream # Deserialize objects from a file\nFileInputStream fis = new FileInputStream(\u0026#34;example.ser\u0026#34;); ObjectInputStream ois = new ObjectInputStream(fis); MyClass o = (MyClass) ois.readObject(); ois.close(); Deserialize objects from network\nSocket socket = new Socket(\u0026#34;example.com\u0026#34;, 8080); InputStream is = socket.getInputStream(); ObjectInputStream ois = new ObjectInputStream(is); MyClass o = (MyClass) ois.readObject(); ois.close(); ObjectOutputStream # Purpose: To serialize objects and primitive data types to an OutputStream. It converts objects into a byte stream that can then be written to a file, sent over a network, etc.\nKey Features: Processing stream: serializes objects and primitives to an underlying OutputStream.\nCommon Use Case: Used for saving object states, caching objects, or sending objects over a network.\nExample of ObjectInputStream # Serialize objects to a file\nFileOutputStream fos = new FileOutputStream(\u0026#34;example.ser\u0026#34;); ObjectOutputStream oos = new ObjectOutputStream(fos); MyClass o = MyClass(); oos.writeObject(o); oos.close(); Serialize objects to network\nSocket socket = new Socket(\u0026#34;example.com\u0026#34;, 8080); OutputStream os = socket.getOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(os); MyClass o = MyClass(); oos.writeObject(o); oos.close(); Serializable Interface # public interface Serializable {} Purpose: The Serializable interface is a marker interface (it has no methods) that indicates the implementor can be serialized by ObjectOutputStream. "},{"id":82,"href":"/docs/programming-core/java/io/stream-reader-bridge-of-byte-and-char/","title":"Stream Reader: Bridge of Byte and Char","section":"Io","content":"There are two special readers in Java IO package, they are InputStreamReader and OutputStreamWriter which serve as bridge between byte data and character data\nInputStreamReader # Important constructor\npublic class InputStreamReader extends Reader { public InputStreamReader(InputStream in, Charset cs); } Key feature (Bridge from byte to char)\nCharacter Encoding: Data stored in files or transmitted over networks is often in the form of bytes. When such data represents text, it needs to be decoded using a specific character encoding (like UTF-8, ISO-8859-1, etc.) to be converted into characters that can be processed by the program. InputStreamReader facilitates this by decoding the byte stream into characters according to the specified or default charset.\nExample # // A socket connection to a server that sends text data Socket socket = new Socket(\u0026#34;example.com\u0026#34;, 80); InputStream input = socket.getInputStream(); // Use InputStreamReader to decode the byte stream from the socket // Assume the server sends data encoded in UTF-8 InputStreamReader reader = new InputStreamReader(input, StandardCharsets.UTF_8); int data; while ((data = reader.read()) != -1) { char character = (char) data; // Process the character } reader.close(); OutputStreamWriter # Bridge from Character to Byte: It bridges Writer (character-oriented abstraction) with OutputStream (byte-oriented stream). Customizable Encoding: Allows specifying a charset for encoding characters, making the character writing process customizable and flexible. "},{"id":83,"href":"/docs/programming-core/java/io/io-stream/","title":"Java IO Stream","section":"Io","content":" What is stream? # Java Input/OutputStream and Reader/Writer are essential components of the Java I/O (Input/Output) library, designed to facilitate reading and writing data in various forms from different sources within Java applications. These classes serve as a bridge between your application and external data sources, making it easier to perform I/O operations efficiently and consistently. In this post, we will explore what Input/OutputStream and Reader/Writer are, their relationships, and how they classify based on the type of data and data source.\nBelow image illustrates the relationship among stream in Java IO, application and data source: Classification # Stream in Java IO could be classified by data type and data flow\n| Stream flow | Byte Stream | Character Stream | |-----------------------------|------------------|------------------| | input: data source -\u0026gt; app | InputStream | Reader | | output: app -\u0026gt; data source | OutputStream | Writer | Data type\nByte data: InputStream/OutputStream Character data: Reader/Writer Data flow\nInput (data source -\u0026gt; app): InputStream/Reader Output (app -\u0026gt; data source): OutputStream/Writer Class Hierarchy # java.io │ ├── Byte Stream │ ├── InputStream (abstract) │ │ ├── ByteArrayInputStream │ │ ├── FileInputStream │ │ ├── FilterInputStream │ │ │ ├── BufferedInputStream │ │ │ ├── DataInputStream │ │ │ └── PushbackInputStream │ │ ├── ObjectInputStream │ │ └── PipedInputStream │ │ │ └── OutputStream (abstract) │ ├── ByteArrayOutputStream │ ├── FileOutputStream │ ├── FilterOutputStream │ │ ├── BufferedOutputStream │ │ ├── DataOutputStream │ │ └── PrintStream │ ├── ObjectOutputStream │ └── PipedOutputStream │ └── Character Stream ├── Reader (abstract) │ ├── BufferedReader │ ├── CharArrayReader │ ├── FileReader │ ├── FilterReader │ │ ├── BufferedReader │ │ └── PushbackReader │ ├── InputStreamReader │ │ └── FileReader │ ├── PipedReader │ ├── StringReader │ └── FilterReader │ └── Writer (abstract) ├── BufferedWriter ├── CharArrayWriter ├── FileWriter ├── FilterWriter ├── OutputStreamWriter │ └── FileWriter ├── PipedWriter ├── PrintWriter └── StringWriter There are more than 40 classes under Java IO, and with a lot of different types of Input/Output Stream and Writer/Reader. However, IO Stream has a well defined hierarchy, here are some of properties of the it:\nJava IO has four abstract stream classes, they are InputStream OutputStream Reader Writer Every abstract stream class has multiple subclasses, and every subclass uses the name of its superclass as its suffix, for example: FileInputStream is subclass of InputStream, its suffix is InputStream, which is superclass name All subclasses under one abstract stream superclass (e.g. InputStream) can be classifed as two types Node Streams: These are the fundamental streams that directly interact with the specific data sources or destinations. They are the building blocks for reading or writing data and connect directly to the source, such as a file, an array in memory, or a network socket. Example FileInputStream ByteArrayInputStream Processing Streams: Also known as wrapper streams or filter streams, these streams do not directly read or write data to the data source. Instead, they are used to wrap around other streams (node streams or other processing streams) to provide additional functionalities like buffering, data conversion, or performance enhancement. Example BufferedInputStream DataInputStream The concept of node stream and processing stream may be hard to understand now. We will dive deeper into them with code examples and introduce the decorator design pattern underlying processing stream.\nInputStream # FileStreamInput # FileStreamInput is a subclass of InputStream. It\u0026rsquo;s a node steam whose data source is files from disk\nExample\nFile file = new File(\u0026#34;example.txt\u0026#34;); InputStream is = new FileInputStream(file); int content; while ((content = is.read()) != -1) { // process the content } is.close(); Read binary files\nFileInputSteam can not only read text files. More importantly, cause it is a byte stream, it is normally used to read binary files, like images, videos and any non-text files. Actually, we should only use InputStream to read binary files. Here is the main problem with using Reader to reading binary files:\nReader classes are deisnged to read character data. When you use a Reader to read a binary file, it may attempt to interpret non-character bytes as character, leading to data curruption and loss of imformation\nBufferedInputStream # BufferedInputStream is a processing stream\n"},{"id":84,"href":"/docs/programming-core/java/io/file/","title":"Java File","section":"Io","content":"The File class in Java, found in the java.io package, is not used for file content manipulation (reading/writing) but for file and directory pathnames operations. It\u0026rsquo;s used to obtain or manipulate the information associated with a file or directory, such as metadata, permissions, and path details.\nCommonly Used APIs in File Class for Files and Directories # File Handling # Create a New File\ncreateNewFile(): Creates a new file if it does not exist. File myFile = new File(\u0026#34;myfile.txt\u0026#34;); if (myFile.createNewFile()) { System.out.println(\u0026#34;File created.\u0026#34;); } else { System.out.println(\u0026#34;File already exists.\u0026#34;); } Delete a File\ndelete(): Deletes the file or directory. if (myFile.delete()) { System.out.println(\u0026#34;File deleted.\u0026#34;); } else { System.out.println(\u0026#34;Failed to delete the file.\u0026#34;); } Check if File Exists\nexists(): Checks if the file or directory exists. if (myFile.exists()) { System.out.println(\u0026#34;The file exists.\u0026#34;); } else { System.out.println(\u0026#34;The file does not exist.\u0026#34;); } Read File Attributes\ncanRead(), canWrite(), isHidden(): Check read/write permission or if the file is hidden. length(): Returns the file size in bytes. lastModified(): Returns the last modified timestamp. Directory Handling # Create a Directory\nmkdir(): Creates the directory named by this abstract pathname. File dir = new File(\u0026#34;mydir\u0026#34;); if (dir.mkdir()) { System.out.println(\u0026#34;Directory created.\u0026#34;); } else { System.out.println(\u0026#34;Failed to create directory.\u0026#34;); } List Files and Directories\nlist(): Returns an array of strings naming the files and directories in the directory. String[] files = dir.list(); for (String file : files) { System.out.println(file); } Check Directory Status\nisDirectory(): Checks if the File instance is a directory. isFile(): Checks if the File instance is a regular file. File Path Operations\ngetPath(), getAbsolutePath(), getCanonicalPath(): Various ways to get the file/directory path. "},{"id":85,"href":"/docs/programming-core/java/reflection/","title":"Reflection","section":"Java","content":" Introduction to Java Reflection # Java Reflection is a powerful feature that allows runtime introspection of classes, objects, and their members. It enables Java programs to manipulate internal properties and methods of classes dynamically. Reflection is especially useful in scenarios where the program needs to interact with classes and objects whose properties are not known at compile time.\nClass Object # The heart of Java\u0026rsquo;s reflection mechanism. It\u0026rsquo;s an instance that represents classes and interfaces in a running Java application. Every class, including primitive types and arrays, has a corresponding Class object.\nAcquiring Class Objects # Class.forName(\u0026quot;ClassName\u0026quot;): Loads the class dynamically. ClassName.class: Directly accesses the class if it\u0026rsquo;s known at compile time. object.getClass(): Retrieves the runtime class of an object. ClassLoader: For more complex scenarios, especially in modular applications. For primitive types: int.class or Integer.TYPE. // Using Class.forName() Class cls1 = Class.forName(\u0026#34;com.example.model.Person\u0026#34;); // Using .class syntax Class cls2 = Person.class; // From an instance of the class Person person = new Person(); Class cls3 = person.getClass(); // Using ClassLoader ClassLoader classLoader = Person.class.getClassLoader(); Class cls4 = classLoader.loadClass(\u0026#34;com.example.model.Person\u0026#34;); // Primitive type class Class intCls = int.class; Uniqueness and Singleton Nature of Class Object in Java Reflection # Singleton Pattern in Class Objects # The singleton pattern ensures that a class has only one instance and provides a global point of access to that instance. In the context of Java\u0026rsquo;s Class objects, the JVM enforces this pattern.\nHow It Works # Unique Instance: When a class is loaded into the JVM, a single instance of Class is created to represent it. This instance contains all the metadata about the class, including its methods, fields, constructors, and superclass. Global Accessibility: This unique Class instance is globally accessible through different means like Class.forName(), ClassName.class, and object.getClass(). JVM Management: The JVM manages these Class objects, ensuring that for each class loaded, only one Class object exists. Benefits # Consistency: Since there\u0026rsquo;s only one Class object per class, it guarantees consistency across the application. All parts of the program refer to the same metadata of a class. Efficiency: Reduces memory overhead by avoiding multiple instances of metadata for the same class. Example # Class cls1 = String.class; Class cls2 = \u0026#34;Hello, World!\u0026#34;.getClass(); Class cls3 = Class.forName(\u0026#34;java.lang.String\u0026#34;); // All of these Class objects are the same boolean areSame = (cls1 == cls2) \u0026amp;\u0026amp; (cls2 == cls3); // true Loading of Variables in a Class # In Java, variables are loaded and initialized based on their type (static or instance) and the sequence of their declaration in the class.\nStatic Variables # Initialization: Static variables are initialized when the class is first loaded into the JVM. Order of Execution: Static blocks and static variables are executed in the order they appear in the class. public class MyClass { static String staticVar = \u0026#34;Static Variable\u0026#34;; static { System.out.println(staticVar); // Accessed in static block staticVar = \u0026#34;Modified in Static Block\u0026#34;; } } // When MyClass is loaded, the static block is executed after staticVar is initialized. Instance Variables # Initialization: Instance variables are initialized when an instance of the class is created. Constructor Execution: They are typically initialized before the constructor\u0026rsquo;s execution or within it. Field # Represents the fields of a class. You can use Field objects to get and set field values dynamically.\nField field = cls.getField(\u0026#34;fieldName\u0026#34;); field.setAccessible(true); // For private fields Object value = field.get(objectInstance); field.set(objectInstance, newValue); Method # Represents a method of a class. You can invoke methods dynamically using Method objects.\nMethod method = cls.getMethod(\u0026#34;methodName\u0026#34;, parameterTypes); method.setAccessible(true); // For private methods method.invoke(objectInstance, arguments); Constructor # Represents a constructor of a class. Constructors can be used to create new instances of a class dynamically.\nConstructor constructor = cls.getConstructor(parameterTypes); Object newInstance = constructor.newInstance(initargs); TODO # For static variable loading, show the example with static variables declared before and after static block Better example about Method, Field and Constructor, like method getDeclaredMethod or getDeclaredField, and how to get a specific constructor if the class has multiples ones Reference # Perfect Video in Chinese\n"},{"id":86,"href":"/docs/operating-system/linux/linux-manual/","title":"Linux Manual","section":"Linux","content":" Vim: Three Modes # Normal mode Insert mode Command mode Command Mode # Type : to enter.\nw (write): save. q: quit. x (==wq): save and quit. set nu: show line numbers. set nonu: do not show line numbers. Normal Mode # y (yank): copy. p: paste. 5yy: copy 4 lines. dd: delete. 4dd: delete 4 lines. /word_to_search + Enter: search a word. n: next. G: last line. gg (go to the top\u0026hellip;): first line. u: undo. Ctrl + r: redo. 20gg / 20G: go to line 20. User Login and Logout Commands # Command: su - \u0026lt;user\u0026gt;: switch user. logout: If you are the root user, you will be switched to the standard user. If you are not the root user, you will be logged out of the Linux system. adduser \u0026lt;user\u0026gt;: add a user under /home directory. -d \u0026lt;home_dir\u0026gt;: specify the home directory for the new user. passwd: set the password for the root user. \u0026lt;user\u0026gt;: set the password for the user. User # User home directory: /home/\u0026lt;user\u0026gt;. File and Directory Commands # ls: List files and directories in the current directory. Example: ls Common options: -h: make the file sizes in the output more human-readable cd: Change the current directory. Example: cd /path/to/directory pwd: Print the current working directory. Example: pwd mkdir: Create a new directory. Example: mkdir new_directory rmdir: Remove a directory (only if it\u0026rsquo;s empty). Example: rmdir empty_directory rm: Remove files or directories. Example: rm file.txt or rm -r directory cp: Copy files and directories. Example: cp file.txt /path/to/destination mv: Move or rename files and directories. Example: mv file.txt new_name.txt or mv file.txt /path/to/destination touch: Create an empty file. Example: touch new_file.txt cat: Display the content of a file. Options: -n: show line number Example: cat file.txt more and less: Display file content page by page. Example: more file.txt or less file.txt head and tail: Display the beginning or end of a file. Example: head file.txt or tail file.txt chmod: Change file permissions. Example: chmod 755 file.txt chown: Change file ownership. Example: chown user:group file.txt ln: Create symbolic links or hard links to files or directories. Example: ln -s target link_name (for symbolic links) or ln target link_name (for hard links) history: Display latest executed commands in current shell session Use case: history !100: re-execute executed command in line 100 \u0026gt; \u0026amp; \u0026gt;\u0026gt;: In Linux and Unix-like operating systems, \u0026gt; and \u0026raquo; are used as operators for redirecting output from commands. They are often used in the command line to control where the output of a command is sent Difference: \u0026gt; overwrite existed content, but \u0026gt;\u0026gt; append command output to to the output of a file Example: echo \u0026quot;Hello, World!\u0026quot; \u0026gt; output.txt echo \u0026quot;Appended text\u0026quot; \u0026gt;\u0026gt; output.txt echo file1.txt \u0026gt;\u0026gt; file2.txt ls -l \u0026gt;\u0026gt; output.txt Date Command # Date Search \u0026amp; Find Commands # find: Search for files and directories. Basic syntax: find [path] [options] [expression] Common options: -name \u0026lt;pattern\u0026gt;: Search for files and directories with a specific name or pattern. -type \u0026lt;type\u0026gt;: Specify the type of file (e.g., f for regular files, d for directories). -mtime \u0026lt;days\u0026gt;: Search for files modified within a certain number of days. -size \u0026lt;size\u0026gt;: Search for files of a specific size (e.g., +10M for files larger than 10 megabytes). -user \u0026lt;username\u0026gt;: Search for files owned by a specific user. -group \u0026lt;groupname\u0026gt;: Search for files in a specific group. -exec \u0026lt;command\u0026gt; {} \\;: Execute a command on each matching file or directory. -print: Display the path of each matching file or directory. Example: find . -name \u0026quot;example.txt\u0026quot; find ~ -type d -user john find /path/to/directory -type f -mtime +7 -exec rm {} ; find . -type f -size +100M find /path/to/directory -type f -name \u0026quot;*.log\u0026quot; locate: Quick find location of a file. It uses pre-built database (updatedb command to create index for dirs and files) to provide fast search results Usage: sudo yum install mlocate: install locate in CentOS updatedb: create a db to store index of all files in your linux system locate example.txt: find example.txt files Reminder Keep in mind that locate is a powerful tool for quickly finding files and directories on your system. However, it doesn\u0026rsquo;t search for files in real-time, so you need to ensure that the database is updated regularly using the updatedb command for accurate results. grep: filter and find "},{"id":87,"href":"/docs/web/web-fundamentals/cors/","title":"CORS","section":"Web Fundamentals","content":" What\u0026rsquo;s CORS # CORS is a mechanism to stop you from accessing resource in one origin from another origin. For example, there is an image img.jpg from origin images.com, if you don\u0026rsquo;t have CORS set properly, you can\u0026rsquo;t access the img.jpg from other origins like yourOrigin.com.\nWhy needs CORS # CORS is mainly for security usage. Image that your browser stores credential cookies of domain bank.com which is the website of you bank account, and a hacking website hacking.com want to access your bank information and make a transaction from your bank. If without CORS, the javascript script from hacking.com is able to get the cookies under domain bank.com and make requests to bank.com. CORS can protect your website from malicious requests\nAccess-Control-Allow-Origin Header # Issue Demonstration # Start a local host\nconst express = require(\u0026#34;express\u0026#34;); const app = express(); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.json({ name: \u0026#34;Hello\u0026#34;, value: \u0026#34;World\u0026#34;}) }); app.listen(3000); In your browser, navigate to http://localhost:3000. Open console and make a GET request to http://localhost:3000\nfetch(\u0026#34;http://localhost:3000\u0026#34;).then((req) =\u0026gt; req.json()).then(console.log); You will receive a successful response\n{name: \u0026#39;Hello\u0026#39;, value: \u0026#39;World\u0026#39;} You didn\u0026rsquo;t see any CORS issues, cause you were making request from http://localhost:3000 to same origin.\nOpen a new tab in your browser, navigate to www.google.com, and make a same request. You will see error like:\nAccess to fetch at \u0026#39;http://localhost:3000/\u0026#39; from origin \u0026#39;https://www.google.com\u0026#39; has been blocked by CORS policy: No \u0026#39;Access-Control-Allow-Origin\u0026#39; header is present on the requested resource. If an opaque response serves your needs, set the request\u0026#39;s mode to \u0026#39;no-cors\u0026#39; to fetch the resource with CORS disabled. Solution # Add www.google.com in allowed origins in your CORS config\nconst express = require(\u0026#34;express\u0026#34;); const app = express(); const cors = require(\u0026#34;cors\u0026#34;); app.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34; })); app.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.json({ name: \u0026#34;Hello\u0026#34;, value: \u0026#34;World\u0026#34;}) }); app.listen(3000); Make a same request again to local host, and the error is gone and you will see response printed in console:\n{name: \u0026#39;Hello\u0026#39;, value: \u0026#39;World\u0026#39;} Access-Control-Allow-Methods Header # Problem Demonstration # To demonstration the problem with Access-Control-Allow-Methods header\nI add GET and POST as allowed methods in CORS config, which limits that clients can only use GET or POST methods in their requests to the server Also, I changed the method provided by the server from GET to PUT app.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;], })); app.put(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { res.json({ name: \u0026#34;Hello\u0026#34;, value: \u0026#34;World\u0026#34;}) }); Then let\u0026rsquo;s make request in the console under www.google.com domain\nfetch(\u0026#34;http://localhost:3000\u0026#34;, {method: \u0026#34;PUT\u0026#34;}).then((req) =\u0026gt; req.json()).then(console.log); We will see error:\nwww.google.com/:1 Access to fetch at \u0026#39;http://localhost:3000/\u0026#39; from origin \u0026#39;https://www.google.com\u0026#39; has been blocked by CORS policy: Method PUT is not allowed by Access-Control-Allow-Methods in preflight response. Solution # Add PUT in allowed methods\napp.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;], })); Make the same request from www.google.com again, and this time the error was gone and we receive correct response.\n{name: \u0026#39;Hello\u0026#39;, value: \u0026#39;World\u0026#39;} Checking console, we can see two requests, the first request, which is called preflight request, and its response header is lik below:\nHTTP/1.1 204 No Content Access-Control-Allow-Origin: https://www.google.com Vary: Origin, Access-Control-Request-Headers Access-Control-Allow-Methods: GET,POST,PUT Content-Length: 0 Connection: keep-alive Keep-Alive: timeout=5 The headers tell the browser that cross-origin request to \u0026lsquo;http://localhost:3000/\u0026rsquo; is allowed for origin https://www.google.com, but only applies to three methods: GET, POST and PUT, which are same as what we defined in our CORS config.\nAnother request is the real cross-origin request, with PUT method, and server returns response with content.\nPreflight Request # We demonstrate Access-Control-Allow-Methods problem, but what is preflight response in above error\nPreflight request is an OPTIONS request automatically sent by browser before a cross origin request. Preflight request is used to determine if a cross origin request is safe. Preflight request including header Origin, Access-Control-Request-Method and Access-Control-Request-Headers (Optional) How Does Preflight Request Work # Browser sends an OPTIONS request to the server with headers mentioned above OPTIONS / HTTP/1.1 Access-Control-Request-Method: PUT Host: localhost:3000 Origin: https://www.google.com Sec-Fetch-Mode: cors Sec-Fetch-Site: cross-site Server receives the preflight request and check its CORS configuration. It determines if the real cross-origin request has proper permission by checking origin, method and headers in the preflight request\nIf the real request is allowed, server responds to the preflight request with appropriate CORS headers, including allowed origins, methods and headers.\nAfter receiving preflight response from the server, if the server allows the request, then browser will send the real cross-origin request to the server. If not, browser will block the request and prevent it from reaching the server\nIf we include headers in the request\nfetch(\u0026#34;http://localhost:3000\u0026#34;, { method: \u0026#34;PUT\u0026#34;, headers: { \u0026#34;Customer-Header\u0026#34;: \u0026#34;key=value\u0026#34;, \u0026#34;X-Customer-Header\u0026#34;: \u0026#34;key=value\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;value\u0026#34; } }).then((req) =\u0026gt; req.json()).then(console.log); Here we include two customer headers: Customer-Header and X-Customer-Header, and the preflight request and response are like below\nRequest OPTIONS / HTTP/1.1 Access-Control-Request-Headers: customer-header,x-customer-header Access-Control-Request-Method: PUT Host: localhost:3000 Origin: https://www.google.com Some headers are omitted, but you can see here preflight request tells server which headers are included in the cross-origin request.\nResponse HTTP/1.1 204 No Content Access-Control-Allow-Origin: https://www.google.com Access-Control-Allow-Methods: GET,POST,PUT Access-Control-Allow-Headers: customer-header,x-customer-header Access-Control-Allow-Credentials Header # The Access-Control-Allow-Credentials response header tells browsers whether to expose the response to the frontend JavaScript code when the request\u0026rsquo;s credentials mode (Request.credentials) is include.\nIssue Demonstration # Make below call in your browser console under Google domain\nfetch(\u0026#34;http://localhost:3000\u0026#34;, { method: \u0026#34;PUT\u0026#34;, credentials: \u0026#34;include\u0026#34; }).then((req) =\u0026gt; req.json()).then(console.log); You will see error message:\nAccess to fetch at \u0026#39;http://localhost:3000/\u0026#39; from origin \u0026#39;https://www.google.com\u0026#39; has been blocked by CORS policy: Response to preflight request doesn\u0026#39;t pass access control check: The value of the \u0026#39;Access-Control-Allow-Credentials\u0026#39; header in the response is \u0026#39;\u0026#39; which must be \u0026#39;true\u0026#39; when the request\u0026#39;s credentials mode is \u0026#39;include\u0026#39;. That is because you want to include credentials, including cookies, authorization headers, or TLS client certificates, in your request.\nRemember if you don\u0026rsquo;t explicitly specify credentials: \u0026quot;include\u0026quot; in your request, cookies will not be added in your request\u0026rsquo;s headers\nSolution # Set credentials to true in server\u0026rsquo;s CORS config\napp.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;], credentials: true })); Access-Control-Allow-Headers Header # Used in response to a preflight request to indicate which HTTP headers can be used when making the actual request.\nProblem Demonstration # Limit Content-Type as the only header which server allowed\napp.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;], allowedHeaders: [\u0026#39;Content-Type\u0026#39;] })); Then make a call from Google domain with two customer headers\nfetch(\u0026#34;http://localhost:3000\u0026#34;, { method: \u0026#34;PUT\u0026#34;, headers: { \u0026#34;Customer-Header\u0026#34;: \u0026#34;key=value\u0026#34;, \u0026#34;X-Customer-Header\u0026#34;: \u0026#34;key=value\u0026#34;, \u0026#34;Cookie\u0026#34;: \u0026#34;value\u0026#34; } }).then((req) =\u0026gt; req.json()).then(console.log); The request was denied with below error message\nAccess to fetch at \u0026#39;http://localhost:3000/\u0026#39; from origin \u0026#39;https://www.google.com\u0026#39; has been blocked by CORS policy: Request header field customer-header is not allowed by Access-Control-Allow-Headers in preflight response. Solution # Add customers header as allowed headers\napp.use(cors({ origin: \u0026#34;https://www.google.com\u0026#34;, methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;], allowedHeaders: [\u0026#34;Content-Type\u0026#34;, \u0026#34;Customer-Header\u0026#34;, \u0026#34;X-Customer-Header\u0026#34;], })); Reference # YouTube video CORS Preflight Request Access-Control-Allow-Credentials "},{"id":88,"href":"/docs/web/web-api/web-storage-api/","title":"Web Storage Api","section":"Web API","content":" Definition # The Web Storage API provides mechanisms by which browsers can store key/value pairs\nSource code # These mechanisms are available via the Window.sessionStorage and Window.localStorage properties (to be more precise, the Window object implements the WindowLocalStorage and WindowSessionStorage objects, which the localStorage and sessionStorage properties hang off) — invoking one of these will create an instance of the Storage object, through which data items can be set, retrieved and removed. A different Storage object is used for the sessionStorage and localStorage for each origin — they function and are controlled separately.\ninterface Window extends WindowLocalStorage, WindowSessionStorage { ... } interface WindowSessionStorage { readonly sessionStorage: Storage; } interface WindowLocalStorage { readonly localStorage: Storage; } interface Storage { readonly length: number; clear(): void; getItem(key: string): string | null; key(index: number): string | null; removeItem(key: string): void; setItem(key: string, value: string): void; [name: string]: any; } Property # The read-only sessionStorage property accesses a session Storage object for the current origin. sessionStorage is similar to localStorage; the difference is that while data in localStorage doesn\u0026rsquo;t expire, data in sessionStorage is cleared when the page session ends.\nData stored in sessionStorage and localStorage is specific to the protocol of the page. In particular, data stored by a script on a site accessed with HTTP (e.g., http://example.com) is put in a different sessionStorage object from the same site accessed with HTTPS (e.g., https://example.com).\nThe keys and the values are always in the UTF-16 string format, which uses two bytes per character. As with objects, integer keys are automatically converted to strings.\nReference # https://developer.mozilla.org/en-US/docs/Web/API/Web_Storage_API#web_storage_concepts_and_usage YouTube video with examples "},{"id":89,"href":"/docs/tool/hugo/set-up-hugo-in-git-hub-pages/","title":"Set Up Hugo in Git Hub Pages","section":"Hugo","content":" Create a repository to hold the source code of your blogs # Create a repository of Git Hub Pages # Create a new Hugo project in your local machine # cd ~/Projects hugo new site \u0026lt;site name\u0026gt; cd \u0026lt;site name\u0026gt; git init git remote add origin \u0026lt;repository URL of your blogs\u0026gt; git add . git commit -m \u0026#34;Initiate a new hugo project\u0026#34; git push origin main Choose theme for your blog site # Navigate to Hugo theme website Down your favorite theme to directory ~/Projects/\u0026lt;site name\u0026gt;/themes Modify your config file baseURL = \u0026#34;https://\u0026lt;URL of your Git Hub Pages\u0026gt;/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;\u0026lt;website name\u0026gt;\u0026#34; theme = \u0026#34;\u0026lt;theme name\u0026gt;\u0026#34; Run hugo server to check if everything in your local host is expected Add Git Hub Pages repository as submodule of your source repository # cd ~/Projects/\u0026lt;site name\u0026gt; git submodule add -b main \u0026lt;URL of your Git Hub Pages\u0026gt; public # Generate static resouce in public directory hugo cd public git add . git commit -m \u0026#34;\u0026#34;Initiate a new hugo project\u0026#34; git push origin main "}]